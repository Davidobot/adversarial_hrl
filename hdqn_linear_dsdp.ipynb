{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Deep Q-Learning with Experience Replay\n",
    "\n",
    "Based on Playing Atari with Deep Reinforcement Learning [link](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "Main aim is to train a *policy* that tries to maximise $R_{t_0} = \\sum_{t=t_0}^{\\inf}\\gamma^{t-t_0}r_t$ (the discounted cumulative reward) with the discount $0 < \\gamma < 1$\n",
    "\n",
    "**Q-learning** in particular assumes the existence of a function $Q^\\star: \\text{State}\\times\\text{Action}\\rightarrow \\mathbb{R}$ that approximates the return given a state and an action.\n",
    "\n",
    "Hence, one can construct a policy that maximises the expected reward: $\\pi^\\star(s)=\\text{argmax}_aQ^\\star(s, a)$\n",
    "\n",
    "We initialise this **Q-table** and then update it at each training step as so: $Q^\\pi(s,a)=r + \\gamma Q^\\pi(s', \\pi(s'))$\n",
    "\n",
    "Define the temporal difference error $\\delta$ as $\\delta = Q(s, a) - (r + \\gamma \\text{max}_aQ(s',a))$\n",
    "\n",
    "Minimise error using *Huber loss*, so $\\mathcal{L} = \\frac{1}{|B|}_{(s, a, s', r)\\in B}\\sum \\mathcal{L}(\\delta)$ where $\\mathcal{L}(\\delta) = \\frac{1}{2}\\delta^2$ if $\\delta|\\leq 1$ or $|\\delta| - \\frac{1}{2}$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from envs.dsdp import DiscreteStochasticDecisionProcess\n",
    "\n",
    "env = DiscreteStochasticDecisionProcess()\n",
    "\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (state, action) -> (next_state, reward, done)\n",
    "transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# replay memory D with capacity N\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # implemented as a cyclical queue\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.95\n",
    "\n",
    "def one_hot(n, v):\n",
    "    a = np.zeros(n)\n",
    "    a[v] = 1.0\n",
    "    return a\n",
    "\n",
    "def rev_one_hot(a):\n",
    "    return np.where(a > 0)[0][0]\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputs, outputs, mem_len = 100000):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.head = nn.Linear(128, outputs)\n",
    "        \n",
    "        self.memory = ReplayMemory(mem_len)\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.n_actions = outputs\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.EPS_START = 1\n",
    "        self.EPS_END = 0.1\n",
    "        self.EPS_DECAY = 50000 # in number of steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.head(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # With probability eps select a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "        # otherwise select action = maxa Q∗(φ(st), a; θ)\n",
    "        with torch.no_grad():\n",
    "            return np.argmax(self(state)).view(1,1)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # in the form (state, action) -> (next_state, reward)\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        for state, action, next_state, reward, done in transitions:\n",
    "            # Set yj = rj for terminal φj+1\n",
    "            updated_q_val = reward\n",
    "            \n",
    "            # Set yj = rj + γ maxa0 Q(φj+1, a0; θ) for non-terminal φj+1\n",
    "            if done is not None: # ie if not terminal state\n",
    "                updated_q_val = reward + GAMMA * self(next_state).max(0)[0]\n",
    "                \n",
    "            q_table = self(state)\n",
    "            q_table[action] = updated_q_val\n",
    "            \n",
    "            old_q_table = self(state)\n",
    "            \n",
    "            # Huber loss\n",
    "            loss = F.smooth_l1_loss(old_q_table, q_table)\n",
    "            \n",
    "            # fit the model\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "class HDQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_goals, n_actions):\n",
    "        super(HDQN, self).__init__()\n",
    "        # Optimizer\n",
    "        learning_rate = 2.5e-4\n",
    "        \n",
    "        self.meta_controller = DQN(n_observations, n_goals).to(device)\n",
    "        self.meta_controller.optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n",
    "        self.controller = DQN(n_goals + n_observations, n_actions).to(device)\n",
    "        self.controller.optimizer = optim.RMSprop(self.controller.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.n_observations = n_observations\n",
    "        self.n_goals = n_goals\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def store_controller(self, *args):\n",
    "        self.controller.memory.store(*args)\n",
    "    \n",
    "    def store_meta_controller(self, *args):\n",
    "        self.meta_controller.memory.store(*args)\n",
    "    \n",
    "    def select_goal(self, external_observation):\n",
    "        return self.meta_controller.act(external_observation)\n",
    "        \n",
    "    def select_action(self, joint_goal_obs):\n",
    "        return self.controller.act(joint_goal_obs)\n",
    "    \n",
    "    def intrinsic_reward(self, goal, state):\n",
    "        if goal == state:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        self.meta_controller.experience_replay()\n",
    "        self.controller.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal: tensor([[3]])\n",
      "action: 1\n",
      "action: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2d5ae00a46e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mhdqnAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Store transition for meta controller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7f4e0d6a0a65>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_controller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7f4e0d6a0a65>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mupdated_q_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mq_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdated_q_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7f4e0d6a0a65>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get number of actions and observations from gym action space\n",
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]\n",
    "\n",
    "# Initialize action-value function Q with random weights\n",
    "hdqnAgent = HDQN(n_observations, n_observations, n_actions).to(device)\n",
    "\n",
    "num_episodes = 10000 # M\n",
    "episode_durations = []\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    state = torch.from_numpy(observation).float()\n",
    "    \n",
    "    overall_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # select a goal\n",
    "        goal = hdqnAgent.select_goal(state)\n",
    "        print(f\"goal: {goal}\")\n",
    "        encoded_goal = one_hot(n_observations, [goal.item()])\n",
    "        \n",
    "        goal_done = False\n",
    "        total_extrinsic = 0\n",
    "        while not done and not goal_done:\n",
    "            \n",
    "            joint_goal_state = torch.from_numpy(np.concatenate([encoded_goal, state])).float()\n",
    "            \n",
    "            # Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n",
    "            action = hdqnAgent.select_action(joint_goal_state)\n",
    "            \n",
    "            print(f\"action: {action.item()}\")\n",
    "            observation, reward, done, _ = env.step(action.item())\n",
    "            extrinsic_reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            overall_reward += reward\n",
    "            total_extrinsic += reward\n",
    "            \n",
    "            intrinsic_reward = torch.tensor([hdqnAgent.intrinsic_reward(goal, rev_one_hot(observation))], device=device)\n",
    "\n",
    "            # preprocess φ_{t+1} = φ(s_{t+1})\n",
    "            next_state = torch.from_numpy(observation).float()\n",
    "            joint_next_state = torch.from_numpy(np.concatenate([encoded_goal, next_state])).float()\n",
    "\n",
    "            goal_done = (rev_one_hot(observation) == goal)\n",
    "                \n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            hdqnAgent.store_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            hdqnAgent.experience_replay()\n",
    "            \n",
    "        # Store transition for meta controller\n",
    "        hdqnAgent.store_meta_controller(state, action, next_state, torch.tensor([total_extrinsic], device=device), done)\n",
    "        hdqnAgent.experience_replay()\n",
    "        \n",
    "    episode_durations.append(overall_reward)\n",
    "    plot_durations()\n",
    "\n",
    "print('Complete')\n",
    "plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
