{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(episode_durations, state_visits):\n",
    "    fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "    \n",
    "    durations_t, durations = list(map(list, zip(*episode_durations)))\n",
    "    durations = torch.tensor(durations, dtype=torch.float)\n",
    "    visits_t = [(x[0], torch.tensor(x[1], dtype=torch.float)) for x in state_visits.items()]\n",
    "    \n",
    "    fig.suptitle('Training')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Reward')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('State Visits')\n",
    "    \n",
    "    axs[0].plot(durations_t, durations.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 10:\n",
    "        means = durations.unfold(0, 10, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(9), means))\n",
    "        axs[0].plot(durations_t, means.numpy())\n",
    "    \n",
    "    if len(durations_t) >= 10:\n",
    "        for t in visits_t:\n",
    "            means = t[1].unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            axs[1].plot(means.numpy(), label=f\"State {t[0]}\")\n",
    "            axs[1].legend()\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (state, action) -> (next_state, reward, done)\n",
    "transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# replay memory D with capacity N\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # implemented as a cyclical queue\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norms(episode_durations):\n",
    "    plt.figure(2, figsize=(10,10))\n",
    "    \n",
    "    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n",
    "    \n",
    "    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n",
    "    plt.xlabel('L2 Norm')\n",
    "    plt.ylabel('Average Reward')\n",
    "    \n",
    "    mu = np.mean(ys, axis=1)\n",
    "    plt.plot(x / 10, mu)\n",
    "    stds = np.std(ys, axis = 1)\n",
    "    plt.fill_between(x / 10, mu + stds , mu - stds, alpha=0.2)\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.95\n",
    "\n",
    "def one_hot(n, v):\n",
    "    a = np.zeros(n)\n",
    "    a[v] = 1.0\n",
    "    return np.expand_dims(a, axis=0)\n",
    "\n",
    "def rev_one_hot(a):\n",
    "    return np.where(a[0] > 0)[0][0]\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputs, outputs, mem_len = 10000):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.head = nn.Linear(256, outputs)\n",
    "        \n",
    "        self.memory = ReplayMemory(mem_len)\n",
    "        self.optimizer = None\n",
    "        self.target = None # to keep parameters frozen while propogating losses\n",
    "        \n",
    "        self.n_actions = outputs\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.EPS_START = 1.0\n",
    "        self.EPS_END = 0.0\n",
    "        self.EPS_DECAY = 1000 #50000 # in number of steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.head(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # With probability eps select a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "        # otherwise select action = maxa Q∗(φ(st), a; θ)\n",
    "        with torch.no_grad():\n",
    "            return self(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # in the form (state, action) -> (next_state, reward, done)\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_mask = np.array(batch.done)\n",
    "        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n",
    "        \n",
    "        current_Q_values = self(state_batch).gather(1, action_batch)\n",
    "        # Compute next Q value based on which goal gives max Q values\n",
    "        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "        next_max_q = self.target(next_state_batch).detach().max(1)[0]\n",
    "        next_Q_values = not_done_mask * next_max_q\n",
    "        # Compute the target of the current Q values\n",
    "        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n",
    "        # Compute Bellman error (using Huber loss)\n",
    "        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Get number of actions and observations from gym action space\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = env.observation_space.shape[0]\n",
    "\n",
    "    # Initialize action-value function Q with random weights\n",
    "    dqnAgent = DQN(n_observations, n_actions).to(device)\n",
    "    dqnAgent.target = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 2.5e-4\n",
    "    dqnAgent.optimizer = optim.RMSprop(dqnAgent.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_episodes = 200 # M\n",
    "    episode_durations = []\n",
    "    state_visits = {0: [], 1: []}\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        # unsqueeze adds batch dimension\n",
    "        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "\n",
    "        overall_reward = 0\n",
    "        ep_state_visits = {0: 0, 1: 0}\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n",
    "            action = dqnAgent.act(state)\n",
    "            observation, reward, done, _ = env.step(action.item())\n",
    "            extrinsic_reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            overall_reward += reward\n",
    "\n",
    "            # preprocess φ_{t+1} = φ(s_{t+1})\n",
    "            next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            dqnAgent.memory.store(state, action, next_state, extrinsic_reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            dqnAgent.experience_replay()\n",
    "\n",
    "        # very needed! see https://stackoverflow.com/a/58730298\n",
    "        if i_episode % 10 == 0:\n",
    "            dqnAgent.target.load_state_dict(dqnAgent.state_dict(), strict = False)\n",
    "\n",
    "        episode_durations.append((i_episode, overall_reward))\n",
    "        for i in ep_state_visits.keys():\n",
    "            state_visits[i].append(ep_state_visits[i])\n",
    "        plot_durations(episode_durations, state_visits)\n",
    "\n",
    "    return dqnAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
