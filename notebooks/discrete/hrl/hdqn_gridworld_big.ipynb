{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from envs.gridworld import GridWorld\n",
    "\n",
    "env = GridWorld(5, 2)\n",
    "\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(episode_durations, internal_dur, goal_hit):\n",
    "    fig, axs = plt.subplots(3, figsize=(10,10))\n",
    "    \n",
    "    durations_t, durations = list(map(list, zip(*episode_durations)))\n",
    "    durations = torch.tensor(durations, dtype=torch.float)\n",
    "    \n",
    "    fig.suptitle('Training')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Reward')\n",
    "    axs[1].set_xlabel('Step')\n",
    "    axs[1].set_ylabel('Internal reward')\n",
    "    axs[2].set_xlabel('Step')\n",
    "    axs[2].set_ylabel('Goal hit')\n",
    "    \n",
    "    axs[0].plot(durations_t, durations.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axs[0].plot(durations_t, means.numpy())\n",
    "        \n",
    "    durations_t, durations = list(map(list, zip(*internal_dur)))\n",
    "    durations = torch.tensor(durations, dtype=torch.float)\n",
    "    #axs[1].plot(durations_t, durations.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 1000:\n",
    "        means = durations.unfold(0, 1000, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(999), means))\n",
    "        axs[1].plot(durations_t, means.numpy())\n",
    "    \n",
    "    durations_t, durations = list(map(list, zip(*goal_hit)))\n",
    "    durations = torch.tensor(durations, dtype=torch.float)\n",
    "    axs[2].plot(durations_t, durations.numpy())\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axs[2].plot(durations_t, means.numpy())\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model, episode_durations, internal_dur, goal_hit):\n",
    "    model_save_name = 'saved_models/hdqn_gridworld_5x2.pt'\n",
    "    path = f\"{model_save_name}\" \n",
    "\n",
    "    torch.save({\n",
    "        'steps': epoch,\n",
    "        'episode_durations': episode_durations,\n",
    "        'internal_dur': internal_dur,\n",
    "        'goal_hit': goal_hit,\n",
    "        'meta_controller_state_dict': model.meta_controller.state_dict(),\n",
    "        'controller_state_dict': model.controller.state_dict(),\n",
    "        'meta_controller_memory': model.meta_controller.memory,\n",
    "        'controller_memory': model.controller.memory\n",
    "    }, path)\n",
    "    \n",
    "def load_model(model, name='saved_models/hdqn_gridworld_5x2.pt'):\n",
    "    model_save_name = name\n",
    "    path = f\"{model_save_name}\" \n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    model.meta_controller.load_state_dict(checkpoint['meta_controller_state_dict'])\n",
    "    model.controller.load_state_dict(checkpoint['controller_state_dict'])\n",
    "    \n",
    "    model.meta_controller_target.load_state_dict(model.meta_controller.state_dict())\n",
    "    model.controller_target.load_state_dict(model.controller.state_dict())\n",
    "    \n",
    "    model.meta_controller.memory = checkpoint['meta_controller_memory']\n",
    "    model.controller.memory = checkpoint['controller_memory']\n",
    "    \n",
    "    model.meta_controller.pretrain = False\n",
    "\n",
    "    # model.eval() for evaluation instead\n",
    "    model.train()\n",
    "    \n",
    "    # todo: copy checkpoint things and then do\n",
    "    # del checkpoint\n",
    "    \n",
    "    steps, episode_durations, internal_dur, goal_hit = np.array(checkpoint['steps'], copy=True), np.array(checkpoint['episode_durations'], copy=True), np.array(checkpoint['internal_dur'], copy=True), np.array(checkpoint['goal_hit'], copy=True)\n",
    "    del checkpoint\n",
    "    return steps, episode_durations, internal_dur, goal_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (state, action) -> (next_state, reward, done)\n",
    "transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# replay memory D with capacity N\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # implemented as a cyclical queue\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputs, outputs, mem_len = 100000):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.head = nn.Linear(128, outputs)\n",
    "        \n",
    "        self.memory = ReplayMemory(mem_len)\n",
    "\n",
    "        self.n_actions = outputs\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.EPS_START = 1\n",
    "        self.EPS_END = 0.1\n",
    "        self.EPS_DECAY = 50000 # in number of steps\n",
    "        \n",
    "        self.pretrain = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.head(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.pretrain:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "        \n",
    "        #eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n",
    "        eps_threshold = 0.2\n",
    "            \n",
    "        self.steps_done += 1\n",
    "\n",
    "        # With probability eps select a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "        # otherwise select action = maxa Q∗(φ(st), a; θ)\n",
    "        with torch.no_grad():\n",
    "            return self(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    def experience_replay(self, optimizer, target):\n",
    "        if len(self.memory) < BATCH_SIZE or self.pretrain:\n",
    "            return\n",
    "        \n",
    "        # in the form (state, action) -> (next_state, reward, done)\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_mask = np.array(batch.done)\n",
    "        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n",
    "        \n",
    "        current_Q_values = self(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_max_q = target(next_state_batch).max(1)[0].detach()\n",
    "        \n",
    "        next_Q_values = not_done_mask * next_max_q\n",
    "        \n",
    "        # Compute the target of the current Q values\n",
    "        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n",
    "        \n",
    "        # Compute Bellman error (using Huber loss)\n",
    "        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "class HDQN(nn.Module):\n",
    "    def __init__(self, inputs, positions, outputs):\n",
    "        super(HDQN, self).__init__()\n",
    "        \n",
    "        self.meta_controller = DQN(inputs, positions, mem_len = 10000 * 200).to(device)\n",
    "        self.meta_controller.EPS_DECAY = 5000\n",
    "        self.meta_controller.pretrain = True\n",
    "        self.meta_controller_optimizer = optim.Adam(self.meta_controller.parameters(), lr = 0.001)\n",
    "        self.meta_controller_target = DQN(inputs, positions, mem_len = 0).to(device)\n",
    "        self.meta_controller_target.eval()\n",
    "        \n",
    "        # append goal state to inputs\n",
    "        self.controller = DQN(inputs + 1, outputs, mem_len = 10000 * 200).to(device)\n",
    "        self.controller_optimizer = optim.Adam(self.controller.parameters(), lr = 0.001)\n",
    "        self.controller_target = DQN(inputs + 1, outputs, mem_len = 0).to(device)\n",
    "        self.controller_target.eval()\n",
    "    \n",
    "    def store_controller(self, *args):\n",
    "        self.controller.memory.store(*args)\n",
    "    \n",
    "    def store_meta_controller(self, *args):\n",
    "        self.meta_controller.memory.store(*args)\n",
    "    \n",
    "    def select_goal(self, external_observation):\n",
    "        return self.meta_controller.act(external_observation)\n",
    "        \n",
    "    def select_action(self, joint_goal_obs):\n",
    "        return self.controller.act(joint_goal_obs)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n",
    "        self.controller.experience_replay(self.controller_optimizer, self.controller_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_model():\n",
    "    # Get number of actions and observations from gym action space\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = env.observation_space.shape[0]\n",
    "    n_positions = env.num_positions\n",
    "\n",
    "    # Initialize action-value function Q with random weights\n",
    "    hdqnAgent = HDQN(n_observations, n_positions, n_actions).to(device)\n",
    "\n",
    "    num_episodes = 10000 # M\n",
    "    episode_durations = []\n",
    "    internal_dur = []\n",
    "    goal_hit = []\n",
    "    last_saved = 0\n",
    "    steps = 0\n",
    "    \n",
    "    #steps, episode_durations, internal_dur, goal_hit = load_model(hdqnAgent)\n",
    "    #last_ep = episode_durations[-1][0]\n",
    "    #last_saved = last_ep\n",
    "    \n",
    "    #for i_episode in range(last_ep + 1, last_ep + num_episodes):\n",
    "    for i_episode in range(0, num_episodes):\n",
    "        if i_episode >= 1000:\n",
    "            hdqnAgent.meta_controller.pretrain = False\n",
    "        \n",
    "        observation = env.reset()\n",
    "\n",
    "        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "\n",
    "        overall_reward = 0\n",
    "        done = False\n",
    "        goals_hit = 0\n",
    "        while not done:\n",
    "            # select a goal\n",
    "            goal = hdqnAgent.select_goal(state)\n",
    "            goal_i = goal.item()\n",
    "\n",
    "            goal_done = False\n",
    "            total_extrinsic = 0\n",
    "            s_0 = state\n",
    "            while not done and not goal_done:\n",
    "                joint_goal_state = torch.cat([goal, state], axis=1)\n",
    "                \n",
    "                action = hdqnAgent.select_action(joint_goal_state)\n",
    "                action_i = action.item()\n",
    "\n",
    "                observation, reward, done, _ = env.step(action_i)\n",
    "                steps += 1\n",
    "                extrinsic_reward = torch.tensor([reward], device=device, dtype=torch.float)\n",
    "                \n",
    "                #display.clear_output(wait=True)\n",
    "                #print(observation, reward)\n",
    "                #env.render()\n",
    "                #time.sleep(0.3)\n",
    "\n",
    "                overall_reward += reward\n",
    "                total_extrinsic += reward\n",
    "\n",
    "                # preprocess φ_{t+1} = φ(s_{t+1})\n",
    "                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "                joint_next_state = torch.cat([goal, next_state], axis=1)\n",
    "\n",
    "                # check that goal was achieved (first index of observation is player position)\n",
    "                goal_done = (goal_i == observation[0])\n",
    "                \n",
    "                if goal_done:\n",
    "                    goals_hit += 1\n",
    "                \n",
    "                # calculate intrinsic reward as distance from player to goal\n",
    "                opc = env.pos_to_coord(state[0][0].item())\n",
    "                npc = env.pos_to_coord(observation[0])\n",
    "                gc = env.pos_to_coord(goal_i)\n",
    "                #intrinsic_reward = 1.0 if math.dist(npc, gc) <= math.dist(opc, gc) else -1.0 \n",
    "                #intrinsic_reward = 1.0 / (1 + math.dist(npc, gc))\n",
    "                intrinsic_reward = 1.0 if goal_done else -1.0\n",
    "                internal_dur.append((steps, intrinsic_reward))\n",
    "\n",
    "                intrinsic_reward = torch.tensor([intrinsic_reward], device=device, dtype=torch.float)\n",
    "\n",
    "                # Store transition (φt, at, rt, φt+1) in D\n",
    "                hdqnAgent.store_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                hdqnAgent.experience_replay()\n",
    "\n",
    "            # Store transition for meta controller\n",
    "            if not hdqnAgent.meta_controller.pretrain:\n",
    "                hdqnAgent.store_meta_controller(s_0, goal, next_state, torch.tensor([total_extrinsic], device=device, dtype=torch.float), done)\n",
    "                hdqnAgent.experience_replay()\n",
    "                \n",
    "        if i_episode % 10 == 0:\n",
    "            hdqnAgent.meta_controller_target.load_state_dict(hdqnAgent.meta_controller.state_dict())\n",
    "            hdqnAgent.controller_target.load_state_dict(hdqnAgent.controller.state_dict())\n",
    "            \n",
    "        episode_durations.append((i_episode, overall_reward))\n",
    "        goal_hit.append((i_episode, goals_hit))\n",
    "        \n",
    "        if i_episode - last_saved >= 1000:\n",
    "            last_saved = i_episode\n",
    "            #save_model(steps, hdqnAgent, episode_durations, internal_dur, goal_hit)\n",
    "        \n",
    "        plot_durations(episode_durations, internal_dur, goal_hit)\n",
    "\n",
    "    return hdqnAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(hdqnAgent, episode_durations):\n",
    "    hdqnAgent.eval()\n",
    "    hdqnAgent.meta_controller.eval()\n",
    "    hdqnAgent.controller.eval()\n",
    "\n",
    "    num_episodes = 100\n",
    "\n",
    "    for l2norm in range(20):\n",
    "\n",
    "        overall_reward = 0\n",
    "        for i_episode in range(num_episodes):\n",
    "            observation = env.reset()\n",
    "\n",
    "            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "            #state = state * (np.ones(3) + (l2norm/10)*(np.random.random(3) - 0.5))\n",
    "            #state = state.float()\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                # select a goal\n",
    "                goal = hdqnAgent.select_goal(state)\n",
    "                goal_i = goal.item()\n",
    "\n",
    "                goal_done = False\n",
    "                while not done and not goal_done:\n",
    "                    joint_goal_state = torch.cat([goal, state], axis=1)\n",
    "\n",
    "                    action = hdqnAgent.select_action(joint_goal_state)\n",
    "                    action_i = action.item()\n",
    "                    observation, reward, done, _ = env.step(action_i)\n",
    "\n",
    "                    overall_reward += reward\n",
    "\n",
    "                    goal_done = (goal_i == observation[0])\n",
    "\n",
    "                    state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "                    #state = state * (np.ones(3) + (l2norm/10)*(np.random.random(3) - 0.5))\n",
    "                    #state = state.float()\n",
    "\n",
    "        episode_durations[l2norm].append(overall_reward / num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-68213b4dcdc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-53ae283c8fad>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mhdqnAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m# Store transition for meta controller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-556ec6f164e9>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_controller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_controller_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_controller_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontroller_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontroller_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-556ec6f164e9>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self, optimizer, target)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mnext_state_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intrinsic_reward = 1.0 if goal_done else -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_positions = env.num_positions\n",
    "\n",
    "# Initialize action-value function Q with random weights\n",
    "hdqnAgent = HDQN(n_observations, n_positions, n_actions).to(device)\n",
    "steps, episode_durations, internal_dur, goal_hit = load_model(hdqnAgent, 'saved_models/hdqn_gridworld_5x2_8k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(0, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norms(episode_durations):\n",
    "    plt.figure(2, figsize=(10,10))\n",
    "    \n",
    "    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n",
    "    \n",
    "    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n",
    "    plt.xlabel('L2 Norm')\n",
    "    plt.ylabel('Average Reward')\n",
    "    \n",
    "    mu = np.mean(ys, axis=1)\n",
    "    plt.plot(x / 10, mu)\n",
    "    stds = np.std(ys, axis = 1)\n",
    "    plt.fill_between(x / 10, mu + stds , mu - stds, alpha=0.2)\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = {}\n",
    "for l2norm in range(20):\n",
    "    episode_durations[l2norm] = []\n",
    "\n",
    "eval_model(agent, episode_durations)\n",
    "plot_norms(episode_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
