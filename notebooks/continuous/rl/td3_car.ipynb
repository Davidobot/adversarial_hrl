{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5016,
     "status": "ok",
     "timestamp": 1605608260658,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "CJMjXntuErvs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1605608266322,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "ESCbXyTAQHNs"
   },
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1605608267909,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "MRSC05Y-Erv0"
   },
   "outputs": [],
   "source": [
    "env = NormalizedEnv(gym.make('MountainCarContinuous-v0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiZFY63MErv3"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1605608269476,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "DQtcj2j8Erv4"
   },
   "outputs": [],
   "source": [
    "def plot_durations(episode_durations):\n",
    "    fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "    \n",
    "    durations_t, durations = list(map(list, zip(*episode_durations)))\n",
    "    durations = torch.tensor(durations, dtype=torch.float)\n",
    "    \n",
    "    fig.suptitle('Training')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Reward')\n",
    "    \n",
    "    axs[0].plot(durations_t, durations.numpy())\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1605608270927,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "HyQnUb6KErv6"
   },
   "outputs": [],
   "source": [
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1605609036587,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "CWIkep5aErv9"
   },
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1605609038230,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "ZtW05marErwA"
   },
   "outputs": [],
   "source": [
    "# (state, action) -> (next_state, reward, done)\n",
    "transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# replay memory D with capacity N\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # implemented as a cyclical queue\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrMrvwO1ErwC"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1605609196057,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "0oyBjK1AErwD"
   },
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.head = nn.Linear(256, nb_actions)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights(3e-3)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.head.weight.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.tanh(self.head(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(nb_states + nb_actions, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(nb_states + nb_actions, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1).float()\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1).float()\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1605609437938,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "u-9mozrWErwG"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "\n",
    "# https://spinningup.openai.com/en/latest/algorithms/td3.html\n",
    "class TD3(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions):\n",
    "        super(TD3, self).__init__()\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "        \n",
    "        self.actor = Actor(self.nb_states, self.nb_actions)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions)\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions)\n",
    "        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "        \n",
    "        #Create replay buffer\n",
    "        self.memory = ReplayMemory(2000000)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.tau = 0.005\n",
    "        self.depsilon = 1.0 / 50000\n",
    "        self.policy_noise=0.2\n",
    "        self.noise_clip=0.5\n",
    "        self.policy_freq=2\n",
    "        self.total_it = 0\n",
    "\n",
    "        # \n",
    "        self.epsilon = 1.0\n",
    "        self.is_training = True\n",
    "\n",
    "    def update_policy(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        self.total_it += 1\n",
    "        \n",
    "        # in the form (state, action) -> (next_state, reward, done)\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_mask = np.array(batch.done)\n",
    "        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n",
    "\n",
    "        # Target Policy Smoothing\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action_batch) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip).float()\n",
    "            \n",
    "            next_action = (\n",
    "                self.actor_target(next_state_batch) + noise\n",
    "            ).clamp(-1.0, 1.0).float()\n",
    "\n",
    "            # Compute the target Q value\n",
    "            # Clipped Double-Q Learning\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n",
    "            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n",
    "        \n",
    "        # Critic update\n",
    "        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n",
    "      \n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n",
    "            \n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Target update\n",
    "            soft_update(self.actor_target, self.actor, self.tau)\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def observe(self, s_t, a_t, s_t1, r_t, done):\n",
    "        self.memory.store(s_t, a_t, s_t1, r_t, done)\n",
    "\n",
    "    def random_action(self):\n",
    "        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n",
    "\n",
    "    def select_action(self, s_t, warmup=True, decay_epsilon=True):\n",
    "        if warmup:\n",
    "            return self.random_action()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(s_t).squeeze(0)\n",
    "            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n",
    "            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n",
    "            action = torch.clamp(action, -1., 1.)\n",
    "\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "            if decay_epsilon:\n",
    "                self.epsilon -= self.depsilon\n",
    "            \n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1605609438486,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "w7_KKbeSErwI"
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    n_observations = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    \n",
    "    agent = TD3(n_observations, n_actions).to(device)\n",
    "    \n",
    "    max_episode_length = 500\n",
    "    \n",
    "    agent.is_training = True\n",
    "    episode_reward = 0.\n",
    "    observation = None\n",
    "    \n",
    "    warmup = 10\n",
    "    num_episodes = 500 # M\n",
    "    episode_durations = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        overall_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # agent pick action ...\n",
    "            action = agent.select_action(state, i_episode <= warmup)\n",
    "\n",
    "            # env response with next_observation, reward, terminate_info\n",
    "            observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n",
    "            steps += 1\n",
    "            \n",
    "            if max_episode_length and episode_steps >= max_episode_length -1:\n",
    "                done = True\n",
    "                \n",
    "            extrinsic_reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            overall_reward += reward\n",
    "            \n",
    "            next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n",
    "\n",
    "            # agent observe and update policy\n",
    "            agent.observe(state, action, next_state, extrinsic_reward, done)\n",
    "\n",
    "            episode_steps += 1            \n",
    "            state = next_state\n",
    "            \n",
    "            if i_episode > warmup:\n",
    "                agent.update_policy()\n",
    "\n",
    "        episode_durations.append((steps, overall_reward))\n",
    "        plot_durations(episode_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 843707,
     "status": "ok",
     "timestamp": 1605610282270,
     "user": {
      "displayName": "D.G. Khachaturov",
      "photoUrl": "",
      "userId": "02831277067458900124"
     },
     "user_tz": 0
    },
    "id": "jazaVnJNErwK",
    "outputId": "13cd954f-6d47-430d-a20f-f3631ec69b6b"
   },
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5kgVRwJErwO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "td3_pendulum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
