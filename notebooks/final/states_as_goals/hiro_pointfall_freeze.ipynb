{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointfall_freeze.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1618854868540,"user_tz":-60,"elapsed":42067,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"559ba0fe-a7a4-4735-fd57-939ecb8a5cbe"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"," \n","!cp \"/content/drive/My Drive/Dissertation/envs/point_fall.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QElqDR-kwV3i","executionInfo":{"status":"ok","timestamp":1611605713573,"user_tz":-60,"elapsed":6555,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"21a0a7c6-f2ac-4185-908f-e3c0fe16f08f"},"source":["# для прикола\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Jan 25 20:15:11 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1618854903980,"user_tz":-60,"elapsed":1901,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/exp_point_fall/{name}\" \n","\n","    torch.save({\n","      'meta_controller': {\n","          'critic': model.meta_controller.critic.state_dict(),\n","          'actor': model.meta_controller.actor.state_dict(),\n","      },\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/exp_point_fall/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1618854907374,"user_tz":-60,"elapsed":4744,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1618854907377,"user_tz":-60,"elapsed":4530,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1618854907395,"user_tz":-60,"elapsed":4355,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["from point_fall import PointFallEnv \n","env = NormalizedEnv(PointFallEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1618854907398,"user_tz":-60,"elapsed":3588,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def plot_durations(episode_durations, goals_done):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*goals_done)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Goals done')\n","    \n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1618854907406,"user_tz":-60,"elapsed":3250,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1618854907409,"user_tz":-60,"elapsed":2767,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1618854907411,"user_tz":-60,"elapsed":2420,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1618854907773,"user_tz":-60,"elapsed":1822,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1618854908092,"user_tz":-60,"elapsed":1697,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 20000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1618854908094,"user_tz":-60,"elapsed":919,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        self.meta_controller = TD3(nb_states, len(self.goal_dim), True).to(device)\n","        self.max_goal_dist = torch.from_numpy(np.array([2., 3.])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0.5, 1.5])).to(device)\n","        #self.meta_controller.depsilon = 1.0 / 10000\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.update_policy(self.off_policy_corrections)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    # correct goals to allow for use in experience replay\n","    def off_policy_corrections(self, sgoals, states, actions, candidate_goals=8):\n","        first_s = [s[0] for s in states] # First x\n","        last_s = [s[-1] for s in states] # Last x\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # diff = 1\n","        diff_goal = (np.array(last_s) - np.array(first_s))[:, np.newaxis, :self.goal_dimen]\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # original = 1\n","        # random = candidate_goals\n","        scale = self.max_goal_dist.cpu().numpy()\n","        original_goal = np.array(sgoals)[:, np.newaxis, :]\n","        random_goals = np.random.normal(loc=diff_goal, scale=.5*scale,\n","                                        size=(BATCH_SIZE, candidate_goals, original_goal.shape[-1]))\n","        random_goals = random_goals.clip(-scale, scale)\n","\n","        # Shape: (batch_size, 10, subgoal_dim)\n","        candidates = np.concatenate([original_goal, diff_goal, random_goals], axis=1)\n","        #states = np.array(states)[:, :-1, :]\n","        actions = np.array(actions)\n","        seq_len = len(states[0])\n","\n","        # For ease\n","        new_batch_sz = seq_len * BATCH_SIZE\n","        action_dim = actions[0][0].shape\n","        obs_dim = states[0][0].shape\n","        ncands = candidates.shape[1]\n","\n","        true_actions = actions.reshape((new_batch_sz,) + action_dim)\n","        observations = states.reshape((new_batch_sz,) + obs_dim)\n","        goal_shape = (new_batch_sz, self.goal_dimen)\n","        # observations = get_obs_tensor(observations, sg_corrections=True)\n","\n","        # batched_candidates = np.tile(candidates, [seq_len, 1, 1])\n","        # batched_candidates = batched_candidates.transpose(1, 0, 2)\n","\n","        policy_actions = np.zeros((ncands, new_batch_sz) + action_dim)\n","\n","        observations = torch.from_numpy(observations).to(device)\n","        for c in range(ncands):\n","            subgoal = candidates[:,c]\n","            candidate = (subgoal + states[:, 0, :self.goal_dimen])[:, None] - states[:, :, :self.goal_dimen]\n","            candidate = candidate.reshape(*goal_shape)\n","            policy_actions[c] = self.controller.actor(torch.cat([observations, torch.from_numpy(candidate).to(device)], 1).float()).detach().cpu().numpy()\n","\n","        difference = (policy_actions - true_actions)\n","        difference = np.where(difference != -np.inf, difference, 0)\n","        difference = difference.reshape((ncands, BATCH_SIZE, seq_len) + action_dim).transpose(1, 0, 2, 3)\n","\n","        logprob = -0.5*np.sum(np.linalg.norm(difference, axis=-1)**2, axis=-1)\n","        max_indices = np.argmax(logprob, axis=-1)\n","\n","        return torch.from_numpy(candidates[np.arange(BATCH_SIZE), max_indices]).to(device).float()\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done, state_seq, action_seq)\n","\n","    def select_goal(self, s_t, warmup, decay_epsilon):\n","        return self.meta_controller.select_action(s_t, warmup, decay_epsilon) * self.max_goal_dist + self.goal_offset\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["SAVE_OFFSET = 5\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 200\n","    num_episodes = 8000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    freeze_ctrl = False\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                if not freeze_ctrl:\n","                    agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup and not freeze_ctrl:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 400 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 70 and not freeze_ctrl:\n","                print(f\"Freezing controller at episode {i_episode}!\")\n","                freeze_ctrl = True\n","                agent.controller.eval()\n","                agent.controller.is_training = False\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_freeze_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1618854922276,"user_tz":-60,"elapsed":6278,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data - eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    #g_state = torch.tensor(g_state, requires_grad=True)\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    if targeted:\n","        # initial forward pass\n","        goal = agent.meta_controller.actor(g_state)\n","        goal = torch.clamp(goal, -1., 1.)\n","\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = agent.meta_controller.critic.Q1(g_state, agent.meta_controller.actor(g_state)).mean()\n","\n","    agent.meta_controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal = agent.select_goal(og_state, False, False)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhC6f7N6sJoa","executionInfo":{"status":"ok","timestamp":1618872860952,"user_tz":-60,"elapsed":17940741,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"a854d3ce-fb31-477c-b04f-d0533102c3bc"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","\"\"\"\n","i = 2\n","while i < 3:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\"\"\"\n","\n","i = 0\n","while i < 5:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [95.70099999999228], 0.02: [84.54999999999183], 0.04: [90.00899999999086], 0.06: [-40.421999999967824], 0.08: [-48.61200000000063], 0.1: [-47.26499999999595], 0.12: [-29.207999999975332], 0.14: [-43.16699999997937], 0.16: [-44.524999999988715], 0.18: [-44.50299999998839], 0.2: [-46.02699999999054]}, 'action': {0.0: [95.69299999999227], 0.02: [89.71999999999012], 0.04: [42.05399999999496], 0.06: [8.640999999996112], 0.08: [-0.894000000003109], 0.1: [-19.009999999982178], 0.12: [0.16999999999668283], 0.14: [-29.857999999980684], 0.16: [-41.97799999997396], 0.18: [-32.772999999972996], 0.2: [-35.29899999997084]}}\n","0 fgsm (t): {'goal': {0.0: [95.69999999999227], 0.02: [89.50099999998993], 0.04: [-6.555000000005501], 0.06: [31.81300000001211], 0.08: [8.641999999997429], 0.1: [30.937000000013107], 0.12: [13.961999999998774], 0.14: [-10.205000000004935], 0.16: [-16.77299999999464], 0.18: [-10.622999999998337], 0.2: [-33.238999999971746]}, 'action': {0.0: [95.69499999999225], 0.02: [74.0759999999865], 0.04: [-17.3939999999953], 0.06: [19.739000000002747], 0.08: [-10.90500000000547], 0.1: [-32.99999999997202], 0.12: [-31.23599999997436], 0.14: [-32.162999999973856], 0.16: [-40.94899999997028], 0.18: [-47.581999999998786], 0.2: [-45.0249999999894]}}\n","1 fgsm (ut): {'goal': {0.0: [95.70099999999228, 79.33199999998178], 0.02: [84.54999999999183, 20.963000000014823], 0.04: [90.00899999999086, -10.136999999996377], 0.06: [-40.421999999967824, -14.760999999993055], 0.08: [-48.61200000000063, -4.553000000000265], 0.1: [-47.26499999999595, -10.208000000005665], 0.12: [-29.207999999975332, 11.034000000016745], 0.14: [-43.16699999997937, 3.786999999992897], 0.16: [-44.524999999988715, -26.25799999997594], 0.18: [-44.50299999998839, -34.94699999997272], 0.2: [-46.02699999999054, -14.821999999994159]}, 'action': {0.0: [95.69299999999227, 89.39399999998612], 0.02: [89.71999999999012, -0.10700000000323624], 0.04: [42.05399999999496, 21.25600000001414], 0.06: [8.640999999996112, -32.78899999997363], 0.08: [-0.894000000003109, -50.00000000000659], 0.1: [-19.009999999982178, -48.81200000000228], 0.12: [0.16999999999668283, -47.83700000000599], 0.14: [-29.857999999980684, -50.00000000000659], 0.16: [-41.97799999997396, -48.994000000003155], 0.18: [-32.772999999972996, -50.00000000000659], 0.2: [-35.29899999997084, -48.9350000000018]}}\n","1 fgsm (t): {'goal': {0.0: [95.69999999999227, 89.38199999998616], 0.02: [89.50099999998993, -42.99299999998309], 0.04: [-6.555000000005501, -42.719999999975556], 0.06: [31.81300000001211, -37.22899999997119], 0.08: [8.641999999997429, -27.314999999983925], 0.1: [30.937000000013107, -48.55500000000127], 0.12: [13.961999999998774, -50.00000000000659], 0.14: [-10.205000000004935, -48.70200000000095], 0.16: [-16.77299999999464, -47.38299999999524], 0.18: [-10.622999999998337, -47.35499999999969], 0.2: [-33.238999999971746, -50.00000000000659]}, 'action': {0.0: [95.69499999999225, 86.49099999998705], 0.02: [74.0759999999865, 5.922999999996289], 0.04: [-17.3939999999953, -17.464999999992354], 0.06: [19.739000000002747, -15.28599999999745], 0.08: [-10.90500000000547, -32.245999999975446], 0.1: [-32.99999999997202, -25.783999999977468], 0.12: [-31.23599999997436, -24.217999999975248], 0.14: [-32.162999999973856, -27.521999999972735], 0.16: [-40.94899999997028, -26.08199999997149], 0.18: [-47.581999999998786, -18.92999999998291], 0.2: [-45.0249999999894, -16.410999999974884]}}\n","2 fgsm (ut): {'goal': {0.0: [95.70099999999228, 79.33199999998178, 89.34099999998288], 0.02: [84.54999999999183, 20.963000000014823, -38.769999999974516], 0.04: [90.00899999999086, -10.136999999996377, -31.68099999997623], 0.06: [-40.421999999967824, -14.760999999993055, -48.64100000000073], 0.08: [-48.61200000000063, -4.553000000000265, -48.707000000000974], 0.1: [-47.26499999999595, -10.208000000005665, -47.211999999994624], 0.12: [-29.207999999975332, 11.034000000016745, -48.61200000000063], 0.14: [-43.16699999997937, 3.786999999992897, -45.939999999989084], 0.16: [-44.524999999988715, -26.25799999997594, -47.33499999999621], 0.18: [-44.50299999998839, -34.94699999997272, -43.548999999981525], 0.2: [-46.02699999999054, -14.821999999994159, -43.65799999998071]}, 'action': {0.0: [95.69299999999227, 89.39399999998612, 87.8269999999826], 0.02: [89.71999999999012, -0.10700000000323624, -0.9890000000015076], 0.04: [42.05399999999496, 21.25600000001414, 68.3009999999904], 0.06: [8.640999999996112, -32.78899999997363, -50.00000000000659], 0.08: [-0.894000000003109, -50.00000000000659, -50.00000000000659], 0.1: [-19.009999999982178, -48.81200000000228, -24.011999999985797], 0.12: [0.16999999999668283, -47.83700000000599, -42.325999999976396], 0.14: [-29.857999999980684, -50.00000000000659, -43.75799999998387], 0.16: [-41.97799999997396, -48.994000000003155, -47.44799999999548], 0.18: [-32.772999999972996, -50.00000000000659, -43.01299999997884], 0.2: [-35.29899999997084, -48.9350000000018, -34.219999999969126]}}\n","2 fgsm (t): {'goal': {0.0: [95.69999999999227, 89.38199999998616, 86.18099999998894], 0.02: [89.50099999998993, -42.99299999998309, -32.49999999997503], 0.04: [-6.555000000005501, -42.719999999975556, -14.566999999994666], 0.06: [31.81300000001211, -37.22899999997119, -5.636000000005297], 0.08: [8.641999999997429, -27.314999999983925, -45.85999999998993], 0.1: [30.937000000013107, -48.55500000000127, -44.60699999998788], 0.12: [13.961999999998774, -50.00000000000659, -39.06999999997241], 0.14: [-10.205000000004935, -48.70200000000095, -50.00000000000659], 0.16: [-16.77299999999464, -47.38299999999524, -50.00000000000659], 0.18: [-10.622999999998337, -47.35499999999969, -44.632999999984285], 0.2: [-33.238999999971746, -50.00000000000659, -47.70599999999642]}, 'action': {0.0: [95.69499999999225, 86.49099999998705, 86.94199999998466], 0.02: [74.0759999999865, 5.922999999996289, -18.392999999988835], 0.04: [-17.3939999999953, -17.464999999992354, -6.755000000005403], 0.06: [19.739000000002747, -15.28599999999745, -11.335000000004804], 0.08: [-10.90500000000547, -32.245999999975446, -19.768999999983013], 0.1: [-32.99999999997202, -25.783999999977468, -18.925999999979002], 0.12: [-31.23599999997436, -24.217999999975248, -33.011999999972495], 0.14: [-32.162999999973856, -27.521999999972735, -34.59399999997086], 0.16: [-40.94899999997028, -26.08199999997149, -21.192999999985126], 0.18: [-47.581999999998786, -18.92999999998291, -26.291999999978078], 0.2: [-45.0249999999894, -16.410999999974884, -14.10999999999559]}}\n","3 fgsm (ut): {'goal': {0.0: [95.70099999999228, 79.33199999998178, 89.34099999998288, 97.39799999999515], 0.02: [84.54999999999183, 20.963000000014823, -38.769999999974516, 77.20899999998562], 0.04: [90.00899999999086, -10.136999999996377, -31.68099999997623, 42.70600000000074], 0.06: [-40.421999999967824, -14.760999999993055, -48.64100000000073, 1.224999999998504], 0.08: [-48.61200000000063, -4.553000000000265, -48.707000000000974, -36.32999999997098], 0.1: [-47.26499999999595, -10.208000000005665, -47.211999999994624, -37.50799999996979], 0.12: [-29.207999999975332, 11.034000000016745, -48.61200000000063, -39.329999999970184], 0.14: [-43.16699999997937, 3.786999999992897, -45.939999999989084, -40.81899999996984], 0.16: [-44.524999999988715, -26.25799999997594, -47.33499999999621, -48.742000000002236], 0.18: [-44.50299999998839, -34.94699999997272, -43.548999999981525, -48.74600000000111], 0.2: [-46.02699999999054, -14.821999999994159, -43.65799999998071, -50.00000000000659]}, 'action': {0.0: [95.69299999999227, 89.39399999998612, 87.8269999999826, 97.35599999999494], 0.02: [89.71999999999012, -0.10700000000323624, -0.9890000000015076, 96.50899999999356], 0.04: [42.05399999999496, 21.25600000001414, 68.3009999999904, 57.29599999998309], 0.06: [8.640999999996112, -32.78899999997363, -50.00000000000659, 34.3980000000151], 0.08: [-0.894000000003109, -50.00000000000659, -50.00000000000659, 8.477999999995841], 0.1: [-19.009999999982178, -48.81200000000228, -24.011999999985797, -41.46999999997285], 0.12: [0.16999999999668283, -47.83700000000599, -42.325999999976396, -47.20899999999461], 0.14: [-29.857999999980684, -50.00000000000659, -43.75799999998387, -44.31999999998505], 0.16: [-41.97799999997396, -48.994000000003155, -47.44799999999548, -48.57100000000161], 0.18: [-32.772999999972996, -50.00000000000659, -43.01299999997884, -31.078999999977697], 0.2: [-35.29899999997084, -48.9350000000018, -34.219999999969126, -27.584999999981346]}}\n","3 fgsm (t): {'goal': {0.0: [95.69999999999227, 89.38199999998616, 86.18099999998894, 97.37399999999525], 0.02: [89.50099999998993, -42.99299999998309, -32.49999999997503, 76.72599999997898], 0.04: [-6.555000000005501, -42.719999999975556, -14.566999999994666, 2.4659999999983824], 0.06: [31.81300000001211, -37.22899999997119, -5.636000000005297, -44.44099999998386], 0.08: [8.641999999997429, -27.314999999983925, -45.85999999998993, -9.740000000000686], 0.1: [30.937000000013107, -48.55500000000127, -44.60699999998788, 6.834999999993646], 0.12: [13.961999999998774, -50.00000000000659, -39.06999999997241, 34.3290000000139], 0.14: [-10.205000000004935, -48.70200000000095, -50.00000000000659, 30.80700000001752], 0.16: [-16.77299999999464, -47.38299999999524, -50.00000000000659, 18.801000000008244], 0.18: [-10.622999999998337, -47.35499999999969, -44.632999999984285, -21.733999999976078], 0.2: [-33.238999999971746, -50.00000000000659, -47.70599999999642, -37.6399999999728]}, 'action': {0.0: [95.69499999999225, 86.49099999998705, 86.94199999998466, 97.3109999999956], 0.02: [74.0759999999865, 5.922999999996289, -18.392999999988835, 59.46599999998678], 0.04: [-17.3939999999953, -17.464999999992354, -6.755000000005403, 37.45000000001379], 0.06: [19.739000000002747, -15.28599999999745, -11.335000000004804, 4.6429999999965], 0.08: [-10.90500000000547, -32.245999999975446, -19.768999999983013, -30.307999999975912], 0.1: [-32.99999999997202, -25.783999999977468, -18.925999999979002, -4.38800000000003], 0.12: [-31.23599999997436, -24.217999999975248, -33.011999999972495, -10.271000000003793], 0.14: [-32.162999999973856, -27.521999999972735, -34.59399999997086, -29.173999999973454], 0.16: [-40.94899999997028, -26.08199999997149, -21.192999999985126, -29.582999999973136], 0.18: [-47.581999999998786, -18.92999999998291, -26.291999999978078, -34.03199999997133], 0.2: [-45.0249999999894, -16.410999999974884, -14.10999999999559, -40.24599999997108]}}\n","4 fgsm (ut): {'goal': {0.0: [95.70099999999228, 79.33199999998178, 89.34099999998288, 97.39799999999515, 88.58599999999434], 0.02: [84.54999999999183, 20.963000000014823, -38.769999999974516, 77.20899999998562, 83.28799999998984], 0.04: [90.00899999999086, -10.136999999996377, -31.68099999997623, 42.70600000000074, -34.21799999997151], 0.06: [-40.421999999967824, -14.760999999993055, -48.64100000000073, 1.224999999998504, -48.587000000000536], 0.08: [-48.61200000000063, -4.553000000000265, -48.707000000000974, -36.32999999997098, -48.663000000000814], 0.1: [-47.26499999999595, -10.208000000005665, -47.211999999994624, -37.50799999996979, -41.55099999997508], 0.12: [-29.207999999975332, 11.034000000016745, -48.61200000000063, -39.329999999970184, -40.267999999968715], 0.14: [-43.16699999997937, 3.786999999992897, -45.939999999989084, -40.81899999996984, -36.226999999975504], 0.16: [-44.524999999988715, -26.25799999997594, -47.33499999999621, -48.742000000002236, -35.06899999997096], 0.18: [-44.50299999998839, -34.94699999997272, -43.548999999981525, -48.74600000000111, -47.358999999995156], 0.2: [-46.02699999999054, -14.821999999994159, -43.65799999998071, -50.00000000000659, -41.92399999997264]}, 'action': {0.0: [95.69299999999227, 89.39399999998612, 87.8269999999826, 97.35599999999494, 82.63799999998238], 0.02: [89.71999999999012, -0.10700000000323624, -0.9890000000015076, 96.50899999999356, 65.82899999998813], 0.04: [42.05399999999496, 21.25600000001414, 68.3009999999904, 57.29599999998309, 11.278000000000837], 0.06: [8.640999999996112, -32.78899999997363, -50.00000000000659, 34.3980000000151, -38.65399999996966], 0.08: [-0.894000000003109, -50.00000000000659, -50.00000000000659, 8.477999999995841, -35.8689999999754], 0.1: [-19.009999999982178, -48.81200000000228, -24.011999999985797, -41.46999999997285, -48.5960000000017], 0.12: [0.16999999999668283, -47.83700000000599, -42.325999999976396, -47.20899999999461, -47.25799999999479], 0.14: [-29.857999999980684, -50.00000000000659, -43.75799999998387, -44.31999999998505, -48.61600000000519], 0.16: [-41.97799999997396, -48.994000000003155, -47.44799999999548, -48.57100000000161, -48.6870000000009], 0.18: [-32.772999999972996, -50.00000000000659, -43.01299999997884, -31.078999999977697, -50.00000000000659], 0.2: [-35.29899999997084, -48.9350000000018, -34.219999999969126, -27.584999999981346, -50.00000000000659]}}\n","4 fgsm (t): {'goal': {0.0: [95.69999999999227, 89.38199999998616, 86.18099999998894, 97.37399999999525, 88.54199999998954], 0.02: [89.50099999998993, -42.99299999998309, -32.49999999997503, 76.72599999997898, 43.7409999999991], 0.04: [-6.555000000005501, -42.719999999975556, -14.566999999994666, 2.4659999999983824, 39.78500000001644], 0.06: [31.81300000001211, -37.22899999997119, -5.636000000005297, -44.44099999998386, 40.03600000001552], 0.08: [8.641999999997429, -27.314999999983925, -45.85999999998993, -9.740000000000686, 24.486000000003877], 0.1: [30.937000000013107, -48.55500000000127, -44.60699999998788, 6.834999999993646, 6.191999999998751], 0.12: [13.961999999998774, -50.00000000000659, -39.06999999997241, 34.3290000000139, 9.28299999999443], 0.14: [-10.205000000004935, -48.70200000000095, -50.00000000000659, 30.80700000001752, -24.70899999998984], 0.16: [-16.77299999999464, -47.38299999999524, -50.00000000000659, 18.801000000008244, -34.33199999997571], 0.18: [-10.622999999998337, -47.35499999999969, -44.632999999984285, -21.733999999976078, -35.3349999999745], 0.2: [-33.238999999971746, -50.00000000000659, -47.70599999999642, -37.6399999999728, -35.293999999973295]}, 'action': {0.0: [95.69499999999225, 86.49099999998705, 86.94199999998466, 97.3109999999956, 83.93199999999432], 0.02: [74.0759999999865, 5.922999999996289, -18.392999999988835, 59.46599999998678, 36.056000000011565], 0.04: [-17.3939999999953, -17.464999999992354, -6.755000000005403, 37.45000000001379, -35.69699999997338], 0.06: [19.739000000002747, -15.28599999999745, -11.335000000004804, 4.6429999999965, -28.28399999997869], 0.08: [-10.90500000000547, -32.245999999975446, -19.768999999983013, -30.307999999975912, -32.090999999978365], 0.1: [-32.99999999997202, -25.783999999977468, -18.925999999979002, -4.38800000000003, -31.07599999997337], 0.12: [-31.23599999997436, -24.217999999975248, -33.011999999972495, -10.271000000003793, -42.08299999997451], 0.14: [-32.162999999973856, -27.521999999972735, -34.59399999997086, -29.173999999973454, -36.76699999997017], 0.16: [-40.94899999997028, -26.08199999997149, -21.192999999985126, -29.582999999973136, -46.20399999999215], 0.18: [-47.581999999998786, -18.92999999998291, -26.291999999978078, -34.03199999997133, -43.56299999998676], 0.2: [-45.0249999999894, -16.410999999974884, -14.10999999999559, -40.24599999997108, -50.00000000000659]}}\n","----\n","fgsm (ut): {'goal': {0.0: [95.70099999999228, 79.33199999998178, 89.34099999998288, 97.39799999999515, 88.58599999999434], 0.02: [84.54999999999183, 20.963000000014823, -38.769999999974516, 77.20899999998562, 83.28799999998984], 0.04: [90.00899999999086, -10.136999999996377, -31.68099999997623, 42.70600000000074, -34.21799999997151], 0.06: [-40.421999999967824, -14.760999999993055, -48.64100000000073, 1.224999999998504, -48.587000000000536], 0.08: [-48.61200000000063, -4.553000000000265, -48.707000000000974, -36.32999999997098, -48.663000000000814], 0.1: [-47.26499999999595, -10.208000000005665, -47.211999999994624, -37.50799999996979, -41.55099999997508], 0.12: [-29.207999999975332, 11.034000000016745, -48.61200000000063, -39.329999999970184, -40.267999999968715], 0.14: [-43.16699999997937, 3.786999999992897, -45.939999999989084, -40.81899999996984, -36.226999999975504], 0.16: [-44.524999999988715, -26.25799999997594, -47.33499999999621, -48.742000000002236, -35.06899999997096], 0.18: [-44.50299999998839, -34.94699999997272, -43.548999999981525, -48.74600000000111, -47.358999999995156], 0.2: [-46.02699999999054, -14.821999999994159, -43.65799999998071, -50.00000000000659, -41.92399999997264]}, 'action': {0.0: [95.69299999999227, 89.39399999998612, 87.8269999999826, 97.35599999999494, 82.63799999998238], 0.02: [89.71999999999012, -0.10700000000323624, -0.9890000000015076, 96.50899999999356, 65.82899999998813], 0.04: [42.05399999999496, 21.25600000001414, 68.3009999999904, 57.29599999998309, 11.278000000000837], 0.06: [8.640999999996112, -32.78899999997363, -50.00000000000659, 34.3980000000151, -38.65399999996966], 0.08: [-0.894000000003109, -50.00000000000659, -50.00000000000659, 8.477999999995841, -35.8689999999754], 0.1: [-19.009999999982178, -48.81200000000228, -24.011999999985797, -41.46999999997285, -48.5960000000017], 0.12: [0.16999999999668283, -47.83700000000599, -42.325999999976396, -47.20899999999461, -47.25799999999479], 0.14: [-29.857999999980684, -50.00000000000659, -43.75799999998387, -44.31999999998505, -48.61600000000519], 0.16: [-41.97799999997396, -48.994000000003155, -47.44799999999548, -48.57100000000161, -48.6870000000009], 0.18: [-32.772999999972996, -50.00000000000659, -43.01299999997884, -31.078999999977697, -50.00000000000659], 0.2: [-35.29899999997084, -48.9350000000018, -34.219999999969126, -27.584999999981346, -50.00000000000659]}}\n","fgsm (t): {'goal': {0.0: [95.69999999999227, 89.38199999998616, 86.18099999998894, 97.37399999999525, 88.54199999998954], 0.02: [89.50099999998993, -42.99299999998309, -32.49999999997503, 76.72599999997898, 43.7409999999991], 0.04: [-6.555000000005501, -42.719999999975556, -14.566999999994666, 2.4659999999983824, 39.78500000001644], 0.06: [31.81300000001211, -37.22899999997119, -5.636000000005297, -44.44099999998386, 40.03600000001552], 0.08: [8.641999999997429, -27.314999999983925, -45.85999999998993, -9.740000000000686, 24.486000000003877], 0.1: [30.937000000013107, -48.55500000000127, -44.60699999998788, 6.834999999993646, 6.191999999998751], 0.12: [13.961999999998774, -50.00000000000659, -39.06999999997241, 34.3290000000139, 9.28299999999443], 0.14: [-10.205000000004935, -48.70200000000095, -50.00000000000659, 30.80700000001752, -24.70899999998984], 0.16: [-16.77299999999464, -47.38299999999524, -50.00000000000659, 18.801000000008244, -34.33199999997571], 0.18: [-10.622999999998337, -47.35499999999969, -44.632999999984285, -21.733999999976078, -35.3349999999745], 0.2: [-33.238999999971746, -50.00000000000659, -47.70599999999642, -37.6399999999728, -35.293999999973295]}, 'action': {0.0: [95.69499999999225, 86.49099999998705, 86.94199999998466, 97.3109999999956, 83.93199999999432], 0.02: [74.0759999999865, 5.922999999996289, -18.392999999988835, 59.46599999998678, 36.056000000011565], 0.04: [-17.3939999999953, -17.464999999992354, -6.755000000005403, 37.45000000001379, -35.69699999997338], 0.06: [19.739000000002747, -15.28599999999745, -11.335000000004804, 4.6429999999965, -28.28399999997869], 0.08: [-10.90500000000547, -32.245999999975446, -19.768999999983013, -30.307999999975912, -32.090999999978365], 0.1: [-32.99999999997202, -25.783999999977468, -18.925999999979002, -4.38800000000003, -31.07599999997337], 0.12: [-31.23599999997436, -24.217999999975248, -33.011999999972495, -10.271000000003793, -42.08299999997451], 0.14: [-32.162999999973856, -27.521999999972735, -34.59399999997086, -29.173999999973454, -36.76699999997017], 0.16: [-40.94899999997028, -26.08199999997149, -21.192999999985126, -29.582999999973136, -46.20399999999215], 0.18: [-47.581999999998786, -18.92999999998291, -26.291999999978078, -34.03199999997133, -43.56299999998676], 0.2: [-45.0249999999894, -16.410999999974884, -14.10999999999559, -40.24599999997108, -50.00000000000659]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yh9idbuV4Xei"},"source":["Solved after 1300 episodes!\n","0 noise_hrl: {'both': {0.0: [95.71899999999233], 0.05: [36.33100000001844], 0.1: [10.241999999996773], 0.15: [1.145999999996496], 0.2: [-16.41199999999501], 0.25: [-23.414999999981568], 0.3: [-27.295999999977866], 0.35: [-28.383999999976886], 0.4: [-20.068999999983657], 0.45: [-19.84999999998699], 0.5: [-23.236999999974486]}, 'action_only': {0.0: [95.70099999999228], 0.05: [43.98100000000906], 0.1: [21.754000000022216], 0.15: [11.739000000003678], 0.2: [-2.9150000000057847], 0.25: [-5.92100000000165], 0.3: [-13.43099999998752], 0.35: [-13.83499999999286], 0.4: [-25.502999999981387], 0.45: [-17.43299999998726], 0.5: [-15.567999999992255]}, 'goal_only': {0.0: [95.7029999999923], 0.05: [32.93200000000838], 0.1: [25.218000000012385], 0.15: [25.8400000000093], 0.2: [22.131000000013795], 0.25: [-3.235000000005317], 0.3: [-18.186999999987098], 0.35: [6.225999999994963], 0.4: [-6.694000000003512], 0.45: [-6.2610000000056605], 0.5: [-17.512999999990562]}, 'both_same': {0.0: [95.69599999999227], 0.05: [31.450000000015415], 0.1: [18.973000000010583], 0.15: [-7.004000000005846], 0.2: [-14.76799999999664], 0.25: [-32.06499999997657], 0.3: [-24.702999999985607], 0.35: [-27.210999999977318], 0.4: [-31.786999999970107], 0.45: [-22.224999999983837], 0.5: [-34.331999999969305]}}\n","Solved after 1429 episodes!\n","1 noise_hrl: {'both': {0.0: [85.08099999998196], 0.05: [11.312000000009155], 0.1: [-20.354999999982756], 0.15: [-30.607999999975956], 0.2: [-38.30699999996973], 0.25: [-36.13599999997647], 0.3: [-37.7729999999675], 0.35: [-37.136999999970996], 0.4: [-43.04699999998149], 0.45: [-37.62699999997014], 0.5: [-41.92099999997878]}, 'action_only': {0.0: [90.82999999999062], 0.05: [64.63899999998216], 0.1: [7.302999999995705], 0.15: [-15.188999999989951], 0.2: [-36.132999999975816], 0.25: [-32.380999999972666], 0.3: [-33.53399999997083], 0.35: [-35.26499999997313], 0.4: [-41.25499999996954], 0.45: [-36.65299999996966], 0.5: [-38.405999999968984]}, 'goal_only': {0.0: [89.37399999998674], 0.05: [-11.597999999998075], 0.1: [6.292999999996778], 0.15: [-20.079999999980313], 0.2: [-12.224000000002805], 0.25: [-10.928999999988038], 0.3: [-3.0220000000030103], 0.35: [-9.265000000003653], 0.4: [-5.708000000001275], 0.45: [-3.4700000000029765], 0.5: [-5.959000000005124]}, 'both_same': {0.0: [90.80799999998992], 0.05: [2.97699999999885], 0.1: [-14.219999999999523], 0.15: [-21.262999999979737], 0.2: [-34.332999999974184], 0.25: [-36.180999999971455], 0.3: [-35.55799999996861], 0.35: [-38.969999999970625], 0.4: [-44.29499999998308], 0.45: [-42.74899999997589], 0.5: [-43.93499999998452]}}\n","Solved after 1333 episodes!\n","2 noise_hrl: {'both': {0.0: [86.97799999998416], 0.05: [-3.9930000000039865], 0.1: [-3.491000000006273], 0.15: [-11.86200000000281], 0.2: [-2.4150000000037797], 0.25: [-1.1120000000008377], 0.3: [0.25999999999848994], 0.35: [1.2079999999999975], 0.4: [-0.6480000000017168], 0.45: [-6.936000000006163], 0.5: [5.732999999998718]}, 'action_only': {0.0: [93.4529999999898], 0.05: [-9.202000000003634], 0.1: [-27.195999999977044], 0.15: [-30.285999999970677], 0.2: [-21.29399999998592], 0.25: [-25.88499999997753], 0.3: [-18.364999999993547], 0.35: [-12.288999999990486], 0.4: [-18.46999999998882], 0.45: [-6.792000000007956], 0.5: [-19.954999999984487]}, 'goal_only': {0.0: [82.17399999998875], 0.05: [32.58200000001195], 0.1: [8.136999999993355], 0.15: [21.349000000014502], 0.2: [35.15900000001691], 0.25: [39.915000000019376], 0.3: [42.73600000001706], 0.35: [47.096000000007216], 0.4: [48.91700000000682], 0.45: [29.11900000001679], 0.5: [40.65900000001662]}, 'both_same': {0.0: [93.38699999999172], 0.05: [9.605999999995054], 0.1: [-7.424000000003502], 0.15: [-9.652000000004413], 0.2: [-8.519000000008072], 0.25: [-5.402000000005314], 0.3: [-18.974999999987094], 0.35: [-5.910000000005767], 0.4: [5.334999999994836], 0.45: [-14.994999999982833], 0.5: [-3.668999999999288]}}\n","Freezing controller at episode 1721!\n","Solved after 1747 episodes!\n","3 noise_hrl: {'both': {0.0: [97.42999999999535], 0.05: [19.733000000010012], 0.1: [14.699000000005894], 0.15: [27.917000000015047], 0.2: [25.576000000010794], 0.25: [35.38600000001604], 0.3: [29.137000000017075], 0.35: [-5.070000000002102], 0.4: [-4.344000000003842], 0.45: [-3.03100000000078], 0.5: [-13.470999999987923]}, 'action_only': {0.0: [97.51899999999554], 0.05: [-3.5110000000047803], 0.1: [14.302000000012729], 0.15: [12.120000000004026], 0.2: [14.287000000015965], 0.25: [9.75299999999406], 0.3: [8.659999999998776], 0.35: [3.519999999996262], 0.4: [-10.375000000005683], 0.45: [-8.703000000000342], 0.5: [-7.305000000002894]}, 'goal_only': {0.0: [97.43499999999538], 0.05: [73.93099999998068], 0.1: [43.629999999995526], 0.15: [48.3590000000009], 0.2: [50.90599999999582], 0.25: [60.138999999986396], 0.3: [61.07999999997956], 0.35: [69.44099999997752], 0.4: [56.97699999998388], 0.45: [64.06999999998418], 0.5: [54.94599999998625]}, 'both_same': {0.0: [97.49899999999552], 0.05: [20.24500000001163], 0.1: [11.841000000010572], 0.15: [35.15400000001422], 0.2: [38.41900000002001], 0.25: [40.90100000001321], 0.3: [34.0030000000203], 0.35: [11.14100000000534], 0.4: [-7.367000000005667], 0.45: [-4.705000000006102], 0.5: [4.847999999997078]}}\n","Freezing controller at episode 1621!\n","4 noise_hrl: {'both': {0.0: [85.65499999999057], 0.05: [24.589000000016302], 0.1: [0.5699999999998835], 0.15: [-4.063000000002095], 0.2: [-16.114999999996527], 0.25: [-35.646999999973985], 0.3: [-30.398999999978074], 0.35: [-34.836999999972306], 0.4: [-38.1349999999715], 0.45: [-42.12699999998083], 0.5: [-35.97699999997155]}, 'action_only': {0.0: [84.09499999998452], 0.05: [61.480999999983766], 0.1: [44.114000000003955], 0.15: [26.927000000012328], 0.2: [11.278000000007976], 0.25: [4.37399999999488], 0.3: [-11.894000000003293], 0.35: [-18.935999999978932], 0.4: [-35.99299999997648], 0.45: [-36.102999999975125], 0.5: [-44.50799999998865]}, 'goal_only': {0.0: [88.51699999999379], 0.05: [46.51000000000161], 0.1: [10.096999999999454], 0.15: [-13.362999999990642], 0.2: [-13.661999999992078], 0.25: [-26.85199999997972], 0.3: [-21.35099999999175], 0.35: [-28.549999999976045], 0.4: [-30.43699999997395], 0.45: [-19.694999999985413], 0.5: [-27.763999999977223]}, 'both_same': {0.0: [85.56699999998786], 0.05: [34.85300000000829], 0.1: [-1.015999999999794], 0.15: [-14.036999999995691], 0.2: [-22.892999999980976], 0.25: [-23.205999999991473], 0.3: [-33.63399999997173], 0.35: [-27.909999999977273], 0.4: [-27.08199999997258], 0.45: [-28.621999999975632], 0.5: [-36.71999999997275]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdQEb3LDjtxd"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointFallEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHVoTScvj8Ru","executionInfo":{"status":"ok","timestamp":1617094424200,"user_tz":-60,"elapsed":1199221,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"05b3dbd5-a1a0-4e7a-a94d-7e78e003244e"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 5:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [31.672000000011476], 1.5: [-26.693999999974118], 2.0: [87.00999999999144], 2.5: [94.40999999999336], 3.0: [95.71799999999226], 3.5: [94.27899999999313], 4.0: [95.71599999999229], 4.5: [95.57599999999208], 5.0: [94.05399999999022], 5.5: [94.03899999999273], 6.0: [95.34199999999164], 6.5: [93.70999999998989], 7.0: [93.6129999999921]}\n","1 scale: {1.0: [31.672000000011476, 25.863000000019515], 1.5: [-26.693999999974118, 39.771000000015015], 2.0: [87.00999999999144, 46.586000000012916], 2.5: [94.40999999999336, 67.86199999998246], 3.0: [95.71799999999226, 75.05099999998178], 3.5: [94.27899999999313, 82.22899999998535], 4.0: [95.71599999999229, 89.02999999998967], 4.5: [95.57599999999208, 86.34599999998792], 5.0: [94.05399999999022, 76.31799999997781], 5.5: [94.03899999999273, 54.74899999999717], 6.0: [95.34199999999164, 69.06199999998958], 6.5: [93.70999999998989, 74.7349999999945], 7.0: [93.6129999999921, 86.30199999998433]}\n","2 scale: {1.0: [31.672000000011476, 25.863000000019515, 12.505999999995415], 1.5: [-26.693999999974118, 39.771000000015015, 7.263999999994676], 2.0: [87.00999999999144, 46.586000000012916, 96.70799999999463], 2.5: [94.40999999999336, 67.86199999998246, 80.11299999998849], 3.0: [95.71799999999226, 75.05099999998178, 89.33399999999169], 3.5: [94.27899999999313, 82.22899999998535, 81.837999999988], 4.0: [95.71599999999229, 89.02999999998967, 88.01499999998268], 4.5: [95.57599999999208, 86.34599999998792, 90.2939999999874], 5.0: [94.05399999999022, 76.31799999997781, 88.22999999999286], 5.5: [94.03899999999273, 54.74899999999717, 63.863999999995556], 6.0: [95.34199999999164, 69.06199999998958, 7.452999999995538], 6.5: [93.70999999998989, 74.7349999999945, 4.900999999995186], 7.0: [93.6129999999921, 86.30199999998433, -33.004999999972014]}\n","3 scale: {1.0: [31.672000000011476, 25.863000000019515, 12.505999999995415, 76.62099999997932], 1.5: [-26.693999999974118, 39.771000000015015, 7.263999999994676, -12.15700000000484], 2.0: [87.00999999999144, 46.586000000012916, 96.70799999999463, 80.35099999998633], 2.5: [94.40999999999336, 67.86199999998246, 80.11299999998849, 86.96599999998968], 3.0: [95.71799999999226, 75.05099999998178, 89.33399999999169, 66.26699999999994], 3.5: [94.27899999999313, 82.22899999998535, 81.837999999988, 79.24999999998073], 4.0: [95.71599999999229, 89.02999999998967, 88.01499999998268, 97.285999999995], 4.5: [95.57599999999208, 86.34599999998792, 90.2939999999874, 97.53499999999558], 5.0: [94.05399999999022, 76.31799999997781, 88.22999999999286, 97.36199999999526], 5.5: [94.03899999999273, 54.74899999999717, 63.863999999995556, 97.00899999999463], 6.0: [95.34199999999164, 69.06199999998958, 7.452999999995538, 95.29799999999238], 6.5: [93.70999999998989, 74.7349999999945, 4.900999999995186, 61.31199999997988], 7.0: [93.6129999999921, 86.30199999998433, -33.004999999972014, 33.43900000001956]}\n","4 scale: {1.0: [31.672000000011476, 25.863000000019515, 12.505999999995415, 76.62099999997932, 94.91599999999671], 1.5: [-26.693999999974118, 39.771000000015015, 7.263999999994676, -12.15700000000484, 57.51399999999], 2.0: [87.00999999999144, 46.586000000012916, 96.70799999999463, 80.35099999998633, 96.63599999999478], 2.5: [94.40999999999336, 67.86199999998246, 80.11299999998849, 86.96599999998968, 95.78499999999322], 3.0: [95.71799999999226, 75.05099999998178, 89.33399999999169, 66.26699999999994, 93.51799999999744], 3.5: [94.27899999999313, 82.22899999998535, 81.837999999988, 79.24999999998073, 90.42299999999197], 4.0: [95.71599999999229, 89.02999999998967, 88.01499999998268, 97.285999999995, 85.64999999999378], 4.5: [95.57599999999208, 86.34599999998792, 90.2939999999874, 97.53499999999558, 82.49099999998651], 5.0: [94.05399999999022, 76.31799999997781, 88.22999999999286, 97.36199999999526, 80.66999999998265], 5.5: [94.03899999999273, 54.74899999999717, 63.863999999995556, 97.00899999999463, 69.75999999997536], 6.0: [95.34199999999164, 69.06199999998958, 7.452999999995538, 95.29799999999238, 71.59499999999007], 6.5: [93.70999999998989, 74.7349999999945, 4.900999999995186, 61.31199999997988, 70.55599999998527], 7.0: [93.6129999999921, 86.30199999998433, -33.004999999972014, 33.43900000001956, 39.04000000001194]}\n","----\n","scale: {1.0: [31.672000000011476, 25.863000000019515, 12.505999999995415, 76.62099999997932, 94.91599999999671], 1.5: [-26.693999999974118, 39.771000000015015, 7.263999999994676, -12.15700000000484, 57.51399999999], 2.0: [87.00999999999144, 46.586000000012916, 96.70799999999463, 80.35099999998633, 96.63599999999478], 2.5: [94.40999999999336, 67.86199999998246, 80.11299999998849, 86.96599999998968, 95.78499999999322], 3.0: [95.71799999999226, 75.05099999998178, 89.33399999999169, 66.26699999999994, 93.51799999999744], 3.5: [94.27899999999313, 82.22899999998535, 81.837999999988, 79.24999999998073, 90.42299999999197], 4.0: [95.71599999999229, 89.02999999998967, 88.01499999998268, 97.285999999995, 85.64999999999378], 4.5: [95.57599999999208, 86.34599999998792, 90.2939999999874, 97.53499999999558, 82.49099999998651], 5.0: [94.05399999999022, 76.31799999997781, 88.22999999999286, 97.36199999999526, 80.66999999998265], 5.5: [94.03899999999273, 54.74899999999717, 63.863999999995556, 97.00899999999463, 69.75999999997536], 6.0: [95.34199999999164, 69.06199999998958, 7.452999999995538, 95.29799999999238, 71.59499999999007], 6.5: [93.70999999998989, 74.7349999999945, 4.900999999995186, 61.31199999997988, 70.55599999998527], 7.0: [93.6129999999921, 86.30199999998433, -33.004999999972014, 33.43900000001956, 39.04000000001194]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCngiUhykCBW"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VRa-o8mkMvW","executionInfo":{"status":"ok","timestamp":1617095216752,"user_tz":-60,"elapsed":791655,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"767de213-5fe6-4c34-ec87-43963c4a5753"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 5:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [95.69499999999226], 0.05: [95.69299999999228], 0.1: [92.76999999998893], 0.15: [94.25099999999313], 0.2: [92.7689999999914], 0.25: [89.80599999999198], 0.3: [81.08999999998365], 0.35: [78.12399999998554], 0.4: [60.701999999986576]}\n","1 range: {0.0: [95.69499999999226, 89.387999999989], 0.05: [95.69299999999228, 87.95199999998745], 0.1: [92.76999999998893, 83.36299999997965], 0.15: [94.25099999999313, 75.04199999998538], 0.2: [92.7689999999914, 60.39900000000048], 0.25: [89.80599999999198, 60.51599999998968], 0.3: [81.08999999998365, 41.96199999999914], 0.35: [78.12399999998554, 48.72900000000037], 0.4: [60.701999999986576, 18.851999999997545]}\n","2 range: {0.0: [95.69499999999226, 89.387999999989, 91.36199999999164], 0.05: [95.69299999999228, 87.95199999998745, 78.13199999999216], 0.1: [92.76999999998893, 83.36299999997965, 69.0099999999904], 0.15: [94.25099999999313, 75.04199999998538, 68.42799999998418], 0.2: [92.7689999999914, 60.39900000000048, 61.71399999998523], 0.25: [89.80599999999198, 60.51599999998968, 66.85299999999303], 0.3: [81.08999999998365, 41.96199999999914, 69.43399999998088], 0.35: [78.12399999998554, 48.72900000000037, 56.94199999998006], 0.4: [60.701999999986576, 18.851999999997545, 57.18299999997907]}\n","3 range: {0.0: [95.69499999999226, 89.387999999989, 91.36199999999164, 97.52599999999553], 0.05: [95.69299999999228, 87.95199999998745, 78.13199999999216, 96.10899999999609], 0.1: [92.76999999998893, 83.36299999997965, 69.0099999999904, 85.67299999998728], 0.15: [94.25099999999313, 75.04199999998538, 68.42799999998418, 79.74799999998082], 0.2: [92.7689999999914, 60.39900000000048, 61.71399999998523, 83.61099999998683], 0.25: [89.80599999999198, 60.51599999998968, 66.85299999999303, 62.835999999994165], 0.3: [81.08999999998365, 41.96199999999914, 69.43399999998088, 57.300999999983496], 0.35: [78.12399999998554, 48.72900000000037, 56.94199999998006, 57.402000000000605], 0.4: [60.701999999986576, 18.851999999997545, 57.18299999997907, 61.38999999998856]}\n","4 range: {0.0: [95.69499999999226, 89.387999999989, 91.36199999999164, 97.52599999999553, 75.25099999997768], 0.05: [95.69299999999228, 87.95199999998745, 78.13199999999216, 96.10899999999609, 79.74299999998972], 0.1: [92.76999999998893, 83.36299999997965, 69.0099999999904, 85.67299999998728, 79.26599999998811], 0.15: [94.25099999999313, 75.04199999998538, 68.42799999998418, 79.74799999998082, 77.74399999998937], 0.2: [92.7689999999914, 60.39900000000048, 61.71399999998523, 83.61099999998683, 59.761999999976844], 0.25: [89.80599999999198, 60.51599999998968, 66.85299999999303, 62.835999999994165, 61.24399999999157], 0.3: [81.08999999998365, 41.96199999999914, 69.43399999998088, 57.300999999983496, 37.719000000011036], 0.35: [78.12399999998554, 48.72900000000037, 56.94199999998006, 57.402000000000605, 48.15500000000451], 0.4: [60.701999999986576, 18.851999999997545, 57.18299999997907, 61.38999999998856, 48.07100000000395]}\n","----\n","range: {0.0: [95.69499999999226, 89.387999999989, 91.36199999999164, 97.52599999999553, 75.25099999997768], 0.05: [95.69299999999228, 87.95199999998745, 78.13199999999216, 96.10899999999609, 79.74299999998972], 0.1: [92.76999999998893, 83.36299999997965, 69.0099999999904, 85.67299999998728, 79.26599999998811], 0.15: [94.25099999999313, 75.04199999998538, 68.42799999998418, 79.74799999998082, 77.74399999998937], 0.2: [92.7689999999914, 60.39900000000048, 61.71399999998523, 83.61099999998683, 59.761999999976844], 0.25: [89.80599999999198, 60.51599999998968, 66.85299999999303, 62.835999999994165, 61.24399999999157], 0.3: [81.08999999998365, 41.96199999999914, 69.43399999998088, 57.300999999983496, 37.719000000011036], 0.35: [78.12399999998554, 48.72900000000037, 56.94199999998006, 57.402000000000605, 48.15500000000451], 0.4: [60.701999999986576, 18.851999999997545, 57.18299999997907, 61.38999999998856, 48.07100000000395]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1g-1WbYSck9B","executionInfo":{"status":"ok","timestamp":1617096406182,"user_tz":-60,"elapsed":168278,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"c0f3d7c0-fc54-4877-b914-ec1fbcd66a94"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 4\n","while i < 5:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4 range: {0.0: [88.5599999999965], 0.05: [78.1489999999857], 0.1: [80.89999999998535], 0.15: [74.74499999999215], 0.2: [76.34499999998677], 0.25: [71.91599999998736], 0.3: [54.342999999997026], 0.35: [46.83299999999952], 0.4: [34.7620000000102]}\n","----\n","range: {0.0: [88.5599999999965], 0.05: [78.1489999999857], 0.1: [80.89999999998535], 0.15: [74.74499999999215], 0.2: [76.34499999998677], 0.25: [71.91599999998736], 0.3: [54.342999999997026], 0.35: [46.83299999999952], 0.4: [34.7620000000102]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fXVuyJCHOES6"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmuu11RLSSo1","executionInfo":{"status":"ok","timestamp":1612798729883,"user_tz":-60,"elapsed":63303,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"d32abe58-f333-4cba-d4c1-d7b0a59209b7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","torch.save(episodes, \"PointFall_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RqCEeJ2WSZ3F"},"source":[""],"execution_count":null,"outputs":[]}]}