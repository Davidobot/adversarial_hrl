{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointmaze_freeze.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617354920383,"user_tz":-60,"elapsed":28870,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"dd050bf3-7bd9-45cb-8c36-d3db2197d2d2"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_maze.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617354921948,"user_tz":-60,"elapsed":1494,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/exp_point_maze/{name}\" \n","\n","    torch.save({\n","      'meta_controller': {\n","          'critic': model.meta_controller.critic.state_dict(),\n","          'actor': model.meta_controller.actor.state_dict(),\n","      },\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name, dir=\"exp_point_maze\"):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{dir}/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617354925470,"user_tz":-60,"elapsed":4969,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617354925491,"user_tz":-60,"elapsed":4963,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617354925494,"user_tz":-60,"elapsed":4939,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_maze import PointMazeEnv \n","env = NormalizedEnv(PointMazeEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1617354925504,"user_tz":-60,"elapsed":4920,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def plot_durations(episode_durations, goals_done):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*goals_done)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Goals done')\n","    \n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617354925508,"user_tz":-60,"elapsed":4895,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617354925515,"user_tz":-60,"elapsed":4872,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617354925523,"user_tz":-60,"elapsed":4850,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617354925525,"user_tz":-60,"elapsed":4825,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617354925527,"user_tz":-60,"elapsed":4802,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 50000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617354925532,"user_tz":-60,"elapsed":4779,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        self.meta_controller = TD3(nb_states, len(self.goal_dim), True).to(device)\n","        self.max_goal_dist = torch.from_numpy(np.array([2.5, 2.5])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([1., 1.])).to(device)\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 500000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.update_policy(self.off_policy_corrections)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    # correct goals to allow for use in experience replay\n","    def off_policy_corrections(self, sgoals, states, actions, candidate_goals=8):\n","        first_s = [s[0] for s in states] # First x\n","        last_s = [s[-1] for s in states] # Last x\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # diff = 1\n","        diff_goal = (np.array(last_s) - np.array(first_s))[:, np.newaxis, :self.goal_dimen]\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # original = 1\n","        # random = candidate_goals\n","        scale = self.max_goal_dist.cpu().numpy()\n","        original_goal = np.array(sgoals)[:, np.newaxis, :]\n","        random_goals = np.random.normal(loc=diff_goal, scale=.5*scale,\n","                                        size=(BATCH_SIZE, candidate_goals, original_goal.shape[-1]))\n","        random_goals = random_goals.clip(-scale, scale)\n","\n","        # Shape: (batch_size, 10, subgoal_dim)\n","        candidates = np.concatenate([original_goal, diff_goal, random_goals], axis=1)\n","        #states = np.array(states)[:, :-1, :]\n","        actions = np.array(actions)\n","        seq_len = len(states[0])\n","\n","        # For ease\n","        new_batch_sz = seq_len * BATCH_SIZE\n","        action_dim = actions[0][0].shape\n","        obs_dim = states[0][0].shape\n","        ncands = candidates.shape[1]\n","\n","        true_actions = actions.reshape((new_batch_sz,) + action_dim)\n","        observations = states.reshape((new_batch_sz,) + obs_dim)\n","        goal_shape = (new_batch_sz, self.goal_dimen)\n","        # observations = get_obs_tensor(observations, sg_corrections=True)\n","\n","        # batched_candidates = np.tile(candidates, [seq_len, 1, 1])\n","        # batched_candidates = batched_candidates.transpose(1, 0, 2)\n","\n","        policy_actions = np.zeros((ncands, new_batch_sz) + action_dim)\n","\n","        observations = torch.from_numpy(observations).to(device)\n","        for c in range(ncands):\n","            subgoal = candidates[:,c]\n","            candidate = (subgoal + states[:, 0, :self.goal_dimen])[:, None] - states[:, :, :self.goal_dimen]\n","            candidate = candidate.reshape(*goal_shape)\n","            policy_actions[c] = self.controller.actor(torch.cat([observations, torch.from_numpy(candidate).to(device)], 1).float()).detach().cpu().numpy()\n","\n","        difference = (policy_actions - true_actions)\n","        difference = np.where(difference != -np.inf, difference, 0)\n","        difference = difference.reshape((ncands, BATCH_SIZE, seq_len) + action_dim).transpose(1, 0, 2, 3)\n","\n","        logprob = -0.5*np.sum(np.linalg.norm(difference, axis=-1)**2, axis=-1)\n","        max_indices = np.argmax(logprob, axis=-1)\n","\n","        return torch.from_numpy(candidates[np.arange(BATCH_SIZE), max_indices]).to(device).float()\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done, state_seq, action_seq)\n","\n","    def select_goal(self, s_t, warmup, decay_epsilon):\n","        return self.meta_controller.select_action(s_t, warmup, decay_epsilon) * self.max_goal_dist + self.goal_offset\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 13\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 6000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    freeze_ctrl = False\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                if not freeze_ctrl:\n","                    agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup and not freeze_ctrl:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 70 and not freeze_ctrl:\n","                print(\"Freezing controller!\")\n","                freeze_ctrl = True\n","                agent.controller.eval()\n","                agent.controller.is_training = False\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_freeze_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nj1smUjnRiSJ"},"source":["def plot_fgsm(episode_durations):\n","    plt.figure(2, figsize=(10,10))\n","    \n","    for kk in ['both', 'goal_only', 'action_only']:\n","        x, ys = np.array(list(episode_durations[kk].keys())), np.array(list(episode_durations[kk].values()))\n","        #plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n","        plt.xlabel('$\\epsilon$')\n","        plt.ylabel('Average Reward')\n","        \n","        mu = np.mean(ys, axis=1)\n","        plt.plot(x, mu, label=kk)\n","        stds = np.std(ys, axis = 1)\n","        plt.fill_between(x, mu + stds , mu - stds, alpha=0.2)\n","    \n","    plt.legend()\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1617354943826,"user_tz":-60,"elapsed":7776,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    #g_state = torch.tensor(g_state, requires_grad=True)\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    if targeted:\n","        # initial forward pass\n","        goal = agent.meta_controller.actor(g_state)\n","        goal = torch.clamp(goal, -1., 1.)\n","\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = agent.meta_controller.critic.Q1(g_state, agent.meta_controller.actor(g_state)).mean()\n","\n","    agent.meta_controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal = agent.select_goal(og_state, False, False)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrR0kvDhFwRa"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 10\n","while i < 13:\n","    agent = train_model()\n","    #agent = HIRO(n_observations, n_actions).to(device)\n","    #load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QY8Hlh4y2gYn"},"source":["9 noise_hrl: {'both': {0.0: [63.2499999999849, 92.62599999998866, 92.31099999998783, 94.70999999999141], 0.05: [14.492000000012606, 39.58800000001435, 64.96599999998476, 38.94600000001676], 0.1: [27.415000000013574, 23.046000000015955, 61.344999999986904, -4.900000000001306], 0.15: [47.72899999999095, 22.247000000012477, 55.017999999995396, 10.567000000004402], 0.2: [50.80099999999987, 24.51300000000916, 43.03500000000624, 31.84200000001661], 0.25: [53.86899999998959, 15.924000000008748, 19.286000000015747, 58.125999999991194], 0.3: [34.78200000001577, 21.711000000008095, 7.511999999999667, 52.45899999999412], 0.35: [26.629000000019698, 18.1200000000116, -3.275000000007525, 60.42699999998834], 0.4: [19.557000000008628, 22.47100000000919, -23.402999999972213, 59.040999999967426], 0.45: [-4.09700000000639, 6.053999999996466, -19.44299999997619, 60.16299999997944], 0.5: [-8.143000000004607, 15.72800000000255, -21.04399999997687, 55.539999999987515]}, 'action_only': {0.0: [58.75999999999063, 95.86499999999238, 92.30499999998774, 90.55699999999197], 0.05: [29.87600000002182, 55.19099999999902, 67.9449999999877, 52.17100000000133], 0.1: [46.36299999999904, 20.90900000000578, 66.85699999997661, 19.912000000013027], 0.15: [54.87999999998949, 23.43400000002506, 64.09799999998218, 33.75300000001192], 0.2: [76.63399999998049, 22.928000000011806, 43.45100000000866, 20.442000000017774], 0.25: [78.81899999998069, 27.3010000000064, 10.337000000007185, 6.874999999997202], 0.3: [85.70599999998, 17.397000000012728, -7.6540000000037445, 24.177000000012864], 0.35: [78.25299999997547, 1.9629999999933683, 9.887000000003987, 25.651000000012246], 0.4: [77.64799999997418, 9.123999999997647, -5.537000000002687, 27.55700000002196], 0.45: [56.51399999998225, -7.306999999999587, -19.77099999998575, 39.76900000001726], 0.5: [50.33100000000164, -20.02099999998817, -21.518999999982515, 31.696000000008556]}, 'goal_only': {0.0: [59.769999999987476, 95.62799999999221, 92.19899999998616, 93.87399999999221], 0.05: [35.131000000014204, 62.51799999998825, 72.83799999998267, 59.45999999998941], 0.1: [26.36100000000998, 53.15099999999298, 60.073999999983805, 54.76099999998571], 0.15: [35.130000000012174, 54.36499999998766, 54.2170000000001, 42.323000000013465], 0.2: [25.874000000013115, 31.476000000012395, 49.57599999999768, 25.031000000019247], 0.25: [21.236000000014343, 39.32500000001559, 47.60000000000948, 33.29400000001551], 0.3: [25.672000000011593, 31.068000000016582, 37.42500000001646, 41.73600000001942], 0.35: [31.451000000013394, 34.369000000021295, 34.557000000013154, 54.80899999999975], 0.4: [23.645000000013045, 38.04700000002391, 37.967000000017045, 50.046999999996196], 0.45: [28.30700000001605, 33.88500000002205, 26.04500000001779, 61.12099999998854], 0.5: [18.08400000000915, 29.41500000000941, 30.211000000019908, 66.42099999997994]}, 'both_same': {0.0: [63.642999999990316, 94.2719999999927, 91.68999999998597, 93.88599999999315], 0.05: [15.846000000008443, 31.610000000012086, 60.87099999998507, 27.758000000005477], 0.1: [31.624000000018036, 36.2120000000135, 56.117999999986004, 34.24000000001498], 0.15: [46.39800000001035, 20.90900000001271, 54.18399999999171, 21.030000000021097], 0.2: [54.509999999997014, 17.569000000006533, 43.06300000001263, 42.34500000001426], 0.25: [56.717000000000574, 10.208000000001284, 26.152000000008215, 60.74199999997605], 0.3: [41.77400000000545, 11.583000000005931, 6.198999999995586, 61.699999999983454], 0.35: [23.481000000003664, 15.708000000003663, 1.498999999993432, 60.32999999997483], 0.4: [12.37399999999473, 10.773999999996962, -12.274999999997883, 59.45699999998995], 0.45: [-17.304999999992965, 7.211000000007619, -11.917000000005585, 51.73500000000291], 0.5: [-26.178999999978142, 10.981999999997097, 1.3509999999985889, 61.060999999988255]}}\n","11 noise_hrl: {'both': {0.0: [90.44699999999301, 85.6029999999881], 0.05: [45.50200000000331, 79.016999999981], 0.1: [23.311000000013543, 81.6189999999816], 0.15: [22.320000000016943, 84.08099999998045], 0.2: [3.7879999999954874, 86.78999999997967], 0.25: [17.85800000000643, 85.96299999997724], 0.3: [27.53300000001715, 80.55599999997425], 0.35: [27.769000000014106, 69.42799999997487], 0.4: [31.9910000000116, 71.42599999997374], 0.45: [26.696000000017538, 63.51099999998461], 0.5: [38.63300000002091, 55.88799999998856]}, 'action_only': {0.0: [91.99599999998799, 86.85499999999107], 0.05: [57.68499999999114, 57.69399999999099], 0.1: [53.7989999999945, 76.25699999997798], 0.15: [25.449000000012962, 85.83299999998671], 0.2: [12.089000000000249, 87.89099999998263], 0.25: [-21.29799999998237, 90.3689999999837], 0.3: [-13.080999999997305, 88.74099999998197], 0.35: [-20.342999999981043, 87.46099999997746], 0.4: [-2.54600000000251, 85.58999999997626], 0.45: [-10.524000000004303, 83.54799999997563], 0.5: [-5.9690000000077506, 77.9169999999724]}, 'goal_only': {0.0: [92.82999999999069, 87.26199999998536], 0.05: [52.06699999998825, 80.56699999998072], 0.1: [36.70000000001948, 81.03999999998052], 0.15: [39.82800000001473, 78.27399999998565], 0.2: [22.08100000001644, 76.13399999998721], 0.25: [21.891000000006358, 79.90099999997358], 0.3: [17.768000000007028, 70.84899999997918], 0.35: [10.941999999996876, 64.9749999999712], 0.4: [14.791000000001102, 63.97199999998661], 0.45: [16.041000000010484, 58.29899999997931], 0.5: [4.992999999994562, 60.55899999997716]}, 'both_same': {0.0: [86.10999999998796, 91.69699999999492], 0.05: [52.9359999999792, 62.37599999999462], 0.1: [17.33900000000972, 86.21199999998187], 0.15: [22.938000000013485, 86.66399999997918], 0.2: [18.021999999997423, 85.34599999997661], 0.25: [10.391999999998037, 88.02899999998174], 0.3: [13.498000000007284, 77.50599999997374], 0.35: [18.640000000008964, 73.6089999999795], 0.4: [25.423000000015968, 61.505999999981896], 0.45: [15.59900000000903, 59.255999999987026], 0.5: [35.21600000002025, 51.14899999999404]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhC6f7N6sJoa","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"error","timestamp":1617371435631,"user_tz":-60,"elapsed":16479810,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"17370b4e-ac42-43f1-c071-5b7a223afe24"},"source":["targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","untargeted = {'goal': {0.0: [89.63199999998264, 92.69899999998768], 0.02: [22.219000000011334, 14.479000000008135], 0.04: [-1.7639999999994902, 50.276000000001595], 0.06: [-20.585999999983134, 45.57100000001389], 0.08: [-48.63500000000071, 41.828000000017774], 0.1: [-50.00000000000659, 5.393999999998393], 0.12: [-50.00000000000659, -21.635999999978562], 0.14: [-50.00000000000659, -44.028999999980314], 0.16: [-50.00000000000659, -23.011999999985015], 0.18: [-50.00000000000659, -37.35499999997054], 0.2: [-50.00000000000659, -40.6509999999705]}, 'action': {0.0: [88.49699999998339, 92.71999999998708], 0.02: [45.500000000011624, 47.67099999999557], 0.04: [-34.6019999999728, 27.390000000016094], 0.06: [-34.890999999970354, -2.2620000000002363], 0.08: [-29.85499999997812, -34.20399999996939], 0.1: [-40.6059999999697, -38.63699999997104], 0.12: [-36.37899999997046, -32.90799999997615], 0.14: [-41.836999999974076, -41.75699999997383], 0.16: [-44.59799999998391, -45.401999999990764], 0.18: [-42.20299999997276, -37.89799999997212], 0.2: [-40.68799999996872, -41.19799999997083]}}\n","targeted = {'goal': {0.0: [88.27999999998546, 93.59499999998805], 0.02: [-47.398999999995304, 84.3629999999788], 0.04: [-50.00000000000659, 76.70199999996719], 0.06: [-50.00000000000659, 11.929000000015035], 0.08: [-50.00000000000659, 12.72600000001143], 0.1: [-50.00000000000659, 7.328999999997643], 0.12: [-50.00000000000659, -26.9119999999751], 0.14: [-50.00000000000659, -22.144999999978953], 0.16: [-50.00000000000659, -19.400999999987615], 0.18: [-50.00000000000659, -4.718000000006989], 0.2: [-50.00000000000659, -25.586999999977458]}, 'action': {0.0: [90.78799999998557, 93.1209999999873], 0.02: [-18.121999999986013, 43.52100000001702], 0.04: [-40.83099999996745, 73.08899999997301], 0.06: [-35.61699999997032, 79.1579999999779], 0.08: [-27.06899999997877, 76.79799999997324], 0.1: [-31.12099999997143, 49.63200000000012], 0.12: [-36.7749999999695, 19.154000000018264], 0.14: [-25.246999999980662, 9.938999999998707], 0.16: [-24.44299999997733, 9.25699999999933], 0.18: [-21.167999999988297, 29.279000000017295], 0.2: [-25.51499999997278, 21.907000000014342]}}\n","\n","i = 2\n","while i < 8:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["2 fgsm (ut): {'goal': {0.0: [89.63199999998264, 92.69899999998768, 58.12599999998011], 0.02: [22.219000000011334, 14.479000000008135, 73.48699999997999], 0.04: [-1.7639999999994902, 50.276000000001595, 46.62300000000042], 0.06: [-20.585999999983134, 45.57100000001389, 31.775000000006486], 0.08: [-48.63500000000071, 41.828000000017774, 5.012999999996788], 0.1: [-50.00000000000659, 5.393999999998393, -22.19299999998472], 0.12: [-50.00000000000659, -21.635999999978562, -23.120999999980427], 0.14: [-50.00000000000659, -44.028999999980314, -33.385999999972995], 0.16: [-50.00000000000659, -23.011999999985015, -37.49999999997], 0.18: [-50.00000000000659, -37.35499999997054, -44.61099999998334], 0.2: [-50.00000000000659, -40.6509999999705, -50.00000000000659]}, 'action': {0.0: [88.49699999998339, 92.71999999998708, 57.479999999964804], 0.02: [45.500000000011624, 47.67099999999557, 50.946000000004986], 0.04: [-34.6019999999728, 27.390000000016094, 35.52000000001667], 0.06: [-34.890999999970354, -2.2620000000002363, 5.988999999996428], 0.08: [-29.85499999997812, -34.20399999996939, 2.3539999999944836], 0.1: [-40.6059999999697, -38.63699999997104, -12.033000000003677], 0.12: [-36.37899999997046, -32.90799999997615, -12.869999999994413], 0.14: [-41.836999999974076, -41.75699999997383, -17.534999999984958], 0.16: [-44.59799999998391, -45.401999999990764, 8.532999999992722], 0.18: [-42.20299999997276, -37.89799999997212, 3.4699999999972477], 0.2: [-40.68799999996872, -41.19799999997083, 7.899999999995397]}}\n","2 fgsm (t): {'goal': {0.0: [88.27999999998546, 93.59499999998805, 49.65299999998286], 0.02: [-47.398999999995304, 84.3629999999788, 71.56499999997384], 0.04: [-50.00000000000659, 76.70199999996719, 46.14900000000103], 0.06: [-50.00000000000659, 11.929000000015035, 56.05900000000102], 0.08: [-50.00000000000659, 12.72600000001143, 65.51899999998896], 0.1: [-50.00000000000659, 7.328999999997643, 65.06199999998434], 0.12: [-50.00000000000659, -26.9119999999751, 51.570000000003795], 0.14: [-50.00000000000659, -22.144999999978953, 8.455999999998298], 0.16: [-50.00000000000659, -19.400999999987615, 16.34100000000907], 0.18: [-50.00000000000659, -4.718000000006989, 16.45300000000846], 0.2: [-50.00000000000659, -25.586999999977458, 11.17399999999582]}, 'action': {0.0: [90.78799999998557, 93.1209999999873, 59.10099999996636], 0.02: [-18.121999999986013, 43.52100000001702, 61.964999999977735], 0.04: [-40.83099999996745, 73.08899999997301, 52.55099999998633], 0.06: [-35.61699999997032, 79.1579999999779, 70.84399999998104], 0.08: [-27.06899999997877, 76.79799999997324, 69.45699999997468], 0.1: [-31.12099999997143, 49.63200000000012, 73.73899999997393], 0.12: [-36.7749999999695, 19.154000000018264, 79.02699999998104], 0.14: [-25.246999999980662, 9.938999999998707, 74.34899999997157], 0.16: [-24.44299999997733, 9.25699999999933, 73.7799999999847], 0.18: [-21.167999999988297, 29.279000000017295, 75.6129999999847], 0.2: [-25.51499999997278, 21.907000000014342, 67.1749999999742]}}\n","3 fgsm (ut): {'goal': {0.0: [89.63199999998264, 92.69899999998768, 58.12599999998011, 91.73699999999084], 0.02: [22.219000000011334, 14.479000000008135, 73.48699999997999, -18.07399999999127], 0.04: [-1.7639999999994902, 50.276000000001595, 46.62300000000042, -33.96099999997297], 0.06: [-20.585999999983134, 45.57100000001389, 31.775000000006486, -42.81299999998452], 0.08: [-48.63500000000071, 41.828000000017774, 5.012999999996788, -43.35299999997899], 0.1: [-50.00000000000659, 5.393999999998393, -22.19299999998472, -45.96399999999032], 0.12: [-50.00000000000659, -21.635999999978562, -23.120999999980427, -50.00000000000659], 0.14: [-50.00000000000659, -44.028999999980314, -33.385999999972995, -48.64300000000074], 0.16: [-50.00000000000659, -23.011999999985015, -37.49999999997, -45.981999999991515], 0.18: [-50.00000000000659, -37.35499999997054, -44.61099999998334, -46.34399999999056], 0.2: [-50.00000000000659, -40.6509999999705, -50.00000000000659, -47.55799999999588]}, 'action': {0.0: [88.49699999998339, 92.71999999998708, 57.479999999964804, 92.65299999999387], 0.02: [45.500000000011624, 47.67099999999557, 50.946000000004986, -34.02999999997313], 0.04: [-34.6019999999728, 27.390000000016094, 35.52000000001667, -41.367999999969726], 0.06: [-34.890999999970354, -2.2620000000002363, 5.988999999996428, -47.139999999995226], 0.08: [-29.85499999997812, -34.20399999996939, 2.3539999999944836, -45.020999999985726], 0.1: [-40.6059999999697, -38.63699999997104, -12.033000000003677, -43.92099999999015], 0.12: [-36.37899999997046, -32.90799999997615, -12.869999999994413, -43.84699999998079], 0.14: [-41.836999999974076, -41.75699999997383, -17.534999999984958, -42.31099999998225], 0.16: [-44.59799999998391, -45.401999999990764, 8.532999999992722, -50.00000000000659], 0.18: [-42.20299999997276, -37.89799999997212, 3.4699999999972477, -37.2449999999714], 0.2: [-40.68799999996872, -41.19799999997083, 7.899999999995397, -45.25299999998795]}}\n","3 fgsm (t): {'goal': {0.0: [88.27999999998546, 93.59499999998805, 49.65299999998286, 94.52699999999425], 0.02: [-47.398999999995304, 84.3629999999788, 71.56499999997384, -31.04399999997793], 0.04: [-50.00000000000659, 76.70199999996719, 46.14900000000103, -23.79199999998481], 0.06: [-50.00000000000659, 11.929000000015035, 56.05900000000102, -21.547999999980462], 0.08: [-50.00000000000659, 12.72600000001143, 65.51899999998896, -36.859999999970384], 0.1: [-50.00000000000659, 7.328999999997643, 65.06199999998434, -37.69699999996848], 0.12: [-50.00000000000659, -26.9119999999751, 51.570000000003795, -20.5819999999881], 0.14: [-50.00000000000659, -22.144999999978953, 8.455999999998298, -20.912999999976822], 0.16: [-50.00000000000659, -19.400999999987615, 16.34100000000907, -27.00899999998002], 0.18: [-50.00000000000659, -4.718000000006989, 16.45300000000846, -29.16399999997103], 0.2: [-50.00000000000659, -25.586999999977458, 11.17399999999582, -35.74399999997046]}, 'action': {0.0: [90.78799999998557, 93.1209999999873, 59.10099999996636, 84.91899999998921], 0.02: [-18.121999999986013, 43.52100000001702, 61.964999999977735, 0.1819999999997971], 0.04: [-40.83099999996745, 73.08899999997301, 52.55099999998633, 19.73000000000508], 0.06: [-35.61699999997032, 79.1579999999779, 70.84399999998104, -9.235000000003451], 0.08: [-27.06899999997877, 76.79799999997324, 69.45699999997468, -16.845999999994405], 0.1: [-31.12099999997143, 49.63200000000012, 73.73899999997393, -24.85999999997823], 0.12: [-36.7749999999695, 19.154000000018264, 79.02699999998104, -23.495999999980263], 0.14: [-25.246999999980662, 9.938999999998707, 74.34899999997157, -27.26399999997893], 0.16: [-24.44299999997733, 9.25699999999933, 73.7799999999847, -29.439999999976145], 0.18: [-21.167999999988297, 29.279000000017295, 75.6129999999847, -12.215999999986911], 0.2: [-25.51499999997278, 21.907000000014342, 67.1749999999742, -25.489999999982512]}}\n","4 fgsm (ut): {'goal': {0.0: [89.63199999998264, 92.69899999998768, 58.12599999998011, 91.73699999999084, 90.71399999999345], 0.02: [22.219000000011334, 14.479000000008135, 73.48699999997999, -18.07399999999127, 4.102999999998644], 0.04: [-1.7639999999994902, 50.276000000001595, 46.62300000000042, -33.96099999997297, 18.439000000011468], 0.06: [-20.585999999983134, 45.57100000001389, 31.775000000006486, -42.81299999998452, 23.42900000001259], 0.08: [-48.63500000000071, 41.828000000017774, 5.012999999996788, -43.35299999997899, 13.305000000010141], 0.1: [-50.00000000000659, 5.393999999998393, -22.19299999998472, -45.96399999999032, -3.150000000004022], 0.12: [-50.00000000000659, -21.635999999978562, -23.120999999980427, -50.00000000000659, -17.05299999998856], 0.14: [-50.00000000000659, -44.028999999980314, -33.385999999972995, -48.64300000000074, -35.10199999997067], 0.16: [-50.00000000000659, -23.011999999985015, -37.49999999997, -45.981999999991515, -36.82199999997521], 0.18: [-50.00000000000659, -37.35499999997054, -44.61099999998334, -46.34399999999056, -38.581999999967906], 0.2: [-50.00000000000659, -40.6509999999705, -50.00000000000659, -47.55799999999588, -34.8669999999731]}, 'action': {0.0: [88.49699999998339, 92.71999999998708, 57.479999999964804, 92.65299999999387, 93.17899999998932], 0.02: [45.500000000011624, 47.67099999999557, 50.946000000004986, -34.02999999997313, -21.925999999988257], 0.04: [-34.6019999999728, 27.390000000016094, 35.52000000001667, -41.367999999969726, -7.167000000004276], 0.06: [-34.890999999970354, -2.2620000000002363, 5.988999999996428, -47.139999999995226, 13.056999999995849], 0.08: [-29.85499999997812, -34.20399999996939, 2.3539999999944836, -45.020999999985726, 5.394999999998148], 0.1: [-40.6059999999697, -38.63699999997104, -12.033000000003677, -43.92099999999015, 14.002999999995733], 0.12: [-36.37899999997046, -32.90799999997615, -12.869999999994413, -43.84699999998079, 6.784999999994918], 0.14: [-41.836999999974076, -41.75699999997383, -17.534999999984958, -42.31099999998225, 5.461999999994742], 0.16: [-44.59799999998391, -45.401999999990764, 8.532999999992722, -50.00000000000659, -25.035999999970713], 0.18: [-42.20299999997276, -37.89799999997212, 3.4699999999972477, -37.2449999999714, -28.938999999974364], 0.2: [-40.68799999996872, -41.19799999997083, 7.899999999995397, -45.25299999998795, -33.67299999997397]}}\n","4 fgsm (t): {'goal': {0.0: [88.27999999998546, 93.59499999998805, 49.65299999998286, 94.52699999999425, 91.12499999999362], 0.02: [-47.398999999995304, 84.3629999999788, 71.56499999997384, -31.04399999997793, 43.51099999999971], 0.04: [-50.00000000000659, 76.70199999996719, 46.14900000000103, -23.79199999998481, 53.5809999999836], 0.06: [-50.00000000000659, 11.929000000015035, 56.05900000000102, -21.547999999980462, 39.924000000014054], 0.08: [-50.00000000000659, 12.72600000001143, 65.51899999998896, -36.859999999970384, 24.747000000011443], 0.1: [-50.00000000000659, 7.328999999997643, 65.06199999998434, -37.69699999996848, -23.513999999979454], 0.12: [-50.00000000000659, -26.9119999999751, 51.570000000003795, -20.5819999999881, -32.330999999975944], 0.14: [-50.00000000000659, -22.144999999978953, 8.455999999998298, -20.912999999976822, -20.6409999999924], 0.16: [-50.00000000000659, -19.400999999987615, 16.34100000000907, -27.00899999998002, -38.46699999997006], 0.18: [-50.00000000000659, -4.718000000006989, 16.45300000000846, -29.16399999997103, -42.52499999997621], 0.2: [-50.00000000000659, -25.586999999977458, 11.17399999999582, -35.74399999997046, -48.70500000000551]}, 'action': {0.0: [90.78799999998557, 93.1209999999873, 59.10099999996636, 84.91899999998921, 93.80699999998919], 0.02: [-18.121999999986013, 43.52100000001702, 61.964999999977735, 0.1819999999997971, 63.729999999989], 0.04: [-40.83099999996745, 73.08899999997301, 52.55099999998633, 19.73000000000508, 82.87899999998001], 0.06: [-35.61699999997032, 79.1579999999779, 70.84399999998104, -9.235000000003451, 85.48999999998163], 0.08: [-27.06899999997877, 76.79799999997324, 69.45699999997468, -16.845999999994405, 74.28299999998512], 0.1: [-31.12099999997143, 49.63200000000012, 73.73899999997393, -24.85999999997823, 66.02299999998448], 0.12: [-36.7749999999695, 19.154000000018264, 79.02699999998104, -23.495999999980263, 50.99699999998881], 0.14: [-25.246999999980662, 9.938999999998707, 74.34899999997157, -27.26399999997893, 43.396000000018056], 0.16: [-24.44299999997733, 9.25699999999933, 73.7799999999847, -29.439999999976145, 33.16300000001588], 0.18: [-21.167999999988297, 29.279000000017295, 75.6129999999847, -12.215999999986911, 28.704000000015427], 0.2: [-25.51499999997278, 21.907000000014342, 67.1749999999742, -25.489999999982512, 15.387000000011756]}}\n","5 fgsm (ut): {'goal': {0.0: [89.63199999998264, 92.69899999998768, 58.12599999998011, 91.73699999999084, 90.71399999999345, 78.43999999997285], 0.02: [22.219000000011334, 14.479000000008135, 73.48699999997999, -18.07399999999127, 4.102999999998644, 56.00899999999482], 0.04: [-1.7639999999994902, 50.276000000001595, 46.62300000000042, -33.96099999997297, 18.439000000011468, 20.191000000020477], 0.06: [-20.585999999983134, 45.57100000001389, 31.775000000006486, -42.81299999998452, 23.42900000001259, 50.46899999999884], 0.08: [-48.63500000000071, 41.828000000017774, 5.012999999996788, -43.35299999997899, 13.305000000010141, 54.548999999998905], 0.1: [-50.00000000000659, 5.393999999998393, -22.19299999998472, -45.96399999999032, -3.150000000004022, 41.69700000000989], 0.12: [-50.00000000000659, -21.635999999978562, -23.120999999980427, -50.00000000000659, -17.05299999998856, 18.968000000011113], 0.14: [-50.00000000000659, -44.028999999980314, -33.385999999972995, -48.64300000000074, -35.10199999997067, 3.552999999994695], 0.16: [-50.00000000000659, -23.011999999985015, -37.49999999997, -45.981999999991515, -36.82199999997521, -10.834000000003131], 0.18: [-50.00000000000659, -37.35499999997054, -44.61099999998334, -46.34399999999056, -38.581999999967906, -35.55799999996861], 0.2: [-50.00000000000659, -40.6509999999705, -50.00000000000659, -47.55799999999588, -34.8669999999731, -38.62199999996926]}, 'action': {0.0: [88.49699999998339, 92.71999999998708, 57.479999999964804, 92.65299999999387, 93.17899999998932, 86.02099999998923], 0.02: [45.500000000011624, 47.67099999999557, 50.946000000004986, -34.02999999997313, -21.925999999988257, 4.460999999997174], 0.04: [-34.6019999999728, 27.390000000016094, 35.52000000001667, -41.367999999969726, -7.167000000004276, 36.747000000016584], 0.06: [-34.890999999970354, -2.2620000000002363, 5.988999999996428, -47.139999999995226, 13.056999999995849, 42.60700000001229], 0.08: [-29.85499999997812, -34.20399999996939, 2.3539999999944836, -45.020999999985726, 5.394999999998148, 48.89399999998337], 0.1: [-40.6059999999697, -38.63699999997104, -12.033000000003677, -43.92099999999015, 14.002999999995733, 57.736999999982416], 0.12: [-36.37899999997046, -32.90799999997615, -12.869999999994413, -43.84699999998079, 6.784999999994918, 41.35000000000391], 0.14: [-41.836999999974076, -41.75699999997383, -17.534999999984958, -42.31099999998225, 5.461999999994742, 36.14200000001346], 0.16: [-44.59799999998391, -45.401999999990764, 8.532999999992722, -50.00000000000659, -25.035999999970713, 37.625000000018716], 0.18: [-42.20299999997276, -37.89799999997212, 3.4699999999972477, -37.2449999999714, -28.938999999974364, 17.667000000013324], 0.2: [-40.68799999996872, -41.19799999997083, 7.899999999995397, -45.25299999998795, -33.67299999997397, 10.519000000009276]}}\n","5 fgsm (t): {'goal': {0.0: [88.27999999998546, 93.59499999998805, 49.65299999998286, 94.52699999999425, 91.12499999999362, 74.07299999998688], 0.02: [-47.398999999995304, 84.3629999999788, 71.56499999997384, -31.04399999997793, 43.51099999999971, 72.95599999999138], 0.04: [-50.00000000000659, 76.70199999996719, 46.14900000000103, -23.79199999998481, 53.5809999999836, 47.094000000004804], 0.06: [-50.00000000000659, 11.929000000015035, 56.05900000000102, -21.547999999980462, 39.924000000014054, 63.44399999997134], 0.08: [-50.00000000000659, 12.72600000001143, 65.51899999998896, -36.859999999970384, 24.747000000011443, 34.73500000001853], 0.1: [-50.00000000000659, 7.328999999997643, 65.06199999998434, -37.69699999996848, -23.513999999979454, 13.229000000002463], 0.12: [-50.00000000000659, -26.9119999999751, 51.570000000003795, -20.5819999999881, -32.330999999975944, 8.718999999995624], 0.14: [-50.00000000000659, -22.144999999978953, 8.455999999998298, -20.912999999976822, -20.6409999999924, -6.065000000005092], 0.16: [-50.00000000000659, -19.400999999987615, 16.34100000000907, -27.00899999998002, -38.46699999997006, -0.1950000000014309], 0.18: [-50.00000000000659, -4.718000000006989, 16.45300000000846, -29.16399999997103, -42.52499999997621, 9.62099999999907], 0.2: [-50.00000000000659, -25.586999999977458, 11.17399999999582, -35.74399999997046, -48.70500000000551, 14.472000000016138]}, 'action': {0.0: [90.78799999998557, 93.1209999999873, 59.10099999996636, 84.91899999998921, 93.80699999998919, 76.1719999999775], 0.02: [-18.121999999986013, 43.52100000001702, 61.964999999977735, 0.1819999999997971, 63.729999999989, 72.34999999998973], 0.04: [-40.83099999996745, 73.08899999997301, 52.55099999998633, 19.73000000000508, 82.87899999998001, 79.51199999998394], 0.06: [-35.61699999997032, 79.1579999999779, 70.84399999998104, -9.235000000003451, 85.48999999998163, 58.3809999999973], 0.08: [-27.06899999997877, 76.79799999997324, 69.45699999997468, -16.845999999994405, 74.28299999998512, 62.664999999978114], 0.1: [-31.12099999997143, 49.63200000000012, 73.73899999997393, -24.85999999997823, 66.02299999998448, 56.30299999998029], 0.12: [-36.7749999999695, 19.154000000018264, 79.02699999998104, -23.495999999980263, 50.99699999998881, 49.36699999999687], 0.14: [-25.246999999980662, 9.938999999998707, 74.34899999997157, -27.26399999997893, 43.396000000018056, 52.489999999986], 0.16: [-24.44299999997733, 9.25699999999933, 73.7799999999847, -29.439999999976145, 33.16300000001588, 50.882000000001305], 0.18: [-21.167999999988297, 29.279000000017295, 75.6129999999847, -12.215999999986911, 28.704000000015427, 48.19899999999836], 0.2: [-25.51499999997278, 21.907000000014342, 67.1749999999742, -25.489999999982512, 15.387000000011756, 62.39499999998043]}}\n","6 fgsm (ut): {'goal': {0.0: [89.63199999998264, 92.69899999998768, 58.12599999998011, 91.73699999999084, 90.71399999999345, 78.43999999997285, 88.1789999999915], 0.02: [22.219000000011334, 14.479000000008135, 73.48699999997999, -18.07399999999127, 4.102999999998644, 56.00899999999482, 38.807000000009054], 0.04: [-1.7639999999994902, 50.276000000001595, 46.62300000000042, -33.96099999997297, 18.439000000011468, 20.191000000020477, 47.95499999997735], 0.06: [-20.585999999983134, 45.57100000001389, 31.775000000006486, -42.81299999998452, 23.42900000001259, 50.46899999999884, 54.01999999999525], 0.08: [-48.63500000000071, 41.828000000017774, 5.012999999996788, -43.35299999997899, 13.305000000010141, 54.548999999998905, 42.56000000000367], 0.1: [-50.00000000000659, 5.393999999998393, -22.19299999998472, -45.96399999999032, -3.150000000004022, 41.69700000000989, 48.71000000000784], 0.12: [-50.00000000000659, -21.635999999978562, -23.120999999980427, -50.00000000000659, -17.05299999998856, 18.968000000011113, 43.683000000008825], 0.14: [-50.00000000000659, -44.028999999980314, -33.385999999972995, -48.64300000000074, -35.10199999997067, 3.552999999994695, 16.352000000012463], 0.16: [-50.00000000000659, -23.011999999985015, -37.49999999997, -45.981999999991515, -36.82199999997521, -10.834000000003131, 23.167000000007743], 0.18: [-50.00000000000659, -37.35499999997054, -44.61099999998334, -46.34399999999056, -38.581999999967906, -35.55799999996861, 51.41999999998953], 0.2: [-50.00000000000659, -40.6509999999705, -50.00000000000659, -47.55799999999588, -34.8669999999731, -38.62199999996926, 7.11599999999889]}, 'action': {0.0: [88.49699999998339, 92.71999999998708, 57.479999999964804, 92.65299999999387, 93.17899999998932, 86.02099999998923, 83.82199999999246], 0.02: [45.500000000011624, 47.67099999999557, 50.946000000004986, -34.02999999997313, -21.925999999988257, 4.460999999997174, 42.08999999999256], 0.04: [-34.6019999999728, 27.390000000016094, 35.52000000001667, -41.367999999969726, -7.167000000004276, 36.747000000016584, 37.126000000015786], 0.06: [-34.890999999970354, -2.2620000000002363, 5.988999999996428, -47.139999999995226, 13.056999999995849, 42.60700000001229, 31.014000000015194], 0.08: [-29.85499999997812, -34.20399999996939, 2.3539999999944836, -45.020999999985726, 5.394999999998148, 48.89399999998337, 14.058999999998143], 0.1: [-40.6059999999697, -38.63699999997104, -12.033000000003677, -43.92099999999015, 14.002999999995733, 57.736999999982416, -24.91899999997274], 0.12: [-36.37899999997046, -32.90799999997615, -12.869999999994413, -43.84699999998079, 6.784999999994918, 41.35000000000391, -26.4859999999791], 0.14: [-41.836999999974076, -41.75699999997383, -17.534999999984958, -42.31099999998225, 5.461999999994742, 36.14200000001346, -19.490999999989075], 0.16: [-44.59799999998391, -45.401999999990764, 8.532999999992722, -50.00000000000659, -25.035999999970713, 37.625000000018716, -26.764999999980752], 0.18: [-42.20299999997276, -37.89799999997212, 3.4699999999972477, -37.2449999999714, -28.938999999974364, 17.667000000013324, -16.70899999998986], 0.2: [-40.68799999996872, -41.19799999997083, 7.899999999995397, -45.25299999998795, -33.67299999997397, 10.519000000009276, -44.416999999982636]}}\n","6 fgsm (t): {'goal': {0.0: [88.27999999998546, 93.59499999998805, 49.65299999998286, 94.52699999999425, 91.12499999999362, 74.07299999998688, 91.22499999999265], 0.02: [-47.398999999995304, 84.3629999999788, 71.56499999997384, -31.04399999997793, 43.51099999999971, 72.95599999999138, 27.584000000012992], 0.04: [-50.00000000000659, 76.70199999996719, 46.14900000000103, -23.79199999998481, 53.5809999999836, 47.094000000004804, 85.88199999998413], 0.06: [-50.00000000000659, 11.929000000015035, 56.05900000000102, -21.547999999980462, 39.924000000014054, 63.44399999997134, 92.89399999999212], 0.08: [-50.00000000000659, 12.72600000001143, 65.51899999998896, -36.859999999970384, 24.747000000011443, 34.73500000001853, 42.800000000012076], 0.1: [-50.00000000000659, 7.328999999997643, 65.06199999998434, -37.69699999996848, -23.513999999979454, 13.229000000002463, -41.17499999997036], 0.12: [-50.00000000000659, -26.9119999999751, 51.570000000003795, -20.5819999999881, -32.330999999975944, 8.718999999995624, -43.34199999997873], 0.14: [-50.00000000000659, -22.144999999978953, 8.455999999998298, -20.912999999976822, -20.6409999999924, -6.065000000005092, -23.354999999981455], 0.16: [-50.00000000000659, -19.400999999987615, 16.34100000000907, -27.00899999998002, -38.46699999997006, -0.1950000000014309, -35.06399999997106], 0.18: [-50.00000000000659, -4.718000000006989, 16.45300000000846, -29.16399999997103, -42.52499999997621, 9.62099999999907, -33.65599999997389], 0.2: [-50.00000000000659, -25.586999999977458, 11.17399999999582, -35.74399999997046, -48.70500000000551, 14.472000000016138, -45.92799999999359]}, 'action': {0.0: [90.78799999998557, 93.1209999999873, 59.10099999996636, 84.91899999998921, 93.80699999998919, 76.1719999999775, 86.65299999998798], 0.02: [-18.121999999986013, 43.52100000001702, 61.964999999977735, 0.1819999999997971, 63.729999999989, 72.34999999998973, 59.46899999999158], 0.04: [-40.83099999996745, 73.08899999997301, 52.55099999998633, 19.73000000000508, 82.87899999998001, 79.51199999998394, 65.64899999997618], 0.06: [-35.61699999997032, 79.1579999999779, 70.84399999998104, -9.235000000003451, 85.48999999998163, 58.3809999999973, 53.538999999983716], 0.08: [-27.06899999997877, 76.79799999997324, 69.45699999997468, -16.845999999994405, 74.28299999998512, 62.664999999978114, 22.031000000009712], 0.1: [-31.12099999997143, 49.63200000000012, 73.73899999997393, -24.85999999997823, 66.02299999998448, 56.30299999998029, 43.182000000017425], 0.12: [-36.7749999999695, 19.154000000018264, 79.02699999998104, -23.495999999980263, 50.99699999998881, 49.36699999999687, 49.805999999992395], 0.14: [-25.246999999980662, 9.938999999998707, 74.34899999997157, -27.26399999997893, 43.396000000018056, 52.489999999986, 28.242000000016297], 0.16: [-24.44299999997733, 9.25699999999933, 73.7799999999847, -29.439999999976145, 33.16300000001588, 50.882000000001305, 27.304000000022416], 0.18: [-21.167999999988297, 29.279000000017295, 75.6129999999847, -12.215999999986911, 28.704000000015427, 48.19899999999836, 28.403000000014814], 0.2: [-25.51499999997278, 21.907000000014342, 67.1749999999742, -25.489999999982512, 15.387000000011756, 62.39499999998043, 24.5070000000138]}}\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-358d767728b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mapply_fgsm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntargeted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mapply_fgsm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntargeted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'goal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i} fgsm (ut): {untargeted}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-d7da4b822a44>\u001b[0m in \u001b[0;36mapply_fgsm\u001b[0;34m(agent, episode_durations, goal_attack, targeted)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0;31m#goal_done = agent.goal_reached(action, goal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mgoal_reached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_reached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-bdcdf7f0b419>\u001b[0m in \u001b[0;36mgoal_reached\u001b[0;34m(self, state, goal, next_state, threshold)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_dim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgoal_reached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_dim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"8pnki1eCngGQ"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointMazeEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmUPiWlPqUts","executionInfo":{"status":"ok","timestamp":1616663003725,"user_tz":0,"elapsed":3128507,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"70487cbe-c3e7-4774-d4c1-1d4ac4039409"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 1\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 scale: {1.0: [8.74999999999583], 1.5: [69.19699999996914], 2.0: [47.82500000000049], 2.5: [86.61299999997932], 3.0: [79.5679999999713], 3.5: [81.04199999997928], 4.0: [93.19499999998936], 4.5: [94.84599999999077], 5.0: [95.07499999999136], 5.5: [94.95799999999116], 6.0: [87.24199999997923], 6.5: [91.59399999998874], 7.0: [72.72199999998386]}\n","2 scale: {1.0: [8.74999999999583, -46.09599999999051], 1.5: [69.19699999996914, 63.01099999999226], 2.0: [47.82500000000049, 96.87999999999765], 2.5: [86.61299999997932, 73.29699999999026], 3.0: [79.5679999999713, 67.86699999998353], 3.5: [81.04199999997928, 78.12699999998014], 4.0: [93.19499999998936, 55.85399999998363], 4.5: [94.84599999999077, 32.357000000021074], 5.0: [95.07499999999136, 31.005000000015926], 5.5: [94.95799999999116, 32.563000000020686], 6.0: [87.24199999997923, 54.996999999976694], 6.5: [91.59399999998874, 46.548000000001366], 7.0: [72.72199999998386, 42.8470000000059]}\n","3 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226]}\n","4 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892]}\n","5 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877]}\n","6 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665]}\n","7 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659]}\n","8 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625]}\n","9 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486, -4.705000000003497], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664, 53.082999999995494], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772, 63.824999999984094], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625, 39.19500000001303], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448, 57.531999999995485], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618, 85.19299999998182], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124, 91.80599999998562], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378, 92.65899999999017], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172, 44.88400000000259], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389, -34.10299999997367], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675, -50.00000000000659], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899, -50.00000000000659], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625, -50.00000000000659]}\n","10 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486, -4.705000000003497, 50.45899999999232], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664, 53.082999999995494, 43.20900000000344], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772, 63.824999999984094, 75.03099999997703], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625, 39.19500000001303, 64.72699999998635], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448, 57.531999999995485, 81.25399999998028], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618, 85.19299999998182, 81.76699999998601], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124, 91.80599999998562, 92.123999999994], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378, 92.65899999999017, 64.8469999999827], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172, 44.88400000000259, 13.362000000006809], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389, -34.10299999997367, -26.754999999976526], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675, -50.00000000000659, -32.89599999997283], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899, -50.00000000000659, -21.657999999986068], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625, -50.00000000000659, -24.064999999980717]}\n","11 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486, -4.705000000003497, 50.45899999999232, 60.72099999998626], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664, 53.082999999995494, 43.20900000000344, 50.35299999998265], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772, 63.824999999984094, 75.03099999997703, 30.04700000001344], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625, 39.19500000001303, 64.72699999998635, 48.00800000000597], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448, 57.531999999995485, 81.25399999998028, 75.18099999998248], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618, 85.19299999998182, 81.76699999998601, 79.2829999999895], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124, 91.80599999998562, 92.123999999994, 89.01499999998865], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378, 92.65899999999017, 64.8469999999827, 69.43699999999546], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172, 44.88400000000259, 13.362000000006809, -37.076999999973744], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389, -34.10299999997367, -26.754999999976526, -42.873999999976114], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675, -50.00000000000659, -32.89599999997283, -20.35899999998607], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899, -50.00000000000659, -21.657999999986068, 8.840000000016996], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625, -50.00000000000659, -24.064999999980717, -22.217999999979032]}\n","12 scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486, -4.705000000003497, 50.45899999999232, 60.72099999998626, -34.688999999969404], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664, 53.082999999995494, 43.20900000000344, 50.35299999998265, 22.507000000014408], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772, 63.824999999984094, 75.03099999997703, 30.04700000001344, -4.96100000000725], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625, 39.19500000001303, 64.72699999998635, 48.00800000000597, -3.433000000004373], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448, 57.531999999995485, 81.25399999998028, 75.18099999998248, 36.890000000015874], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618, 85.19299999998182, 81.76699999998601, 79.2829999999895, 69.34199999998806], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124, 91.80599999998562, 92.123999999994, 89.01499999998865, 96.20599999999642], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378, 92.65899999999017, 64.8469999999827, 69.43699999999546, 94.8679999999918], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172, 44.88400000000259, 13.362000000006809, -37.076999999973744, 42.51800000001154], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389, -34.10299999997367, -26.754999999976526, -42.873999999976114, 42.04400000001285], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675, -50.00000000000659, -32.89599999997283, -20.35899999998607, -16.255999999993563], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899, -50.00000000000659, -21.657999999986068, 8.840000000016996, -31.202999999973073], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625, -50.00000000000659, -24.064999999980717, -22.217999999979032, -36.3359999999754]}\n","----\n","scale: {1.0: [8.74999999999583, -46.09599999999051, -10.270999999996173, -14.85899999999904, 25.36600000000382, -5.808000000006542, -13.087999999983388, -31.823999999974486, -4.705000000003497, 50.45899999999232, 60.72099999998626, -34.688999999969404], 1.5: [69.19699999996914, 63.01099999999226, -25.541999999973118, 73.09799999998572, 54.82399999999498, 36.18300000001324, -16.904999999996438, -9.224999999996664, 53.082999999995494, 43.20900000000344, 50.35299999998265, 22.507000000014408], 2.0: [47.82500000000049, 96.87999999999765, 12.447000000008773, 68.89099999998002, 51.271000000005046, 38.70900000001606, 58.725999999993356, 81.3729999999772, 63.824999999984094, 75.03099999997703, 30.04700000001344, -4.96100000000725], 2.5: [86.61299999997932, 73.29699999999026, 59.6369999999939, 43.676000000001366, 55.21699999999486, 47.76400000000321, 35.2690000000168, 41.96500000000625, 39.19500000001303, 64.72699999998635, 48.00800000000597, -3.433000000004373], 3.0: [79.5679999999713, 67.86699999998353, 83.5409999999943, 92.96499999999249, 55.15600000000106, 77.31099999998715, 33.72800000000897, 97.30199999999448, 57.531999999995485, 81.25399999998028, 75.18099999998248, 36.890000000015874], 3.5: [81.04199999997928, 78.12699999998014, 89.57499999999122, 96.03999999999243, 50.34700000000229, 83.77799999999026, 56.73499999999469, 97.92299999999618, 85.19299999998182, 81.76699999998601, 79.2829999999895, 69.34199999998806], 4.0: [93.19499999998936, 55.85399999998363, 93.1409999999893, 92.62399999999133, 81.29199999998278, 89.07099999999386, 60.577999999984385, 92.80199999999124, 91.80599999998562, 92.123999999994, 89.01499999998865, 96.20599999999642], 4.5: [94.84599999999077, 32.357000000021074, 60.38199999999133, 86.90699999998593, 86.03899999998214, 44.63299999999936, -33.724999999981236, 96.54899999999378, 92.65899999999017, 64.8469999999827, 69.43699999999546, 94.8679999999918], 5.0: [95.07499999999136, 31.005000000015926, -1.7460000000017883, 81.67699999998098, 91.81899999998711, 51.37100000000109, -33.06699999997338, 90.08399999999172, 44.88400000000259, 13.362000000006809, -37.076999999973744, 42.51800000001154], 5.5: [94.95799999999116, 32.563000000020686, -50.00000000000659, 80.85199999997656, 73.16399999997934, 66.73599999997406, -47.198999999999124, 64.36899999999389, -34.10299999997367, -26.754999999976526, -42.873999999976114, 42.04400000001285], 6.0: [87.24199999997923, 54.996999999976694, -13.915999999989204, 81.88099999998802, 80.89299999998525, 88.398999999985, -50.00000000000659, 58.685999999996675, -50.00000000000659, -32.89599999997283, -20.35899999998607, -16.255999999993563], 6.5: [91.59399999998874, 46.548000000001366, -9.310000000007435, 62.4539999999841, 71.47499999997976, 88.77099999998741, -50.00000000000659, 68.11099999997899, -50.00000000000659, -21.657999999986068, 8.840000000016996, -31.202999999973073], 7.0: [72.72199999998386, 42.8470000000059, -38.330999999971226, 60.82699999998892, 84.55899999997877, 65.24499999998665, -50.00000000000659, 49.155999999989625, -50.00000000000659, -24.064999999980717, -22.217999999979032, -36.3359999999754]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gd2_86AIqOt4"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJjpZcCLqjua","executionInfo":{"status":"ok","timestamp":1617013288075,"user_tz":-60,"elapsed":451142,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"786dfe8d-5220-4aa4-9eae-dd8f85cc7705"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","env = NormalizedEnv(PointMazeEnv(4))\n","i = 1\n","while i < 7:\n","    if i == 2:\n","        i +=1 \n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 range: {0.0: [92.82199999998544], 0.05: [91.98399999998786], 0.1: [92.43799999998642], 0.15: [90.82299999998472], 0.2: [85.13999999997932], 0.25: [81.94599999998168], 0.3: [78.89999999998243], 0.35: [82.0229999999837], 0.4: [85.52099999997822]}\n","3 range: {0.0: [92.82199999998544, 90.28999999998503], 0.05: [91.98399999998786, 90.56099999999176], 0.1: [92.43799999998642, 85.2229999999852], 0.15: [90.82299999998472, 77.09299999998385], 0.2: [85.13999999997932, 72.65999999998601], 0.25: [81.94599999998168, 61.15299999998189], 0.3: [78.89999999998243, 57.37299999999338], 0.35: [82.0229999999837, 58.83899999997649], 0.4: [85.52099999997822, 48.97900000000111]}\n","4 range: {0.0: [92.82199999998544, 90.28999999998503, 89.41399999998845], 0.05: [91.98399999998786, 90.56099999999176, 88.54499999998474], 0.1: [92.43799999998642, 85.2229999999852, 91.05099999998916], 0.15: [90.82299999998472, 77.09299999998385, 87.0249999999876], 0.2: [85.13999999997932, 72.65999999998601, 86.98999999998163], 0.25: [81.94599999998168, 61.15299999998189, 88.5939999999909], 0.3: [78.89999999998243, 57.37299999999338, 90.62899999998828], 0.35: [82.0229999999837, 58.83899999997649, 85.35199999998896], 0.4: [85.52099999997822, 48.97900000000111, 79.66699999999027]}\n","5 range: {0.0: [92.82199999998544, 90.28999999998503, 89.41399999998845, 82.72699999998143], 0.05: [91.98399999998786, 90.56099999999176, 88.54499999998474, 74.59899999998598], 0.1: [92.43799999998642, 85.2229999999852, 91.05099999998916, 74.84099999998116], 0.15: [90.82299999998472, 77.09299999998385, 87.0249999999876, 76.59399999998432], 0.2: [85.13999999997932, 72.65999999998601, 86.98999999998163, 59.475999999987195], 0.25: [81.94599999998168, 61.15299999998189, 88.5939999999909, 69.73099999998122], 0.3: [78.89999999998243, 57.37299999999338, 90.62899999998828, 77.22899999998101], 0.35: [82.0229999999837, 58.83899999997649, 85.35199999998896, 78.63499999998834], 0.4: [85.52099999997822, 48.97900000000111, 79.66699999999027, 71.44199999999124]}\n","6 range: {0.0: [92.82199999998544, 90.28999999998503, 89.41399999998845, 82.72699999998143, 89.8809999999922], 0.05: [91.98399999998786, 90.56099999999176, 88.54499999998474, 74.59899999998598, 64.8109999999852], 0.1: [92.43799999998642, 85.2229999999852, 91.05099999998916, 74.84099999998116, 49.60599999999229], 0.15: [90.82299999998472, 77.09299999998385, 87.0249999999876, 76.59399999998432, 49.74699999999903], 0.2: [85.13999999997932, 72.65999999998601, 86.98999999998163, 59.475999999987195, 57.94299999998625], 0.25: [81.94599999998168, 61.15299999998189, 88.5939999999909, 69.73099999998122, 33.42700000001381], 0.3: [78.89999999998243, 57.37299999999338, 90.62899999998828, 77.22899999998101, 35.21900000001682], 0.35: [82.0229999999837, 58.83899999997649, 85.35199999998896, 78.63499999998834, 38.159000000010835], 0.4: [85.52099999997822, 48.97900000000111, 79.66699999999027, 71.44199999999124, 19.049000000010345]}\n","----\n","range: {0.0: [92.82199999998544, 90.28999999998503, 89.41399999998845, 82.72699999998143, 89.8809999999922], 0.05: [91.98399999998786, 90.56099999999176, 88.54499999998474, 74.59899999998598, 64.8109999999852], 0.1: [92.43799999998642, 85.2229999999852, 91.05099999998916, 74.84099999998116, 49.60599999999229], 0.15: [90.82299999998472, 77.09299999998385, 87.0249999999876, 76.59399999998432, 49.74699999999903], 0.2: [85.13999999997932, 72.65999999998601, 86.98999999998163, 59.475999999987195, 57.94299999998625], 0.25: [81.94599999998168, 61.15299999998189, 88.5939999999909, 69.73099999998122, 33.42700000001381], 0.3: [78.89999999998243, 57.37299999999338, 90.62899999998828, 77.22899999998101, 35.21900000001682], 0.35: [82.0229999999837, 58.83899999997649, 85.35199999998896, 78.63499999998834, 38.159000000010835], 0.4: [85.52099999997822, 48.97900000000111, 79.66699999999027, 71.44199999999124, 19.049000000010345]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BhvsIWF-qrHj"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)\n","\n","def save_random_manager(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_points = 10000\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","    for _ in range(num_points):\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        noise = torch.FloatTensor(state.shape).uniform_(0.0, 1.0).to(device)\n","\n","        g_state = state_min + state_range * noise\n","        g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","\n","        goal = agent.select_goal(g_state, False, False)\n","        path[\"manager\"].append((0, g_state.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","    episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWeLBDKTP3Ao","executionInfo":{"status":"ok","timestamp":1616493270550,"user_tz":0,"elapsed":38321,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"0107b42c-f0dc-4328-af79-819acc958d8a"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        #save_trajectories(agent, episodes, False)\n","        save_random_manager(agent, episodes)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","torch.save(episodes, \"PointMaze_Freeze_manager.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y6VhJjD8QHJl"},"source":["def get_intrinsic_reward(agent):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    overall_reward = 0\n","    intr_rews = []\n","\n","    for i_episode in range(num_episodes):\n","        cur_intr = []\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        episode_steps = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                cur_intr.append(agent.intrinsic_reward(reward, state, goal, next_state).detach().item())\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","        intr_rews.append(cur_intr)\n","    print(overall_reward / num_episodes)\n","    return intr_rews"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"SBjnn7HWZTyF","executionInfo":{"status":"ok","timestamp":1616492202635,"user_tz":0,"elapsed":868,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"8fdc8c67-b902-442f-b4f1-c33f9b6077ad"},"source":["import matplotlib.pyplot as plt\n","\n","episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 6\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_freeze_{i}\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Freeze\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[3])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["97.31999999999961\n","Freeze\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"11oJHE1hZknp","executionInfo":{"status":"ok","timestamp":1616492002184,"user_tz":0,"elapsed":1263,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"b31e2135-12e2-4224-8ae0-c3f0be8a00d7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 3\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_{i}\", \"point_maze_time\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Standard\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[2])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["95.64999999999947\n","Standard\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"cpUcYkH1afDr"},"source":[""],"execution_count":null,"outputs":[]}]}