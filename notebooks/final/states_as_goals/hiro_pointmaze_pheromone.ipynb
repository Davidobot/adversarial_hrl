{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointmaze_pheromone.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617436530819,"user_tz":-60,"elapsed":18383,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"3d0c55a6-e7e6-471e-c7f3-aaefb96754cd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_maze.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617436679344,"user_tz":-60,"elapsed":429,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_maze_pheromone/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.pheromone_paths,\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name, dir=\"point_maze_pheromone\"):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{dir}/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    #model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    #model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    #model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    #model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.pheromone_paths = copy.deepcopy(checkpoint['meta_controller'])\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617436682597,"user_tz":-60,"elapsed":3349,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617436682600,"user_tz":-60,"elapsed":2434,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617436682601,"user_tz":-60,"elapsed":2095,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["from point_maze import PointMazeEnv \n","env = NormalizedEnv(PointMazeEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617436682601,"user_tz":-60,"elapsed":1598,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617436682602,"user_tz":-60,"elapsed":1328,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617436682602,"user_tz":-60,"elapsed":914,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617436683891,"user_tz":-60,"elapsed":376,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617436684699,"user_tz":-60,"elapsed":901,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 50000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 100000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.0\n","        self.EPS_DECAY = 10000 # in number of steps\n","        self.TAU = 0.001\n","\n","        self.eps_printed = False\n","\n","        self.policy_update = 2\n","        self.tot_updates = 0\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, warmup, is_training):\n","        if warmup: \n","            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            if eps_threshold <= 0.2 and not self.eps_printed:\n","                self.eps_printed = True\n","                print(\"EPS_THRESHOLD below 0.2\")\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.tot_updates += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        loss_val = loss.item()\n","\n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","\n","        if self.tot_updates % self.policy_update == 0:\n","            soft_update(target, self, self.TAU)\n","\n","        return loss_val"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617436684701,"user_tz":-60,"elapsed":623,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["from operator import itemgetter\n","class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions = nb_actions\n","\n","        self.goal_dim = [0, 1]\n","        \n","        # a list of tuple of form (reward, path); keep top 5\n","        self.pheromone_paths = []\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 500000\n","\n","    def add_path(self, reward, path):\n","        # prefer higher-reward paths\n","        self.pheromone_paths.append((reward, path))\n","        self.pheromone_paths.sort(key=itemgetter(0), reverse=True)\n","        self.pheromone_paths = self.pheromone_paths[:5] # only keep top 5\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        if warmup or len(self.pheromone_paths) == 0:\n","            return torch.tensor([np.random.uniform(-1.,1.,len(self.goal_dim))], device=device, dtype=torch.float)\n","        \n","        time_index = 3\n","        #cur_t = s_t.squeeze(0)[time_index] # time\n","        cur_pos = s_t.squeeze(0)[self.goal_dim]\n","\n","        goal = torch.tensor([0] * len(self.goal_dim), device=device, dtype=torch.float)\n","\n","        min_rew = -60 # min(self.pheromone_paths, key = lambda t: t[0])[0]\n","        tot_rew = sum([t[0] for t in self.pheromone_paths]) - len(self.pheromone_paths) * min_rew\n","\n","        for rew, path in self.pheromone_paths:\n","            breakdown = tuple(map(torch.stack, zip(*path)))\n","            positions = torch.stack([breakdown[i] for i in self.goal_dim], axis=-1)\n","            chosen_i = torch.argmin(torch.pow(torch.sum(torch.pow(positions - cur_pos, 2), axis=1), 0.5))\n","            \n","            # assume c = 10\n","            # basically, in chosen path, go 10 steps ahead from position closest\n","            # to the currently observed one\n","            chosen_point = path[min(chosen_i + 10, len(path) - 1)]\n","\n","            #chosen_point = path[torch.argmin(torch.abs(breakdown[time_index] - cur_t))]\n","            goal += (rew - min_rew) * chosen_point[self.goal_dim]\n","        \n","        goal /= tot_rew\n","        goal = goal - s_t.squeeze(0)[self.goal_dim] # make goal relative to given position\n","\n","        return goal.unsqueeze(0)\n","    \n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 6\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        state_seq = None\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        # once episode finishes, append full path to manager\n","        agent.add_path(overall_reward, state_seq)\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1617436710871,"user_tz":-60,"elapsed":6097,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, targeted):\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            goal = agent.select_goal(og_state, False, False)\n","            state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    goal_temp = agent.h(state, goal, next_og_state)\n","                    next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrR0kvDhFwRa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616806405731,"user_tz":0,"elapsed":8377504,"user":{"displayName":"David Khachaturov","photoUrl":"","userId":"00449378819393486934"}},"outputId":"fdd88b9b-4e4a-4c5d-bc5f-31737fe680c9"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\n","untargeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['both', 'goal_only', 'action_only']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 4\n","while i < 6:\n","    agent = train_model()\n","    #agent = HIRO(n_observations, n_actions).to(device)\n","    #load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100: -48.81300000000043\n","200: -47.76000000000042\n","300: -50.000000000000426\n","Unlucky after 300 eps! Terminating...\n","100: -46.84100000000042\n","200: -43.92000000000042\n","300: -45.45500000000042\n","400: -18.2820000000004\n","500: -37.53400000000042\n","600: -27.34800000000042\n","700: 18.40499999999971\n","800: 50.47699999999977\n","900: 41.797999999999845\n","1000: 53.06899999999987\n","1100: 40.63199999999986\n","1200: 11.139999999999736\n","1300: 20.71999999999977\n","1400: 28.211999999999822\n","1500: 45.624999999999865\n","1600: 66.48199999999993\n","1700: 82.436\n","1800: 78.92399999999999\n","1900: 86.91800000000003\n","Solved after 1948 episodes!\n","4 noise_hrl: {'both': {0.0: [89.47199999998358], 0.05: [89.99799999998434], 0.1: [89.48999999997977], 0.15: [88.51499999998151], 0.2: [85.50299999997637], 0.25: [80.88299999997103], 0.3: [72.49799999997136], 0.35: [44.72300000001539], 0.4: [32.02300000002053], 0.45: [34.25000000002293], 0.5: [4.272999999998563]}, 'action_only': {0.0: [89.36299999998495], 0.05: [91.00399999998527], 0.1: [91.2199999999859], 0.15: [86.71999999998543], 0.2: [87.2999999999849], 0.25: [89.01899999998602], 0.3: [86.68799999997887], 0.35: [84.97699999997701], 0.4: [82.17699999997629], 0.45: [78.52099999996982], 0.5: [75.96699999996494]}, 'goal_only': {0.0: [88.21199999998078], 0.05: [91.47699999998491], 0.1: [91.22299999998421], 0.15: [88.87399999998225], 0.2: [88.5309999999821], 0.25: [85.81699999997942], 0.3: [80.73599999996893], 0.35: [68.38999999997189], 0.4: [56.96499999997816], 0.45: [27.59400000002094], 0.5: [24.66800000002333]}, 'both_same': {0.0: [84.97199999997879], 0.05: [88.34199999998543], 0.1: [89.96199999998178], 0.15: [87.35699999998191], 0.2: [86.08499999997892], 0.25: [81.84999999997433], 0.3: [75.85199999996966], 0.35: [60.63599999997999], 0.4: [52.84299999999846], 0.45: [38.510000000021904], 0.5: [27.67100000001295]}}\n","100: -46.41800000000042\n","200: -25.60200000000037\n","300: 0.019999999999714076\n","400: 56.0319999999999\n","500: 72.46499999999996\n","600: 85.09399999999998\n","Solved after 627 episodes!\n","5 noise_hrl: {'both': {0.0: [89.47199999998358, 79.9469999999834], 0.05: [89.99799999998434, 88.99999999998478], 0.1: [89.48999999997977, 90.0769999999834], 0.15: [88.51499999998151, 83.97799999997598], 0.2: [85.50299999997637, 75.00899999997483], 0.25: [80.88299999997103, 46.466999999996], 0.3: [72.49799999997136, 41.517000000018754], 0.35: [44.72300000001539, 25.304000000017542], 0.4: [32.02300000002053, 12.194000000001315], 0.45: [34.25000000002293, 0.07399999999961678], 0.5: [4.272999999998563, 1.5359999999969525]}, 'action_only': {0.0: [89.36299999998495, 79.82599999998412], 0.05: [91.00399999998527, 79.93099999998056], 0.1: [91.2199999999859, 88.55599999998323], 0.15: [86.71999999998543, 81.3699999999784], 0.2: [87.2999999999849, 82.39499999997797], 0.25: [89.01899999998602, 78.58699999997523], 0.3: [86.68799999997887, 68.62399999997723], 0.35: [84.97699999997701, 65.20699999996592], 0.4: [82.17699999997629, 56.85599999998515], 0.45: [78.52099999996982, 51.597999999989014], 0.5: [75.96699999996494, 47.54500000000201]}, 'goal_only': {0.0: [88.21199999998078, 82.36299999999041], 0.05: [91.47699999998491, 84.96399999998285], 0.1: [91.22299999998421, 91.08299999998552], 0.15: [88.87399999998225, 86.97899999998303], 0.2: [88.5309999999821, 80.53399999997816], 0.25: [85.81699999997942, 65.10499999997859], 0.3: [80.73599999996893, 43.08200000001605], 0.35: [68.38999999997189, 37.367000000021406], 0.4: [56.96499999997816, 27.22800000001631], 0.45: [27.59400000002094, 12.362999999995239], 0.5: [24.66800000002333, 15.835000000015452]}, 'both_same': {0.0: [84.97199999997879, 78.79399999998478], 0.05: [88.34199999998543, 86.59899999998115], 0.1: [89.96199999998178, 89.60399999998208], 0.15: [87.35699999998191, 85.04099999998037], 0.2: [86.08499999997892, 77.36299999997819], 0.25: [81.84999999997433, 53.26099999998215], 0.3: [75.85199999996966, 26.14500000002168], 0.35: [60.63599999997999, 9.445000000009374], 0.4: [52.84299999999846, 11.476000000015233], 0.45: [38.510000000021904, -13.92699999999712], 0.5: [27.67100000001295, -6.872000000007319]}}\n","----\n","noise_hrl: {'both': {0.0: [89.47199999998358, 79.9469999999834], 0.05: [89.99799999998434, 88.99999999998478], 0.1: [89.48999999997977, 90.0769999999834], 0.15: [88.51499999998151, 83.97799999997598], 0.2: [85.50299999997637, 75.00899999997483], 0.25: [80.88299999997103, 46.466999999996], 0.3: [72.49799999997136, 41.517000000018754], 0.35: [44.72300000001539, 25.304000000017542], 0.4: [32.02300000002053, 12.194000000001315], 0.45: [34.25000000002293, 0.07399999999961678], 0.5: [4.272999999998563, 1.5359999999969525]}, 'action_only': {0.0: [89.36299999998495, 79.82599999998412], 0.05: [91.00399999998527, 79.93099999998056], 0.1: [91.2199999999859, 88.55599999998323], 0.15: [86.71999999998543, 81.3699999999784], 0.2: [87.2999999999849, 82.39499999997797], 0.25: [89.01899999998602, 78.58699999997523], 0.3: [86.68799999997887, 68.62399999997723], 0.35: [84.97699999997701, 65.20699999996592], 0.4: [82.17699999997629, 56.85599999998515], 0.45: [78.52099999996982, 51.597999999989014], 0.5: [75.96699999996494, 47.54500000000201]}, 'goal_only': {0.0: [88.21199999998078, 82.36299999999041], 0.05: [91.47699999998491, 84.96399999998285], 0.1: [91.22299999998421, 91.08299999998552], 0.15: [88.87399999998225, 86.97899999998303], 0.2: [88.5309999999821, 80.53399999997816], 0.25: [85.81699999997942, 65.10499999997859], 0.3: [80.73599999996893, 43.08200000001605], 0.35: [68.38999999997189, 37.367000000021406], 0.4: [56.96499999997816, 27.22800000001631], 0.45: [27.59400000002094, 12.362999999995239], 0.5: [24.66800000002333, 15.835000000015452]}, 'both_same': {0.0: [84.97199999997879, 78.79399999998478], 0.05: [88.34199999998543, 86.59899999998115], 0.1: [89.96199999998178, 89.60399999998208], 0.15: [87.35699999998191, 85.04099999998037], 0.2: [86.08499999997892, 77.36299999997819], 0.25: [81.84999999997433, 53.26099999998215], 0.3: [75.85199999996966, 26.14500000002168], 0.35: [60.63599999997999, 9.445000000009374], 0.4: [52.84299999999846, 11.476000000015233], 0.45: [38.510000000021904, -13.92699999999712], 0.5: [27.67100000001295, -6.872000000007319]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHXLsS8CxU_M"},"source":["Solved after 1167 episodes!\n","0 noise_hrl: {'both': {0.0: [90.64099999998433], 0.05: [90.66699999998426], 0.1: [87.51699999997884], 0.15: [70.93499999997381], 0.2: [-1.7470000000022432], 0.25: [-35.77599999996864], 0.3: [-40.18699999997145], 0.35: [-40.28599999996815], 0.4: [-33.3149999999731], 0.45: [-46.110999999994256], 0.5: [-40.49499999996995]}, 'action_only': {0.0: [90.6739999999843], 0.05: [90.11099999998319], 0.1: [88.86499999998155], 0.15: [85.93199999997613], 0.2: [83.64899999997593], 0.25: [74.51999999997177], 0.3: [55.223999999974446], 0.35: [11.25900000000002], 0.4: [17.241000000000987], 0.45: [-2.84300000000095], 0.5: [-9.400000000007289]}, 'goal_only': {0.0: [90.65399999998436], 0.05: [91.58499999998544], 0.1: [90.2759999999834], 0.15: [85.04699999997703], 0.2: [39.38000000001497], 0.25: [-25.931999999983773], 0.3: [-39.47899999997119], 0.35: [-47.408999999999885], 0.4: [-48.58500000000053], 0.45: [-50.00000000000659], 0.5: [-48.770000000002085]}, 'both_same': {0.0: [90.68899999998447], 0.05: [90.49099999998407], 0.1: [87.42199999997838], 0.15: [65.90499999997884], 0.2: [-3.4500000000024134], 0.25: [-27.00999999997485], 0.3: [-41.12499999996907], 0.35: [-36.24399999997003], 0.4: [-39.76999999996836], 0.45: [-39.797999999969754], 0.5: [-38.54199999996979]}}\n","3 noise_hrl: {'both': {0.0: [90.31299999998382, 84.26899999998261, 92.5379999999874], 0.05: [79.66999999998305, 91.77099999998708, 88.59199999998555], 0.1: [76.28299999998367, 91.4259999999862, 81.20299999998349], 0.15: [67.71799999997803, 90.02599999998262, 83.53599999997624], 0.2: [50.22399999999123, 84.85799999997847, 81.1859999999684], 0.25: [38.26300000002572, 78.0529999999699, 77.63599999997042], 0.3: [22.14500000000784, 55.269999999991406, 71.30999999997756], 0.35: [0.9389999999967924, 37.75500000002468, 58.87399999999032], 0.4: [-7.284000000007797, 23.987000000010347, 39.489000000017626], 0.45: [-7.1510000000063725, 6.734999999992665, 36.32300000002443], 0.5: [-8.34100000000543, 2.899999999998579, 30.337000000016143]}, 'action_only': {0.0: [90.3429999999838, 79.08399999997904, 92.58699999998734], 0.05: [86.31299999998426, 90.68699999998813, 92.21999999998974], 0.1: [68.83499999998848, 92.2209999999898, 81.61999999998464], 0.15: [62.84699999998277, 86.96899999998988, 83.73499999998381], 0.2: [60.029999999984895, 90.74599999998522, 68.1739999999693], 0.25: [65.35599999998045, 86.17199999998147, 57.78999999999767], 0.3: [45.87699999998632, 84.36599999998045, 61.719999999973], 0.35: [32.67200000001711, 84.3789999999793, 54.45899999998722], 0.4: [19.14600000001562, 78.56899999997155, 40.863000000019305], 0.45: [8.406999999997097, 64.9449999999928, 29.249000000019564], 0.5: [3.876999999998259, 65.29499999997651, 35.77000000002095]}, 'goal_only': {0.0: [90.32199999998375, 85.12399999998362, 92.52499999998697], 0.05: [86.20199999997989, 90.92199999998591, 93.5199999999886], 0.1: [80.79299999998554, 90.36099999998446, 90.4559999999833], 0.15: [76.05099999997884, 91.32499999998477, 89.17199999998417], 0.2: [69.61499999998202, 90.10399999998279, 87.06199999997935], 0.25: [64.76899999998236, 83.8499999999736, 81.19199999997167], 0.3: [56.16599999998603, 77.81799999997237, 76.94399999997172], 0.35: [41.283000000020394, 61.72599999998293, 71.60299999997274], 0.4: [30.192000000025374, 29.10800000001745, 68.47899999996893], 0.45: [30.80300000002218, 44.097000000007185, 68.4059999999684], 0.5: [6.719999999992557, 40.56600000002039, 66.67499999996764]}, 'both_same': {0.0: [90.29599999998378, 86.18399999998448, 92.49799999998748], 0.05: [75.957999999977, 92.4129999999891, 89.24599999998406], 0.1: [78.70299999997717, 89.81799999998411, 82.18499999998498], 0.15: [56.156000000001875, 89.3819999999829, 85.10699999997978], 0.2: [37.82300000002312, 82.65099999997523, 76.9829999999727], 0.25: [43.73800000001884, 75.36199999996349, 57.816999999989406], 0.3: [27.664000000019023, 60.72199999998089, 34.42800000002232], 0.35: [-1.6419999999990724, 15.111000000004182, 29.405000000022305], 0.4: [-1.6469999999994278, 19.469000000012173, 21.47300000000858], 0.45: [-6.254000000004339, 3.039999999997252, 0.11799999999989823], 0.5: [-1.544999999999279, -17.30399999998808, 1.9489999999993604]}}\n","5 noise_hrl: {'both': {0.0: [89.47199999998358, 79.9469999999834], 0.05: [89.99799999998434, 88.99999999998478], 0.1: [89.48999999997977, 90.0769999999834], 0.15: [88.51499999998151, 83.97799999997598], 0.2: [85.50299999997637, 75.00899999997483], 0.25: [80.88299999997103, 46.466999999996], 0.3: [72.49799999997136, 41.517000000018754], 0.35: [44.72300000001539, 25.304000000017542], 0.4: [32.02300000002053, 12.194000000001315], 0.45: [34.25000000002293, 0.07399999999961678], 0.5: [4.272999999998563, 1.5359999999969525]}, 'action_only': {0.0: [89.36299999998495, 79.82599999998412], 0.05: [91.00399999998527, 79.93099999998056], 0.1: [91.2199999999859, 88.55599999998323], 0.15: [86.71999999998543, 81.3699999999784], 0.2: [87.2999999999849, 82.39499999997797], 0.25: [89.01899999998602, 78.58699999997523], 0.3: [86.68799999997887, 68.62399999997723], 0.35: [84.97699999997701, 65.20699999996592], 0.4: [82.17699999997629, 56.85599999998515], 0.45: [78.52099999996982, 51.597999999989014], 0.5: [75.96699999996494, 47.54500000000201]}, 'goal_only': {0.0: [88.21199999998078, 82.36299999999041], 0.05: [91.47699999998491, 84.96399999998285], 0.1: [91.22299999998421, 91.08299999998552], 0.15: [88.87399999998225, 86.97899999998303], 0.2: [88.5309999999821, 80.53399999997816], 0.25: [85.81699999997942, 65.10499999997859], 0.3: [80.73599999996893, 43.08200000001605], 0.35: [68.38999999997189, 37.367000000021406], 0.4: [56.96499999997816, 27.22800000001631], 0.45: [27.59400000002094, 12.362999999995239], 0.5: [24.66800000002333, 15.835000000015452]}, 'both_same': {0.0: [84.97199999997879, 78.79399999998478], 0.05: [88.34199999998543, 86.59899999998115], 0.1: [89.96199999998178, 89.60399999998208], 0.15: [87.35699999998191, 85.04099999998037], 0.2: [86.08499999997892, 77.36299999997819], 0.25: [81.84999999997433, 53.26099999998215], 0.3: [75.85199999996966, 26.14500000002168], 0.35: [60.63599999997999, 9.445000000009374], 0.4: [52.84299999999846, 11.476000000015233], 0.45: [38.510000000021904, -13.92699999999712], 0.5: [27.67100000001295, -6.872000000007319]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhC6f7N6sJoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617448797890,"user_tz":-60,"elapsed":12085729,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"d4aa6e29-c5e3-4e54-a83d-4b489dc31217"},"source":["targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False)   \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['action'], True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437], 0.02: [85.4499999999783], 0.04: [70.50499999996984], 0.06: [46.47200000001573], 0.08: [-24.951999999977698], 0.1: [-48.83200000000143], 0.12: [-48.78100000000215], 0.14: [-47.58799999999713], 0.16: [-48.591000000005096], 0.18: [-46.60999999999152], 0.2: [-50.00000000000659]}}\n","0 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444], 0.02: [88.90799999998143], 0.04: [75.40099999998027], 0.06: [35.42600000001603], 0.08: [-30.61999999997724], 0.1: [-34.94699999996917], 0.12: [-38.82099999997564], 0.14: [-38.86799999997291], 0.16: [-48.690000000001945], 0.18: [-47.64899999999824], 0.2: [-50.00000000000659]}}\n","1 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226], 0.02: [85.4499999999783, 60.87899999997735], 0.04: [70.50499999996984, 59.27099999997835], 0.06: [46.47200000001573, 25.13300000001487], 0.08: [-24.951999999977698, -16.392999999972055], 0.1: [-48.83200000000143, -30.876999999971677], 0.12: [-48.78100000000215, -42.59699999997961], 0.14: [-47.58799999999713, -46.40399999999987], 0.16: [-48.591000000005096, -45.35099999998909], 0.18: [-46.60999999999152, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659]}}\n","1 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384], 0.02: [88.90799999998143, 78.75699999997416], 0.04: [75.40099999998027, 75.43699999997884], 0.06: [35.42600000001603, 56.24299999999325], 0.08: [-30.61999999997724, 36.0930000000225], 0.1: [-34.94699999996917, -6.987000000003979], 0.12: [-38.82099999997564, -23.317999999978372], 0.14: [-38.86799999997291, -22.22099999997845], 0.16: [-48.690000000001945, -36.219999999970625], 0.18: [-47.64899999999824, -37.257999999970586], 0.2: [-50.00000000000659, -40.14499999996717]}}\n","2 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226, 84.99299999998092], 0.02: [85.4499999999783, 60.87899999997735, 90.66599999998404], 0.04: [70.50499999996984, 59.27099999997835, 75.73299999998491], 0.06: [46.47200000001573, 25.13300000001487, 38.246000000015734], 0.08: [-24.951999999977698, -16.392999999972055, 1.355999999994889], 0.1: [-48.83200000000143, -30.876999999971677, -40.54399999996874], 0.12: [-48.78100000000215, -42.59699999997961, -46.328999999996185], 0.14: [-47.58799999999713, -46.40399999999987, -47.58300000000052], 0.16: [-48.591000000005096, -45.35099999998909, -45.01999999998657], 0.18: [-46.60999999999152, -50.00000000000659, -48.89100000000164], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659]}}\n","2 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384, 82.92599999998441], 0.02: [88.90799999998143, 78.75699999997416, 45.73199999998726], 0.04: [75.40099999998027, 75.43699999997884, 83.86799999998013], 0.06: [35.42600000001603, 56.24299999999325, 75.53899999997363], 0.08: [-30.61999999997724, 36.0930000000225, 36.644000000015765], 0.1: [-34.94699999996917, -6.987000000003979, 25.858000000018578], 0.12: [-38.82099999997564, -23.317999999978372, -9.862000000002359], 0.14: [-38.86799999997291, -22.22099999997845, -33.44799999996871], 0.16: [-48.690000000001945, -36.219999999970625, -44.97699999998922], 0.18: [-47.64899999999824, -37.257999999970586, -45.55999999998679], 0.2: [-50.00000000000659, -40.14499999996717, -48.58000000000506]}}\n","3 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226, 84.99299999998092, 92.56199999998728], 0.02: [85.4499999999783, 60.87899999997735, 90.66599999998404, 91.9529999999865], 0.04: [70.50499999996984, 59.27099999997835, 75.73299999998491, 74.62599999998432], 0.06: [46.47200000001573, 25.13300000001487, 38.246000000015734, 38.02800000002135], 0.08: [-24.951999999977698, -16.392999999972055, 1.355999999994889, 5.05499999999259], 0.1: [-48.83200000000143, -30.876999999971677, -40.54399999996874, -6.950000000003388], 0.12: [-48.78100000000215, -42.59699999997961, -46.328999999996185, -30.41399999997996], 0.14: [-47.58799999999713, -46.40399999999987, -47.58300000000052, -36.378999999973544], 0.16: [-48.591000000005096, -45.35099999998909, -45.01999999998657, -37.924999999968215], 0.18: [-46.60999999999152, -50.00000000000659, -48.89100000000164, -40.67899999996993], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.583999999971034]}}\n","3 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384, 82.92599999998441, 92.48699999998689], 0.02: [88.90799999998143, 78.75699999997416, 45.73199999998726, 89.38999999998677], 0.04: [75.40099999998027, 75.43699999997884, 83.86799999998013, 79.0009999999895], 0.06: [35.42600000001603, 56.24299999999325, 75.53899999997363, 54.83299999997011], 0.08: [-30.61999999997724, 36.0930000000225, 36.644000000015765, 15.849000000002741], 0.1: [-34.94699999996917, -6.987000000003979, 25.858000000018578, -16.401999999987076], 0.12: [-38.82099999997564, -23.317999999978372, -9.862000000002359, -35.249999999974136], 0.14: [-38.86799999997291, -22.22099999997845, -33.44799999996871, -27.27399999998037], 0.16: [-48.690000000001945, -36.219999999970625, -44.97699999998922, -23.782999999987492], 0.18: [-47.64899999999824, -37.257999999970586, -45.55999999998679, -44.414999999988886], 0.2: [-50.00000000000659, -40.14499999996717, -48.58000000000506, -38.41699999997048]}}\n","4 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226, 84.99299999998092, 92.56199999998728, 90.27299999998405], 0.02: [85.4499999999783, 60.87899999997735, 90.66599999998404, 91.9529999999865, 79.49199999997832], 0.04: [70.50499999996984, 59.27099999997835, 75.73299999998491, 74.62599999998432, 59.10499999997716], 0.06: [46.47200000001573, 25.13300000001487, 38.246000000015734, 38.02800000002135, 31.709000000017635], 0.08: [-24.951999999977698, -16.392999999972055, 1.355999999994889, 5.05499999999259, 18.91100000000844], 0.1: [-48.83200000000143, -30.876999999971677, -40.54399999996874, -6.950000000003388, -22.281999999977728], 0.12: [-48.78100000000215, -42.59699999997961, -46.328999999996185, -30.41399999997996, -37.54899999996823], 0.14: [-47.58799999999713, -46.40399999999987, -47.58300000000052, -36.378999999973544, -42.95699999997892], 0.16: [-48.591000000005096, -45.35099999998909, -45.01999999998657, -37.924999999968215, -47.78600000000126], 0.18: [-46.60999999999152, -50.00000000000659, -48.89100000000164, -40.67899999996993, -45.739999999988974], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.583999999971034, -47.63600000000526]}}\n","4 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384, 82.92599999998441, 92.48699999998689, 89.70999999998598], 0.02: [88.90799999998143, 78.75699999997416, 45.73199999998726, 89.38999999998677, 87.39899999998202], 0.04: [75.40099999998027, 75.43699999997884, 83.86799999998013, 79.0009999999895, 81.82999999998418], 0.06: [35.42600000001603, 56.24299999999325, 75.53899999997363, 54.83299999997011, 63.75799999998981], 0.08: [-30.61999999997724, 36.0930000000225, 36.644000000015765, 15.849000000002741, 57.11199999998296], 0.1: [-34.94699999996917, -6.987000000003979, 25.858000000018578, -16.401999999987076, 37.362000000017716], 0.12: [-38.82099999997564, -23.317999999978372, -9.862000000002359, -35.249999999974136, 11.389999999998201], 0.14: [-38.86799999997291, -22.22099999997845, -33.44799999996871, -27.27399999998037, 5.846999999996231], 0.16: [-48.690000000001945, -36.219999999970625, -44.97699999998922, -23.782999999987492, -6.195000000006115], 0.18: [-47.64899999999824, -37.257999999970586, -45.55999999998679, -44.414999999988886, -5.712000000002215], 0.2: [-50.00000000000659, -40.14499999996717, -48.58000000000506, -38.41699999997048, -11.054000000001288]}}\n","5 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226, 84.99299999998092, 92.56199999998728, 90.27299999998405, 87.110999999987], 0.02: [85.4499999999783, 60.87899999997735, 90.66599999998404, 91.9529999999865, 79.49199999997832, 22.603000000015072], 0.04: [70.50499999996984, 59.27099999997835, 75.73299999998491, 74.62599999998432, 59.10499999997716, -47.240999999999275], 0.06: [46.47200000001573, 25.13300000001487, 38.246000000015734, 38.02800000002135, 31.709000000017635, -50.00000000000659], 0.08: [-24.951999999977698, -16.392999999972055, 1.355999999994889, 5.05499999999259, 18.91100000000844, -50.00000000000659], 0.1: [-48.83200000000143, -30.876999999971677, -40.54399999996874, -6.950000000003388, -22.281999999977728, -50.00000000000659], 0.12: [-48.78100000000215, -42.59699999997961, -46.328999999996185, -30.41399999997996, -37.54899999996823, -48.707000000000974], 0.14: [-47.58799999999713, -46.40399999999987, -47.58300000000052, -36.378999999973544, -42.95699999997892, -44.9299999999901], 0.16: [-48.591000000005096, -45.35099999998909, -45.01999999998657, -37.924999999968215, -47.78600000000126, -46.30799999999497], 0.18: [-46.60999999999152, -50.00000000000659, -48.89100000000164, -40.67899999996993, -45.739999999988974, -37.93199999996736], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.583999999971034, -47.63600000000526, -42.91399999997648]}}\n","5 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384, 82.92599999998441, 92.48699999998689, 89.70999999998598, 88.34899999998873], 0.02: [88.90799999998143, 78.75699999997416, 45.73199999998726, 89.38999999998677, 87.39899999998202, 49.178000000011664], 0.04: [75.40099999998027, 75.43699999997884, 83.86799999998013, 79.0009999999895, 81.82999999998418, 6.847999999995681], 0.06: [35.42600000001603, 56.24299999999325, 75.53899999997363, 54.83299999997011, 63.75799999998981, -35.79599999997485], 0.08: [-30.61999999997724, 36.0930000000225, 36.644000000015765, 15.849000000002741, 57.11199999998296, -38.90799999997032], 0.1: [-34.94699999996917, -6.987000000003979, 25.858000000018578, -16.401999999987076, 37.362000000017716, -44.39099999998254], 0.12: [-38.82099999997564, -23.317999999978372, -9.862000000002359, -35.249999999974136, 11.389999999998201, -41.89199999997472], 0.14: [-38.86799999997291, -22.22099999997845, -33.44799999996871, -27.27399999998037, 5.846999999996231, -42.413999999976696], 0.16: [-48.690000000001945, -36.219999999970625, -44.97699999998922, -23.782999999987492, -6.195000000006115, -48.65400000000078], 0.18: [-47.64899999999824, -37.257999999970586, -45.55999999998679, -44.414999999988886, -5.712000000002215, -48.60300000000514], 0.2: [-50.00000000000659, -40.14499999996717, -48.58000000000506, -38.41699999997048, -11.054000000001288, -48.92000000000175]}}\n","----\n","fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.64899999998437, 88.88199999998226, 84.99299999998092, 92.56199999998728, 90.27299999998405, 87.110999999987], 0.02: [85.4499999999783, 60.87899999997735, 90.66599999998404, 91.9529999999865, 79.49199999997832, 22.603000000015072], 0.04: [70.50499999996984, 59.27099999997835, 75.73299999998491, 74.62599999998432, 59.10499999997716, -47.240999999999275], 0.06: [46.47200000001573, 25.13300000001487, 38.246000000015734, 38.02800000002135, 31.709000000017635, -50.00000000000659], 0.08: [-24.951999999977698, -16.392999999972055, 1.355999999994889, 5.05499999999259, 18.91100000000844, -50.00000000000659], 0.1: [-48.83200000000143, -30.876999999971677, -40.54399999996874, -6.950000000003388, -22.281999999977728, -50.00000000000659], 0.12: [-48.78100000000215, -42.59699999997961, -46.328999999996185, -30.41399999997996, -37.54899999996823, -48.707000000000974], 0.14: [-47.58799999999713, -46.40399999999987, -47.58300000000052, -36.378999999973544, -42.95699999997892, -44.9299999999901], 0.16: [-48.591000000005096, -45.35099999998909, -45.01999999998657, -37.924999999968215, -47.78600000000126, -46.30799999999497], 0.18: [-46.60999999999152, -50.00000000000659, -48.89100000000164, -40.67899999996993, -45.739999999988974, -37.93199999996736], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.583999999971034, -47.63600000000526, -42.91399999997648]}}\n","fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [90.67899999998444, 90.27699999998384, 82.92599999998441, 92.48699999998689, 89.70999999998598, 88.34899999998873], 0.02: [88.90799999998143, 78.75699999997416, 45.73199999998726, 89.38999999998677, 87.39899999998202, 49.178000000011664], 0.04: [75.40099999998027, 75.43699999997884, 83.86799999998013, 79.0009999999895, 81.82999999998418, 6.847999999995681], 0.06: [35.42600000001603, 56.24299999999325, 75.53899999997363, 54.83299999997011, 63.75799999998981, -35.79599999997485], 0.08: [-30.61999999997724, 36.0930000000225, 36.644000000015765, 15.849000000002741, 57.11199999998296, -38.90799999997032], 0.1: [-34.94699999996917, -6.987000000003979, 25.858000000018578, -16.401999999987076, 37.362000000017716, -44.39099999998254], 0.12: [-38.82099999997564, -23.317999999978372, -9.862000000002359, -35.249999999974136, 11.389999999998201, -41.89199999997472], 0.14: [-38.86799999997291, -22.22099999997845, -33.44799999996871, -27.27399999998037, 5.846999999996231, -42.413999999976696], 0.16: [-48.690000000001945, -36.219999999970625, -44.97699999998922, -23.782999999987492, -6.195000000006115, -48.65400000000078], 0.18: [-47.64899999999824, -37.257999999970586, -45.55999999998679, -44.414999999988886, -5.712000000002215, -48.60300000000514], 0.2: [-50.00000000000659, -40.14499999996717, -48.58000000000506, -38.41699999997048, -11.054000000001288, -48.92000000000175]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8pnki1eCngGQ"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointMazeEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmUPiWlPqUts","executionInfo":{"status":"error","timestamp":1616836715323,"user_tz":0,"elapsed":1451257,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"94853fd4-eade-4585-f972-e1465375c187"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [17.492000000005742], 1.5: [46.96500000001087], 2.0: [30.977000000017107], 2.5: [91.31999999998509], 3.0: [91.01599999998496], 3.5: [88.97199999998284], 4.0: [90.68099999998445], 4.5: [90.65899999998432], 5.0: [90.43599999998399], 5.5: [90.06199999998341], 6.0: [90.28699999998376], 6.5: [89.90099999998321], 7.0: [86.64899999997847]}\n","1 scale: {1.0: [17.492000000005742, -9.519000000002526], 1.5: [46.96500000001087, -26.472999999980807], 2.0: [30.977000000017107, -20.739999999974508], 2.5: [91.31999999998509, 12.028000000005207], 3.0: [91.01599999998496, 83.66899999998317], 3.5: [88.97199999998284, 90.83199999998473], 4.0: [90.68099999998445, 90.32299999998378], 4.5: [90.65899999998432, 90.2809999999837], 5.0: [90.43599999998399, 90.22799999998364], 5.5: [90.06199999998341, 89.86499999998297], 6.0: [90.28699999998376, 90.0669999999834], 6.5: [89.90099999998321, 89.94799999998318], 7.0: [86.64899999997847, 89.92799999998319]}\n","2 scale: {1.0: [17.492000000005742, -9.519000000002526, -33.63899999997172], 1.5: [46.96500000001087, -26.472999999980807, 40.067000000011724], 2.0: [30.977000000017107, -20.739999999974508, 66.97199999997396], 2.5: [91.31999999998509, 12.028000000005207, 86.46399999998346], 3.0: [91.01599999998496, 83.66899999998317, 91.93199999999032], 3.5: [88.97199999998284, 90.83199999998473, 81.96399999997885], 4.0: [90.68099999998445, 90.32299999998378, 81.52399999998025], 4.5: [90.65899999998432, 90.2809999999837, 87.6769999999853], 5.0: [90.43599999998399, 90.22799999998364, 84.64499999998615], 5.5: [90.06199999998341, 89.86499999998297, 92.11099999998649], 6.0: [90.28699999998376, 90.0669999999834, 89.10499999998689], 6.5: [89.90099999998321, 89.94799999998318, 82.94499999998939], 7.0: [86.64899999997847, 89.92799999998319, 70.51399999998476]}\n","3 scale: {1.0: [17.492000000005742, -9.519000000002526, -33.63899999997172, -28.78999999997885], 1.5: [46.96500000001087, -26.472999999980807, 40.067000000011724, -6.417000000006102], 2.0: [30.977000000017107, -20.739999999974508, 66.97199999997396, 46.476999999980805], 2.5: [91.31999999998509, 12.028000000005207, 86.46399999998346, 80.6229999999848], 3.0: [91.01599999998496, 83.66899999998317, 91.93199999999032, 90.10299999998234], 3.5: [88.97199999998284, 90.83199999998473, 81.96399999997885, 93.62699999998883], 4.0: [90.68099999998445, 90.32299999998378, 81.52399999998025, 92.54699999998704], 4.5: [90.65899999998432, 90.2809999999837, 87.6769999999853, 91.99899999998615], 5.0: [90.43599999998399, 90.22799999998364, 84.64499999998615, 91.51699999998546], 5.5: [90.06199999998341, 89.86499999998297, 92.11099999998649, 90.8239999999846], 6.0: [90.28699999998376, 90.0669999999834, 89.10499999998689, 90.98199999998485], 6.5: [89.90099999998321, 89.94799999998318, 82.94499999998939, 90.15799999998349], 7.0: [86.64899999997847, 89.92799999998319, 70.51399999998476, 90.42599999998389]}\n","4 scale: {1.0: [17.492000000005742, -9.519000000002526, -33.63899999997172, -28.78999999997885, -18.81999999997709], 1.5: [46.96500000001087, -26.472999999980807, 40.067000000011724, -6.417000000006102, 45.0419999999943], 2.0: [30.977000000017107, -20.739999999974508, 66.97199999997396, 46.476999999980805, 90.39699999998602], 2.5: [91.31999999998509, 12.028000000005207, 86.46399999998346, 80.6229999999848, 91.22099999998323], 3.0: [91.01599999998496, 83.66899999998317, 91.93199999999032, 90.10299999998234, 79.15199999998438], 3.5: [88.97199999998284, 90.83199999998473, 81.96399999997885, 93.62699999998883, 79.42499999997973], 4.0: [90.68099999998445, 90.32299999998378, 81.52399999998025, 92.54699999998704, 87.09799999998313], 4.5: [90.65899999998432, 90.2809999999837, 87.6769999999853, 91.99899999998615, 90.87099999998406], 5.0: [90.43599999998399, 90.22799999998364, 84.64499999998615, 91.51699999998546, 88.32599999998476], 5.5: [90.06199999998341, 89.86499999998297, 92.11099999998649, 90.8239999999846, 89.24999999998482], 6.0: [90.28699999998376, 90.0669999999834, 89.10499999998689, 90.98199999998485, 89.07999999998225], 6.5: [89.90099999998321, 89.94799999998318, 82.94499999998939, 90.15799999998349, 82.06099999998176], 7.0: [86.64899999997847, 89.92799999998319, 70.51399999998476, 90.42599999998389, 83.82599999997989]}\n","5 scale: {1.0: [17.492000000005742, -9.519000000002526, -33.63899999997172, -28.78999999997885, -18.81999999997709, 9.741999999999079], 1.5: [46.96500000001087, -26.472999999980807, 40.067000000011724, -6.417000000006102, 45.0419999999943, 0.7250000000012844], 2.0: [30.977000000017107, -20.739999999974508, 66.97199999997396, 46.476999999980805, 90.39699999998602, 16.957000000011433], 2.5: [91.31999999998509, 12.028000000005207, 86.46399999998346, 80.6229999999848, 91.22099999998323, 42.96000000000702], 3.0: [91.01599999998496, 83.66899999998317, 91.93199999999032, 90.10299999998234, 79.15199999998438, 33.45200000002168], 3.5: [88.97199999998284, 90.83199999998473, 81.96399999997885, 93.62699999998883, 79.42499999997973, 70.32299999999192], 4.0: [90.68099999998445, 90.32299999998378, 81.52399999998025, 92.54699999998704, 87.09799999998313, 83.69399999998834], 4.5: [90.65899999998432, 90.2809999999837, 87.6769999999853, 91.99899999998615, 90.87099999998406, 89.44399999998623], 5.0: [90.43599999998399, 90.22799999998364, 84.64499999998615, 91.51699999998546, 88.32599999998476, 92.84599999998778], 5.5: [90.06199999998341, 89.86499999998297, 92.11099999998649, 90.8239999999846, 89.24999999998482, 90.13899999998313], 6.0: [90.28699999998376, 90.0669999999834, 89.10499999998689, 90.98199999998485, 89.07999999998225, 86.60599999998625], 6.5: [89.90099999998321, 89.94799999998318, 82.94499999998939, 90.15799999998349, 82.06099999998176, 88.45899999998292], 7.0: [86.64899999997847, 89.92799999998319, 70.51399999998476, 90.42599999998389, 83.82599999997989, 65.3949999999889]}\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-93d106618bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#agent = train_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHIRO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"hiro_{i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-50856a265816>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, name, dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"point_maze_pheromone\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/drive/My Drive/Dissertation/saved_models/{dir}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Dissertation/saved_models/point_maze_pheromone/hiro_6'"]}]},{"cell_type":"code","metadata":{"id":"Gd2_86AIqOt4"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJjpZcCLqjua","executionInfo":{"status":"ok","timestamp":1617008288158,"user_tz":-60,"elapsed":698505,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"c7674593-d3ed-4717-b73b-9dfa3ca6a30f"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","env = NormalizedEnv(PointMazeEnv(4))\n","i = 0\n","while i < 6:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [90.67799999998434], 0.05: [90.67599999998441], 0.1: [90.67499999998444], 0.15: [90.67399999998433], 0.2: [90.71299999998436], 0.25: [89.15399999998228], 0.3: [86.4569999999822], 0.35: [83.60699999997922], 0.4: [80.80599999998105]}\n","1 range: {0.0: [90.67799999998434, 90.31399999998382], 0.05: [90.67599999998441, 88.9599999999841], 0.1: [90.67499999998444, 90.37799999998391], 0.15: [90.67399999998433, 90.37999999998382], 0.2: [90.71299999998436, 89.03699999998257], 0.25: [89.15399999998228, 89.02699999998244], 0.3: [86.4569999999822, 87.66199999998115], 0.35: [83.60699999997922, 84.82699999998641], 0.4: [80.80599999998105, 86.25099999998181]}\n","2 range: {0.0: [90.67799999998434, 90.31399999998382, 80.91699999997967], 0.05: [90.67599999998441, 88.9599999999841, 81.89299999998245], 0.1: [90.67499999998444, 90.37799999998391, 79.02499999999027], 0.15: [90.67399999998433, 90.37999999998382, 72.80899999998003], 0.2: [90.71299999998436, 89.03699999998257, 66.82199999997285], 0.25: [89.15399999998228, 89.02699999998244, 61.382999999985195], 0.3: [86.4569999999822, 87.66199999998115, 51.52299999998286], 0.35: [83.60699999997922, 84.82699999998641, 45.32500000000063], 0.4: [80.80599999998105, 86.25099999998181, 45.63100000000817]}\n","3 range: {0.0: [90.67799999998434, 90.31399999998382, 80.91699999997967, 92.57099999998724], 0.05: [90.67599999998441, 88.9599999999841, 81.89299999998245, 92.64599999998731], 0.1: [90.67499999998444, 90.37799999998391, 79.02499999999027, 92.42699999998715], 0.15: [90.67399999998433, 90.37999999998382, 72.80899999998003, 92.42899999998679], 0.2: [90.71299999998436, 89.03699999998257, 66.82199999997285, 92.40699999998715], 0.25: [89.15399999998228, 89.02699999998244, 61.382999999985195, 90.90199999998543], 0.3: [86.4569999999822, 87.66199999998115, 51.52299999998286, 88.22999999998417], 0.35: [83.60699999997922, 84.82699999998641, 45.32500000000063, 89.14699999998875], 0.4: [80.80599999998105, 86.25099999998181, 45.63100000000817, 92.28199999998678]}\n","4 range: {0.0: [90.67799999998434, 90.31399999998382, 80.91699999997967, 92.57099999998724, 90.82299999998565], 0.05: [90.67599999998441, 88.9599999999841, 81.89299999998245, 92.64599999998731, 88.9599999999858], 0.1: [90.67499999998444, 90.37799999998391, 79.02499999999027, 92.42699999998715, 88.51299999998174], 0.15: [90.67399999998433, 90.37999999998382, 72.80899999998003, 92.42899999998679, 90.54399999998245], 0.2: [90.71299999998436, 89.03699999998257, 66.82199999997285, 92.40699999998715, 87.82499999998527], 0.25: [89.15399999998228, 89.02699999998244, 61.382999999985195, 90.90199999998543, 89.83099999998605], 0.3: [86.4569999999822, 87.66199999998115, 51.52299999998286, 88.22999999998417, 84.41299999998365], 0.35: [83.60699999997922, 84.82699999998641, 45.32500000000063, 89.14699999998875, 85.33399999998292], 0.4: [80.80599999998105, 86.25099999998181, 45.63100000000817, 92.28199999998678, 69.84999999997837]}\n","5 range: {0.0: [90.67799999998434, 90.31399999998382, 80.91699999997967, 92.57099999998724, 90.82299999998565, 81.66099999998632], 0.05: [90.67599999998441, 88.9599999999841, 81.89299999998245, 92.64599999998731, 88.9599999999858, 83.20899999998454], 0.1: [90.67499999998444, 90.37799999998391, 79.02499999999027, 92.42699999998715, 88.51299999998174, 85.39899999998501], 0.15: [90.67399999998433, 90.37999999998382, 72.80899999998003, 92.42899999998679, 90.54399999998245, 80.14399999998862], 0.2: [90.71299999998436, 89.03699999998257, 66.82199999997285, 92.40699999998715, 87.82499999998527, 77.0689999999807], 0.25: [89.15399999998228, 89.02699999998244, 61.382999999985195, 90.90199999998543, 89.83099999998605, 90.14099999998898], 0.3: [86.4569999999822, 87.66199999998115, 51.52299999998286, 88.22999999998417, 84.41299999998365, 82.4759999999892], 0.35: [83.60699999997922, 84.82699999998641, 45.32500000000063, 89.14699999998875, 85.33399999998292, 89.04499999998202], 0.4: [80.80599999998105, 86.25099999998181, 45.63100000000817, 92.28199999998678, 69.84999999997837, 81.91399999997805]}\n","----\n","range: {0.0: [90.67799999998434, 90.31399999998382, 80.91699999997967, 92.57099999998724, 90.82299999998565, 81.66099999998632], 0.05: [90.67599999998441, 88.9599999999841, 81.89299999998245, 92.64599999998731, 88.9599999999858, 83.20899999998454], 0.1: [90.67499999998444, 90.37799999998391, 79.02499999999027, 92.42699999998715, 88.51299999998174, 85.39899999998501], 0.15: [90.67399999998433, 90.37999999998382, 72.80899999998003, 92.42899999998679, 90.54399999998245, 80.14399999998862], 0.2: [90.71299999998436, 89.03699999998257, 66.82199999997285, 92.40699999998715, 87.82499999998527, 77.0689999999807], 0.25: [89.15399999998228, 89.02699999998244, 61.382999999985195, 90.90199999998543, 89.83099999998605, 90.14099999998898], 0.3: [86.4569999999822, 87.66199999998115, 51.52299999998286, 88.22999999998417, 84.41299999998365, 82.4759999999892], 0.35: [83.60699999997922, 84.82699999998641, 45.32500000000063, 89.14699999998875, 85.33399999998292, 89.04499999998202], 0.4: [80.80599999998105, 86.25099999998181, 45.63100000000817, 92.28199999998678, 69.84999999997837, 81.91399999997805]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BhvsIWF-qrHj"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)\n","\n","def save_random_manager(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_points = 10000\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","    for _ in range(num_points):\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        noise = torch.FloatTensor(state.shape).uniform_(0.0, 1.0).to(device)\n","\n","        g_state = state_min + state_range * noise\n","        g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","\n","        goal = agent.select_goal(g_state, False, False)\n","        path[\"manager\"].append((0, g_state.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","    episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWeLBDKTP3Ao","executionInfo":{"status":"ok","timestamp":1616493270550,"user_tz":0,"elapsed":38321,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"0107b42c-f0dc-4328-af79-819acc958d8a"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        #save_trajectories(agent, episodes, False)\n","        save_random_manager(agent, episodes)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","torch.save(episodes, \"PointMaze_Freeze_manager.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y6VhJjD8QHJl"},"source":["def get_intrinsic_reward(agent):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    overall_reward = 0\n","    intr_rews = []\n","\n","    for i_episode in range(num_episodes):\n","        cur_intr = []\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        episode_steps = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                cur_intr.append(agent.intrinsic_reward(reward, state, goal, next_state).detach().item())\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","        intr_rews.append(cur_intr)\n","    print(overall_reward / num_episodes)\n","    return intr_rews"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"SBjnn7HWZTyF","executionInfo":{"status":"ok","timestamp":1616492202635,"user_tz":0,"elapsed":868,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"8fdc8c67-b902-442f-b4f1-c33f9b6077ad"},"source":["import matplotlib.pyplot as plt\n","\n","episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 6\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_freeze_{i}\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Freeze\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[3])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["97.31999999999961\n","Freeze\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5hV5bn+8e9D731A6SBFkc5mwBqNiERNMCbGhtIUOcYT9STRGGM0lmg00RhNUKSKIMYWTCQqWBOlzdB7kzJDG3obmPacP2ab3/w8e2Rg75m1y/25rrlmr8J+n8Vmbta8613vMndHRESSX6WgCxARkYqhwBcRSREKfBGRFKHAFxFJEQp8EZEUUSXoAr5JkyZNvG3btkGXISKSMDIzM3e7e1qkbXEd+G3btiUjIyPoMkREEoaZbS5tm7p0RERShAJfRCRFKPBFRFKEAl9EJEUo8EVEUoQCX0QkRSjwRURShAJfRCSOZG7ex9jPNpTLeyvwRUTixL/X7WbIuHlMm7eFw8cLYv7+CnwRkTjw3vIdjJi0gDaNa/HX0edQp3rsJ0KI66kVRERSwZuZWdzz5lK6t6zPxGF9aVCrWrm0o8AXEQnQpM+/5KG/r+S8Do0Ze1OI2uVwZv8VBb6ISADcnec+Ws/Ts9YysEsz/nR9L2pUrVyubSrwRUQqmLvz2LurGPfvL7m6dwue/EF3qlQu/0uqCnwRkQpUWOT88q1lvJaxlWHntuXXV3ahUiWrkLYV+CIiFeR4QSF3v7aYmct28JNvd+DuSzthVjFhDwp8EZEKcTSvgNGvLOSztTn86oqzuOWC9hVegwJfRKScHcjNZ+SkBSzcso/f/aAb1/ZtHUgdCnwRkXJyLL+QORv28NT7a1i36xDPXd+bK7qfHlg9CnwRkRjKOXScj1fvYvaqnfxr3W5y8wupW6MKL90c4qLOTQOtLarAN7NrgIeAs4B0d4/4xHEz2wQcAgqBAncPRdOuiEi8cHfW7jzM7FU7mb1qJ4u37scdmtevwQ/7tGRAl2b0b9+I6lXKd4x9WUR7hr8cuBp4sQz7Xuzuu6NsT0QkcHkFRSzYtJdZK3fy4eqdbN2bC0D3lvW5e0AnLjmrKV1Or1ehI3DKIqrAd/dVQNwdlIhIeThyvICX52zmpX9tZO+RPKpXqcT5HZpw+0Ud+PaZTWlWr0bQJX6jiurDd+ADM3PgRXcfW9qOZjYKGAXQunUwV7JFREr6KujHfraBfUfzuahzGjf2a8P5HZpQs1rwXTVldcLAN7PZwGkRNt3v7jPK2M757p5tZk2BWWa22t0/i7Rj+D+DsQChUMjL+P4iIjF35HgBU+ZuZuxnxWf0F3VO485LOtKrdcOgSzslJwx8dx8QbSPunh3+vsvM3gbSgYiBLyIStKN5BUyZs5kXw0H/rU5p3DmgI70TNOi/Uu5dOmZWG6jk7ofCrwcCD5d3uyIiJ+toXgGvzN3Mi59uZM+RPC7slMZdSRD0X4l2WOb3geeANOBdM1vs7peZWXNgnLtfDjQD3g5f2K0CTHP396KsW0QkZr4e9Bd0bMJdAzrRp01yBP1Xoh2l8zbwdoT124DLw683Aj2iaUdEpDwUFBbxemYWT89aS86h4+Gg70ifNo2CLq1c6E5bEUk57s7sVbv43XurWb/rMH3aNOQvN/amb9vkDPqvKPBFJKUs2rKPx2euZv6mvbRvUpsXb+rDwC7NUuJ+IgW+iKSETbuP8NT7a3h32Xaa1KnGo1d15dq+rahaAU+aihcKfBFJansOH+e5j9bzytzNVK1ciTsv6citF7anTjk+LDxepd4Ri0hKyM0rZMLnXzLmkw3k5hdybd9W3HVJR5rG+fQH5UmBLyJJpbDIeXNhFn/4YA07Dx7n0i7NuHdQZzo0rRt0aYFT4ItI0vhi/W4efXcVK7cfpEerBjx3fW/S2yX3yJuTocAXkYS3Iecwj89cxexVu2jRoCZ/ur4X3+1+ekqMvDkZCnwRSVh7j+Tx7Oy1TJ23hRpVK3PvoDMZfl5balRNnBksK5ICX0QSzvGCQiZ/sYnnPlrPkeMFXJ/emrsv7USTOtWDLi2uKfBFJGG4OzOX7eCJ91axdW8uF3VO45eXn0WnZrogWxYKfBFJCIu37ueRf6wkc/M+Ojery8sj0rmwU1rQZSUUBb6IxLU9h4/zxD9X83pmFk3qVOfxq7vxo1ArKlfSBdmTpcAXkbhUWORMX7CFJ99bw5HjBdx2YXv++5KOKXmHbKzob05E4s7SrP088LflLMk6QL92jXjkqq7qp48BBb6IxI0DR/N56oPVTJ23hca1q/PHa3syuGdzjaePEQW+iASuqMh5Y2EWT/xzNfuP5jH0nLb8z8BO1KtRNejSkooCX0QCtXLbQX49YzkZm/fRu3UDHhmZztnN6wddVlKK9pm2TwHfBfKADcBwd98fYb9BwLNAZYqfdftENO2KSOI7dCyfZ2atY/KcTdSvWZUnf9idH/ZuSSWNvik30Z7hzwLuc/cCM/sdcB9wb8kdzKwy8GfgUiALWGBm77j7yijbFpEEtX7XIYZNXED2/lxuSG/Nzy/rTINa1YIuK+lF+xDzD0oszgV+GGG3dGB9+GHmmNl0YDCgwBdJQV9s2M3oKZlUq1KZN0afk7QPDI9HsXy21wjgnxHWtwC2lljOCq+LyMxGmVmGmWXk5OTEsDwRCdpbC7MYOmE+TevV4O3bz1XYV7ATnuGb2WzgtAib7nf3GeF97gcKgKnRFuTuY4GxAKFQyKN9PxEJnrvzpw/X88zstZx7RmPGDOlD/ZoagVPRThj47j7gm7ab2TDgSuASd48U0NlAqxLLLcPrRCQF5BUUcd9by3hzYRY/6N2Sx6/uRrUqqfPg8HgS7SidQcA9wLfc/Wgpuy0AOppZO4qD/jrghmjaFZHEcCA3n9FTMpmzcQ93D+jETy7poJuoAhTtKJ3ngerArPCHONfdR5tZc4qHX14eHsFzB/A+xcMyJ7j7iijbFZE4t3XvUUZMWsCmPUd4+kc9uLp3y6BLSnnRjtLpUMr6bcDlJZZnAjOjaUtEEsfSrP2MmJRBXkEhL4/oxzlnNA66JEF32opIjM1auZOfvLqIxnWqMX1UPzo01aRn8UKBLyIxM/HzL3n4Hyvp3qI+44b2Ja2uHjkYTxT4IgnK3XnonRW8vSg+Br05cOhYAQO7NOPZ63pRs5oeJB5vFPgiCeoPH6xl8pzNfKfraTSrVyPocgBo07gWN5/TVk+jilMKfJEENHXeZp7/eD3X9W3F41d301BHKRPd/SCSYD5ctZMH/racizun8ehVXRX2UmYKfJEEsnjrfu6Ytoizm9fn+Rt6U6WyfoSl7PSvRSRBbNp9hJGTFtCkbjUmDOtLbT3MW06SAl8kAew5fJxhE+dT5M7k4eka7iinRKcIInEuN6+QkZMz2H7gGNNu7Uf7tDpBlyQJSoEvEscKCov471cXsSRrP2Nu7KP54yUq6tIRiVPuzkN/X8HsVTv5zffOZlDXSI+lECk7Bb5InBrz6QZembuF277VnpvPaRt0OZIEFPgiceithVk8+d4avtejOfdedmbQ5UiSUOCLxJl/r9vNPW8s5Zz2jXnqmu5U0jQFEiMKfJE4smLbAUa/kskZaXV44aY+VK+iCcgkdhT4InFiY85hhk6YT90aVZg4vK8e8i0xp8AXiQPbD+Ry0/j5FDlMGdmP5g1qBl2SJCEFvkjA9hw+zpBx8ziYm8/LI9Lp0FQ3Vkn5iOrGKzN7CvgukAdsAIa7+/4I+20CDgGFQIG7h6JpVyRZHDqWz7CJC8jal8vLI9Lp2qJ+0CVJEov2DH8W0NXduwNrgfu+Yd+L3b2nwl6k2LH84ikTVm0/yJghvenXXg/6lvIVVeC7+wfuXhBenAu0jL4kkeSXX1jEj6cuZMGmvfzhRz349pnNgi5JUkAs+/BHAP8sZZsDH5hZppmN+qY3MbNRZpZhZhk5OTkxLE8kPhQVOT97fQkfrt7FI4O7Mrhni6BLkhRxwj58M5sNRJrE4353nxHe536gAJhaytuc7+7ZZtYUmGVmq939s0g7uvtYYCxAKBTyMhyDSMJwdx58ZwUzFm/j55d1Zkj/NkGXJCnkhIHv7gO+abuZDQOuBC5x94gB7e7Z4e+7zOxtIB2IGPgiyezpWWuZMnczt13YntsvOiPociTFRNWlY2aDgHuA77n70VL2qW1mdb96DQwElkfTrkgiGvevjTz3UfGDx3/xnTP1LFqpcNH24T8P1KW4m2axmb0AYGbNzWxmeJ9mwL/NbAkwH3jX3d+Lsl2RhPLXBVt59N1VXNHtdB77fjeFvQQiqnH47t6hlPXbgMvDrzcCPaJpRySRzVy2nV+8tZQLO6XxzLU9qazJ0CQgutNWpBy9tTCLO6cvonfrhrwwpDfVquhHToKjRxyKlAN359kP1/HH2es494zGvHBTH2pV04+bBEv/AkViLK+giF+8tZS3Fmbzwz4t+e33u+nMXuKCAl8khg4czWf0K5nM2biHn17aiTu+3UEXaCVuKPBFYmTr3qMMmzifLXuP8sy1Pfh+L800IvFFgS8SA4u37ueWyQvIL3SmjOxHf02EJnFIgS8SpfdX7ODO6YtIq1ud6cM0n73ELwW+yClydyZ8volH311Jj5YNGDc0RJM61YMuS6RUCnyRU1BY5Dz89xVMnrOZQWefxjPX9qRmNT1wXOKbAl/kJB3NK+Anry5i9qpd3HpBO+77zllU0t2zkgAU+CIn4bO1Ofzm7yv4cvcRHh58Njef0zbokkTKTIEvUgZf7j7CY++uZPaqXbRpXItJw9O5sFNa0GWJnBQFvsg3OHQsn+c/Ws+Ez7+kWuVK/OI7ZzL8vLZUr6L+ekk8CnyRCIqKnDcys3jy/dXsPpzHNX1a8vNBnWlat0bQpYmcMgW+yNdkbNrLb/6+kmXZB+jTpiEThvWle8sGQZclEjUFvkhY9v5cnvjnav6+ZBun16/Bs9f15Hs9mmsuHEkaCnxJebl5hbz42QZe+HQD7vCTSzoy+lvtNZ2xJB39i5aUVVjkvLkwi6c/WMuOg8e4ovvp3PedM2nZsFbQpYmUi6gD38weAQYDRcAuYFj4EYdf328o8Kvw4qPuPjnatkVO1Wdrc/jtzFWs3nGIHq0a8Kfre5HerlHQZYmUq1ic4T/l7g8AmNlPgF8Do0vuYGaNgAeBEOBAppm94+77YtC+SJmt3HaQx/+5in+t203rRrV4/oZeXNHtdPXTS0qIOvDd/WCJxdoUB/rXXQbMcve9AGY2CxgEvBpt+5KY3sjMYnn2AX51xVlUqVz+T4PafiCXP3ywljcXZlGvRlUeuLILQ/q31nh6SSkx6cM3s8eAm4EDwMURdmkBbC2xnBVeF+m9RgGjAFq3bh2L8iTOvLt0Oz9/Ywnu4UnIBp9dbmfYh47l88KnGxj/7y8pKoJbL2jPjy/qQP1aVculPZF4VqbAN7PZwGkRNt3v7jPc/X7gfjO7D7iD4u6bU+LuY4GxAKFQKNJvC5LA5m7cw92vLaZP64Z0a1mfiZ9vom2T2ow8v11M28kvLOLV+Vt4dvY69hzJY3DP5vxsYGdaNdIFWUldZQp8dx9QxvebCszk/wZ+NnBRieWWwCdlfE9JEqt3HOTWlzNo3bgW44aGqFejKtv3H+PRd1fSqmFNBp4d6Zzi5K3beYjbXslkY84R+rdvxMTLz9KNUyJA1J2nZtaxxOJgYHWE3d4HBppZQzNrCAwMr5MUsW1/LsMmLKBWtcpMHpFOg1rVqFTJeObannRvUZ87py9mWdaBqNvJ3LyPa16cw6FjBYy7OcSrt/ZX2IuExeJq2RNmttzMllIc5HcCmFnIzMYBhC/WPgIsCH89/NUFXEl++4/mMXTCfI4cL2DyiHRaNKj5n201q1XmpaEhGtWuxojJC8jen3vK7Xy8Zhc3jptLg5pVeXP0uQzo0kyjb0RKMPf47SYPhUKekZERdBkShWP5hdw0fh5Lth5g8oh0zjkj8sO91+48xA/+8gUtGtbk9dHnULfGyV1UfXtRFj9/fSmdT6vLpOHppNXVowYlNZlZpruHIm0r//FwkrIKi5w7py8iY/M+nr62R6lhD9CpWV3+MqQ363Yd5o5piygoLCpzO+P+tZG7X1tCertGTB/VX2EvUgoFvpQLd+ehd1bw/oqdPHBFF67s3vyEf+aCjmk8elVXPl2bw4PvrOBEv326O4/PXMWj767i8m6nMXF435P+zUAklWguHSkXf/lkA1Pmbua2C9sz4iSGXF6f3ppNe47w4qcbadekNrdc0D7ifgWFRfzirWW8kZnFTf3b8ND3zqaynisr8o0U+BJzr2ds5an313BVz+bcO+jMk/7z9152Jlv2HOWxmato1agWl31tuGZuXiF3TFvIh6t3cdeAjtx5SUddnBUpA3XpSEx9smYXv3hrGed3aMKTP+xBpVM46/5quGaPlg24c/oilmbt/8+2A0fzuWn8PD5as4tHr+rKXQM6KexFykiBLzGzZOt+bp+6kM7N6jJmSG+qVTn1f141qlbmpZtDNKlTnZGTM8jen8uOA8e45sUvWJp1gD/f0Jsh/dvEsHqR5KcuHYmJzXuOMGLSAhrVrsakEbG5eJpWtzoTh/Xl6jFfMGzCfI7mFXIgN59Jw/tybocmMahaJLXoDF+itvvwcW6eMJ8idyaPSI/pg747NqvLmBv78OXuIxwvKGT6qP4Ke5FTpDN8icrRvAJGTlrAjgPHmHZrf85IqxPzNs7v2IS3bz+PtLrVOa1+7P4zEUk1Cnw5ZQWFRdwxbRHLsg/wwpA+9GnTsNza6tayfrm9t0iqUODLKXF3HpixnI9WF4+WidVMlyJSftSHL6fkuY/W8+r8rfz44jM0WkYkQSjw5aT9dcFWnp61lqt7t+BnAzsHXY6IlJECX07Kx2t2cd/by7igYxOeuLq7bnoSSSAKfCmzpVn7+fHUhZx5Wl3GDOkT1Y1VIlLx9BMrZbJlz1FGTFpAw1rVmDisL3Wq63q/SKLRT62c0J7Dxxk6cT4FRc70Eek0raex8CKJSGf48o1y8woZOTmDbftzGXdziA5NY39jlYhUjKjO8M3sEYofXF4E7AKGufu2CPsVAsvCi1vc/XvRtCsVo6CwiP9+dRFLsvYz5sY+hNo2CrokEYlCtGf4T7l7d3fvCfwD+HUp++W6e8/wl8I+Abg7D76zgtmrdvLQd89mUFfdWCWS6KIKfHc/WGKxNhC/T0SXk/LMrLVMnbeF0d86g6Hntg26HBGJgaj78M3sMTPbCtxI6Wf4Ncwsw8zmmtlVJ3i/UeF9M3JycqItT07BhH9/yZ8+Ws+PQi25d5BurBJJFnaiB0Wb2Wwg0u/z97v7jBL73QfUcPcHI7xHC3fPNrP2wEfAJe6+4UTFhUIhz8jIONFuEkNvZmbx09eXcNnZzfjzDb2pUlnX9UUSiZllunso0rYTXrR19wFlbGcqMBP4P4Hv7tnh7xvN7BOgF3DCwJeKNWvlTu55cynndWjMs9f1UtiLJJmofqLNrGOJxcHA6gj7NDSz6uHXTYDzgJXRtCuxN2fDHn48bSFdm9fjxZtC1KhaOeiSRCTGor3x6gkz60zxsMzNwGgAMwsBo939FuAs4EUzK6L4P5gn3F2BH0eWZR3g1pczaN2oFpOGp+suWpEkFdVPtrv/oJT1GcAt4ddfAN2iaUfKz4acwwydOJ/6NasyZWQ6DWtXC7okESkn6qRNYdv253LTuHkYMGVkOqfXrxl0SSJSjhT4KWrvkTxuGj+PQ8cKmDwinfbl8CxaEYkv6qxNQYePFzBs4nyy9uXy8oh0urbQ82JFUoECP8Ucyy/k1skZrNh2kBeH9KFf+8ZBlyQiFURdOimkoLCIn7y6iDkb9/D7a7ozoEuzoEsSkQqkwE8R+YVF/Oz1JXywcicPfbcL3+/VMuiSRKSCqUsnBRzLL+SOaQuZvWoXP7+sM8POaxd0SSISAAV+kjt4LJ9bJmewYNNeHh58Njef0zbokkQkIAr8JLb78HGGTpjPmh2H+OO1PRncs0XQJYlIgBT4SWrr3qPcPGE+2w/k8tLQEBd3bhp0SSISMAV+Elq38xA3jZ/P0bwCXhnZT48mFBFAgZ90Fm3Zx/BJC6hauRKv3XYOZ51eL+iSRCROKPCTyL/W5XDblEya1KnOKyP70bpxraBLEpE4osBPEjOXbefO6Ys4I60OL49Ip2m9GkGXJCJxRoGfBF6dv4Vfvr2MPq0bMn5oX+rXqhp0SSIShxT4CczdGfPpBp58bw0XdU5jzI19qFlNT6oSkcgU+AmqsMh57N1VTPj8Swb3bM7vr+lBVT2DVkS+gQI/AR3LL+Su6Yt5b8UOhp/Xlgeu6EKlShZ0WSIS52J2SmhmPzUzDz+oPNL2oWa2Lvw1NFbtppq9R/K44aW5vL9yBw9c2YUHv3u2wl5EyiQmZ/hm1goYCGwpZXsj4EEgBDiQaWbvuPu+WLSfKjbtPsKwifPZfuAYf7mhN9/pdnrQJYlIAonVGf4zwD0Uh3kklwGz3H1vOORnAYNi1HZKWLhlH1eP+YIDuflMu7Wfwl5ETlrUgW9mg4Fsd1/yDbu1ALaWWM4Kr4v0fqPMLMPMMnJycqItLym8t3wH14+dS90aVXjr9vPo00ZTJYjIyStTl46ZzQZOi7DpfuCXFHfnxIS7jwXGAoRCodJ+Y0gZkz7/kt/8YyU9WjZg3NAQTepUD7okEUlQZQp8dx8Qab2ZdQPaAUvMDKAlsNDM0t19R4lds4GLSiy3BD45hXpTRlGR89uZqxj37y8Z2KUZz17XS2PsRSQqUV20dfdlwH/m3TWzTUDI3Xd/bdf3gd+aWcPw8kDgvmjaTmbH8gv5n78uZuayHQw7ty0PXNmFyhqJIyJRKrdx+GYWAka7+y3uvtfMHgEWhDc/7O57y6vtRLb3SB63vpxB5uZ9/OqKsxh5fjvCvz2JiEQlpoHv7m1LvM4AbimxPAGYEMv2ks2Xu48wctICsvbn8pcbe3O5RuKISAzpTts4MWfDHka/kkklg2m36KElIhJ7Cvw4MH3+Fn71t+W0a1Kb8UP7ah57ESkXCvwAFRY5j4dH4nyrUxrP3dCLejU0tbGIlA8FfkAOHy/gzlcX8eHqXQw7ty2/uuIsqmi2SxEpRwr8AGTtO8otkzNYt+swj17VlSH92wRdkoikAAV+BcvcvJfbpmSSV1DE5OHpnN8x4uSiIiIxp8CvQH9blM09byyleYMajL+tL2ek1Qm6JBFJIQr8ClBU5Dw9ay3Pf7ye/u0bMebGPjSsXS3oskQkxSjwy1luXiE/fb14moTr+rbi4cFdqVZFF2dFpOIp8MvR1r1H+a+pmazYdlDTJIhI4BT45eTjNbu4a/piitwZPzTEt89sFnRJIpLiFPgxVlTkPPvhOv700TrOPK0eLwzpTZvGtYMuS0REgR9L+4/mcef0xXy6Nocf9G7Jo1d11Rz2IhI3FPgxsjz7AKNfyWTXweM89v2u3JDeWv31IhJXFPgx8NqCLTwwYwVNalfjr6PPoWerBkGXJCLyfyjwo3Asv5AHZ6zgtYytnN+hCX+6vheNNL5eROKUAv8UfTXkcnn2Qe64uAN3X9pJjyEUkbimwD8Fn6zZxV2vLaawyHnp5hCXdtGQSxGJfwr8k+DuPP/Rep6evZbOzerywpA+tG2iIZcikhhiEvhm9lPg90Cau++OsL0QWBZe3OLu34tFuxXpyPECfv7GEmYu28FVPZvz+NXdNeRSRBJK1IFvZq2AgcCWb9gt1917RttWULbuPcqtL2ewduch7r/8LG65QFMkiEjiicUZ/jPAPcCMGLxX3JmzYQ+3T82koMiZODydb3VKC7okEZFTEtW0jWY2GMh29yUn2LWGmWWY2Vwzu+oE7zkqvG9GTk5ONOVFxd15ec4mhoyfR6Pa1Zjx4/MU9iKS0E54hm9ms4HTImy6H/glxd05J9LG3bPNrD3wkZktc/cNkXZ097HAWIBQKORleO+YO15QPL5++oKtXHJmU/54XU/q6uHiIpLgThj47j4g0noz6wa0A5aE+7NbAgvNLN3dd3ztPbLD3zea2SdALyBi4Act59BxRr+SSebmfdxxcQf+59JOVNL4ehFJAqfch+/uy4CmXy2b2SYg9PVROmbWEDjq7sfNrAlwHvDkqbZbnpZm7ee2KZnsO5rH8zf04sruzYMuSUQkZsrl0UtmFjKzceHFs4AMM1sCfAw84e4ry6PdaPxtUTbXvDCHSma8+V/nKuxFJOnE7MYrd29b4nUGcEv49RdAt1i1E2tFRc7v3lvNi59tJL1dI8bc2JvGdaoHXZaISMyl9J227s4DM5Yzdd4WhvRvzYPfPZuqlfW8WRFJTikd+E++v4ap87bwXxedwb2Dzgy6HBGRcpWyp7N/+WQ9Yz7ZwI39WnPPZZ2DLkdEpNylZOBPmbuZJ99bw+CezXlkcFdNkyAiKSHlAv9vi7L59YzlDDirKb+/pofG2ItIykipwJ+9cic/fX0J/ds15vkbeusCrYiklJRJvC827Ob2aQvp2rweLw0NUaOqpjYWkdSSEoG/eOt+bp2cQdvGtZg0PJ061VN6cJKIpKikD/w1Ow4xbOJ8GtepzpSR/Wioh4yLSIpK6sDfvOcIN42fR/UqlZh6Sz+a1asRdEkiIoFJ2sDfceAYQ8bPI7+wiFdG9qNVo1pBlyQiEqik7MzeeySPIePnse9IPtNu7UfHZnWDLklEJHBJd4Z/+HgBQyfMZ+veo4wbGqJ7ywZBlyQiEheS7gy/WuVKnJFWm7sv7Uj/9o2DLkdEJG4kX+BXqcQfr+sVdBkiInEn6bp0REQkMgW+iEiKUOCLiKSIqALfzB4ys2wzWxz+uryU/QaZ2RozW29mv4imTREROTWxuGj7jLv/vrSNZlYZ+DNwKZAFLDCzd+LxQeYiIsmsIrp00oH17r7R3fOA6cDgCmhXRERKiEXg32FmS81sgpk1jLC9BbC1xHJWeF1EZjbKzDLMLCMnJycG5YmICJQh8M1stpktj/A1GBgDnAH0BLYDf4i2IHcf6+4hdw+lpaVF+3YiIhJ2wj58dx9Qljcys5eAf0TYlGoSlsIAAANFSURBVA20KrHcMrzuhDIzM3eb2eay7BtBE2D3Kf7ZRJIqxwmpc6ypcpyQOsdakcfZprQNUV20NbPT3X17ePH7wPIIuy0AOppZO4qD/jrghrK8v7uf8im+mWW4e+hU/3yiSJXjhNQ51lQ5TkidY42X44x2lM6TZtYTcGATcBuAmTUHxrn75e5eYGZ3AO8DlYEJ7r4iynZFROQkRRX47n5TKeu3AZeXWJ4JzIymLRERiU4y32k7NugCKkiqHCekzrGmynFC6hxrXBynuXvQNYiISAVI5jN8EREpQYEvIpIiki7wU2miNjPbZGbLwhPXZQRdTyyF79zeZWbLS6xrZGazzGxd+HukO7sTSinHWaZJCROJmbUys4/NbKWZrTCzO8Prk/EzLe1YA/9ck6oPPzxR21pKTNQGXJ+sE7WZ2SYg5O5Jd+OKmV0IHAZedveu4XVPAnvd/Ynwf+YN3f3eIOuMVinH+RBw+JsmJUw0ZnY6cLq7LzSzukAmcBUwjOT7TEs71h8R8OeabGf4mqgtSbj7Z8Der60eDEwOv55M8Q9RQivlOJOOu29394Xh14eAVRTPqZWMn2lpxxq4ZAv8k5qoLQk48IGZZZrZqKCLqQDNStzZvQNoFmQx5exEkxImLDNrC/QC5pHkn+nXjhUC/lyTLfBTzfnu3hv4DvDjcPdASvDivsjk6Y/8/8V8UsJ4YWZ1gDeBu9z9YMltyfaZRjjWwD/XZAv8U56oLRG5e3b4+y7gbYq7tJLZznD/6Ff9pLsCrqdcuPtOdy909yLgJZLkczWzqhQH4FR3fyu8Oik/00jHGg+fa7IF/n8majOzahRP1PZOwDWVCzOrHb4ghJnVBgYSefK6ZPIOMDT8eigwI8Bays1XARhW2qSECcXMDBgPrHL3p0tsSrrPtLRjjYfPNalG6QCEhzr9kf83UdtjAZdULsysPcVn9VA8J9K0ZDpWM3sVuIjiaWV3Ag8CfwP+CrQGNgM/cveEvuBZynFeRPGv/f+ZlLBEP3dCMrPzgX8By4Ci8OpfUty3nWyfaWnHej0Bf65JF/giIhJZsnXpiIhIKRT4IiIpQoEvIpIiFPgiIilCgS8ikiIU+CIiKUKBLyKSIv4XUKYByF2d4GMAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"11oJHE1hZknp","executionInfo":{"status":"ok","timestamp":1616492002184,"user_tz":0,"elapsed":1263,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"b31e2135-12e2-4224-8ae0-c3f0be8a00d7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 3\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_{i}\", \"point_maze_time\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Standard\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[2])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["95.64999999999947\n","Standard\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"cpUcYkH1afDr"},"source":[""],"execution_count":null,"outputs":[]}]}