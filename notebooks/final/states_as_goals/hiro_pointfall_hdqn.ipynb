{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointfall_hdqn.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1618902495061,"user_tz":-60,"elapsed":21953,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"472dfa87-a984-4ddf-f51e-2aec4ca59ddd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_fall.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1618902559595,"user_tz":-60,"elapsed":3399,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_hdqn/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.meta_controller.state_dict(),\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_hdqn/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.load_state_dict(checkpoint['meta_controller'])\n","    model.meta_controller_target = copy.deepcopy(model.meta_controller)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1618902563269,"user_tz":-60,"elapsed":6580,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1618902563276,"user_tz":-60,"elapsed":6278,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1618902563280,"user_tz":-60,"elapsed":6056,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_fall import PointFallEnv \n","env = NormalizedEnv(PointFallEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1618902563282,"user_tz":-60,"elapsed":5599,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1618902563284,"user_tz":-60,"elapsed":5449,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1618902563287,"user_tz":-60,"elapsed":5235,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1618902563291,"user_tz":-60,"elapsed":4859,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1618902563575,"user_tz":-60,"elapsed":4794,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 20000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 100000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.0\n","        self.EPS_DECAY = 10000 # in number of steps\n","        self.TAU = 0.001\n","\n","        self.eps_printed = False\n","\n","        self.policy_update = 2\n","        self.tot_updates = 0\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, warmup, is_training):\n","        if warmup: \n","            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            if eps_threshold <= 0.2 and not self.eps_printed:\n","                self.eps_printed = True\n","                print(\"EPS_THRESHOLD below 0.2\")\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.tot_updates += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        loss_val = loss.item()\n","\n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","\n","        if self.tot_updates % self.policy_update == 0:\n","            soft_update(target, self, self.TAU)\n","\n","        return loss_val"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1618902563577,"user_tz":-60,"elapsed":4662,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        learning_rate = 2.5e-4\n","        self.meta_controller = DQN(nb_states, 11 * 11).to(device)\n","        self.meta_controller_optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n","        self.meta_controller_target = DQN(nb_states, 11 * 11, mem_len = 0).to(device)\n","        self.meta_controller_target.eval()\n","\n","        self.max_goal_dist = torch.from_numpy(np.array([2., 3.])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0.5, 1.5])).to(device)\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def action_to_2D(self, a):\n","        x = (a % 11)\n","        y = (a // 11)\n","        return -1.0 + 0.2 * torch.cat([x, y], axis=1).float()\n","\n","    def convert_goal(self, a):\n","        return self.action_to_2D(a) * self.max_goal_dist + self.goal_offset\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        return self.meta_controller.act(s_t, warmup, is_training)\n","      \n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI","executionInfo":{"status":"ok","timestamp":1618902563579,"user_tz":-60,"elapsed":4431,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["import time\n","SAVE_OFFSET = 1\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 200\n","    num_episodes = 10000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal_raw = agent.select_goal(state, i_episode <= warmup, True)\n","            goal = agent.convert_goal(goal_raw)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal_raw, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 400 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -47.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1618902573417,"user_tz":-60,"elapsed":6355,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data - eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    # initial forward pass\n","    goal = agent.meta_controller(g_state)\n","\n","    if targeted:\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = F.mse_loss(goal, -goal) # doing -goal inverts the argmax \n","\n","    agent.meta_controller.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0] * (11 * 11)], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal_raw = agent.select_goal(og_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal_raw = agent.select_goal(state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhC6f7N6sJoa","executionInfo":{"status":"ok","timestamp":1618917203329,"user_tz":-60,"elapsed":14629672,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"390cd7d0-7e50-4275-cf59-14ebadac1dca"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","\"\"\"\n","i = 1\n","while i < 7:\n","    agent = train_model()\n","    #agent = HIRO(n_observations, n_actions).to(device)\n","    #load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\"\"\"\n","\n","i = 0\n","while i < 6:\n","    if i == 3:\n","        i += 1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [96.15899999999641], 0.02: [22.258000000005772], 0.04: [48.803000000003514], 0.06: [64.83999999999416], 0.08: [14.94700000000817], 0.1: [46.48200000000523], 0.12: [4.9959999999991], 0.14: [-7.201000000001517], 0.16: [-16.61299999999516], 0.18: [-24.100999999981983], 0.2: [-21.438999999979533]}, 'action': {0.0: [97.2849999999957], 0.02: [98.0429999999964], 0.04: [87.3519999999903], 0.06: [-5.521000000003792], 0.08: [4.3989999999986855], 0.1: [4.115999999999144], 0.12: [9.755999999994508], 0.14: [20.298000000015016], 0.16: [20.649000000010574], 0.18: [-4.505000000008063], 0.2: [-14.637999999998437]}}\n","0 fgsm (t): {'goal': {0.0: [97.17799999999487], 0.02: [38.48200000001634], 0.04: [30.500000000009777], 0.06: [63.32299999998191], 0.08: [15.1250000000028], 0.1: [37.174000000014566], 0.12: [5.028999999999612], 0.14: [-4.286000000006214], 0.16: [-5.905000000005804], 0.18: [-22.57699999998124], 0.2: [-24.368999999984876]}, 'action': {0.0: [95.91599999999549], 0.02: [95.73999999999303], 0.04: [10.925000000003136], 0.06: [8.394999999994972], 0.08: [-35.29499999997204], 0.1: [-32.295999999973446], 0.12: [-46.13599999999286], 0.14: [-30.949999999975486], 0.16: [-41.38599999997002], 0.18: [-33.82399999997483], 0.2: [-47.47100000000125]}}\n","1 fgsm (ut): {'goal': {0.0: [96.15899999999641, 91.9079999999858], 0.02: [22.258000000005772, 97.70799999999578], 0.04: [48.803000000003514, 89.92299999998816], 0.06: [64.83999999999416, 31.98200000001243], 0.08: [14.94700000000817, -20.37099999999178], 0.1: [46.48200000000523, -2.097000000001648], 0.12: [4.9959999999991, -16.9359999999941], 0.14: [-7.201000000001517, -43.02399999998121], 0.16: [-16.61299999999516, -48.624000000001544], 0.18: [-24.100999999981983, -47.253999999994775], 0.2: [-21.438999999979533, -45.94199999999023]}, 'action': {0.0: [97.2849999999957, 89.3059999999852], 0.02: [98.0429999999964, -6.530000000006604], 0.04: [87.3519999999903, 5.311999999994893], 0.06: [-5.521000000003792, 1.99999999999807], 0.08: [4.3989999999986855, -16.74199999999219], 0.1: [4.115999999999144, -25.659999999974893], 0.12: [9.755999999994508, -41.45299999997117], 0.14: [20.298000000015016, -42.82399999997682], 0.16: [20.649000000010574, -48.56200000000499], 0.18: [-4.505000000008063, -48.73500000000107], 0.2: [-14.637999999998437, -45.24299999998668]}}\n","1 fgsm (t): {'goal': {0.0: [97.17799999999487, 91.00099999998632], 0.02: [38.48200000001634, 97.69799999999577], 0.04: [30.500000000009777, 86.64599999998681], 0.06: [63.32299999998191, 37.18800000001513], 0.08: [15.1250000000028, -20.151999999984895], 0.1: [37.174000000014566, -12.823999999994797], 0.12: [5.028999999999612, -4.818000000006438], 0.14: [-4.286000000006214, -36.979999999975455], 0.16: [-5.905000000005804, -48.58000000000506], 0.18: [-22.57699999998124, -50.00000000000659], 0.2: [-24.368999999984876, -44.4309999999927]}, 'action': {0.0: [95.91599999999549, 90.52199999998494], 0.02: [95.73999999999303, -6.730000000003295], 0.04: [10.925000000003136, 16.590000000003414], 0.06: [8.394999999994972, -15.765999999999446], 0.08: [-35.29499999997204, -24.8559999999801], 0.1: [-32.295999999973446, -43.37199999997906], 0.12: [-46.13599999999286, -47.315999999995], 0.14: [-30.949999999975486, -47.73699999999747], 0.16: [-41.38599999997002, -43.93299999998027], 0.18: [-33.82399999997483, -50.00000000000659], 0.2: [-47.47100000000125, -48.93300000000179]}}\n","2 fgsm (ut): {'goal': {0.0: [96.15899999999641, 91.9079999999858, 94.67499999999218], 0.02: [22.258000000005772, 97.70799999999578, 82.57699999999414], 0.04: [48.803000000003514, 89.92299999998816, 53.64399999999787], 0.06: [64.83999999999416, 31.98200000001243, 44.42200000000861], 0.08: [14.94700000000817, -20.37099999999178, 28.263000000010674], 0.1: [46.48200000000523, -2.097000000001648, 32.91900000001815], 0.12: [4.9959999999991, -16.9359999999941, 42.30700000000571], 0.14: [-7.201000000001517, -43.02399999998121, 26.12500000000593], 0.16: [-16.61299999999516, -48.624000000001544, -4.625000000006548], 0.18: [-24.100999999981983, -47.253999999994775, -25.398999999974123], 0.2: [-21.438999999979533, -45.94199999999023, -7.293000000006254]}, 'action': {0.0: [97.2849999999957, 89.3059999999852, 91.74799999999516], 0.02: [98.0429999999964, -6.530000000006604, 85.0189999999916], 0.04: [87.3519999999903, 5.311999999994893, 65.32899999997043], 0.06: [-5.521000000003792, 1.99999999999807, 20.119000000012488], 0.08: [4.3989999999986855, -16.74199999999219, 27.41100000001276], 0.1: [4.115999999999144, -25.659999999974893, 21.62900000001421], 0.12: [9.755999999994508, -41.45299999997117, 21.477000000011188], 0.14: [20.298000000015016, -42.82399999997682, 9.807000000013735], 0.16: [20.649000000010574, -48.56200000000499, -18.67099999998399], 0.18: [-4.505000000008063, -48.73500000000107, -31.628999999972763], 0.2: [-14.637999999998437, -45.24299999998668, -34.6459999999745]}}\n","2 fgsm (t): {'goal': {0.0: [97.17799999999487, 91.00099999998632, 93.18799999999224], 0.02: [38.48200000001634, 97.69799999999577, 82.52799999998882], 0.04: [30.500000000009777, 86.64599999998681, 58.151999999975445], 0.06: [63.32299999998191, 37.18800000001513, 29.48800000000659], 0.08: [15.1250000000028, -20.151999999984895, 33.181000000017185], 0.1: [37.174000000014566, -12.823999999994797, 42.7380000000109], 0.12: [5.028999999999612, -4.818000000006438, 44.66100000000657], 0.14: [-4.286000000006214, -36.979999999975455, 33.3140000000109], 0.16: [-5.905000000005804, -48.58000000000506, 0.1329999999986125], 0.18: [-22.57699999998124, -50.00000000000659, -15.778999999986004], 0.2: [-24.368999999984876, -44.4309999999927, -13.191999999987273]}, 'action': {0.0: [95.91599999999549, 90.52199999998494, 94.67899999999455], 0.02: [95.73999999999303, -6.730000000003295, 61.93399999997664], 0.04: [10.925000000003136, 16.590000000003414, 40.40200000002496], 0.06: [8.394999999994972, -15.765999999999446, 0.49199999999752436], 0.08: [-35.29499999997204, -24.8559999999801, -26.045999999975475], 0.1: [-32.295999999973446, -43.37199999997906, -40.361999999968106], 0.12: [-46.13599999999286, -47.315999999995, -47.3429999999951], 0.14: [-30.949999999975486, -47.73699999999747, -50.00000000000659], 0.16: [-41.38599999997002, -43.93299999998027, -50.00000000000659], 0.18: [-33.82399999997483, -50.00000000000659, -50.00000000000659], 0.2: [-47.47100000000125, -48.93300000000179, -50.00000000000659]}}\n","4 fgsm (ut): {'goal': {0.0: [96.15899999999641, 91.9079999999858, 94.67499999999218, 97.86699999999608], 0.02: [22.258000000005772, 97.70799999999578, 82.57699999999414, 94.0879999999894], 0.04: [48.803000000003514, 89.92299999998816, 53.64399999999787, 72.11799999997588], 0.06: [64.83999999999416, 31.98200000001243, 44.42200000000861, 39.02700000002122], 0.08: [14.94700000000817, -20.37099999999178, 28.263000000010674, -27.550999999972436], 0.1: [46.48200000000523, -2.097000000001648, 32.91900000001815, -19.259999999989727], 0.12: [4.9959999999991, -16.9359999999941, 42.30700000000571, -12.007999999980289], 0.14: [-7.201000000001517, -43.02399999998121, 26.12500000000593, -22.571999999983436], 0.16: [-16.61299999999516, -48.624000000001544, -4.625000000006548, -25.66999999997782], 0.18: [-24.100999999981983, -47.253999999994775, -25.398999999974123, -26.83099999997652], 0.2: [-21.438999999979533, -45.94199999999023, -7.293000000006254, -33.91599999997448]}, 'action': {0.0: [97.2849999999957, 89.3059999999852, 91.74799999999516, 97.87099999999604], 0.02: [98.0429999999964, -6.530000000006604, 85.0189999999916, 89.86599999998408], 0.04: [87.3519999999903, 5.311999999994893, 65.32899999997043, 64.31899999997071], 0.06: [-5.521000000003792, 1.99999999999807, 20.119000000012488, 53.93299999999821], 0.08: [4.3989999999986855, -16.74199999999219, 27.41100000001276, 70.41599999997378], 0.1: [4.115999999999144, -25.659999999974893, 21.62900000001421, 32.40700000001763], 0.12: [9.755999999994508, -41.45299999997117, 21.477000000011188, 15.397999999998824], 0.14: [20.298000000015016, -42.82399999997682, 9.807000000013735, -9.200000000005327], 0.16: [20.649000000010574, -48.56200000000499, -18.67099999998399, -12.49099999999944], 0.18: [-4.505000000008063, -48.73500000000107, -31.628999999972763, -17.40999999997783], 0.2: [-14.637999999998437, -45.24299999998668, -34.6459999999745, -24.72299999998036]}}\n","4 fgsm (t): {'goal': {0.0: [97.17799999999487, 91.00099999998632, 93.18799999999224, 97.86599999999608], 0.02: [38.48200000001634, 97.69799999999577, 82.52799999998882, 93.20599999998765], 0.04: [30.500000000009777, 86.64599999998681, 58.151999999975445, 52.152999999981745], 0.06: [63.32299999998191, 37.18800000001513, 29.48800000000659, 28.263000000010337], 0.08: [15.1250000000028, -20.151999999984895, 33.181000000017185, -27.187999999977816], 0.1: [37.174000000014566, -12.823999999994797, 42.7380000000109, -26.076999999975875], 0.12: [5.028999999999612, -4.818000000006438, 44.66100000000657, -15.692999999992855], 0.14: [-4.286000000006214, -36.979999999975455, 33.3140000000109, -12.783999999998917], 0.16: [-5.905000000005804, -48.58000000000506, 0.1329999999986125, -29.30999999997202], 0.18: [-22.57699999998124, -50.00000000000659, -15.778999999986004, -30.274999999971143], 0.2: [-24.368999999984876, -44.4309999999927, -13.191999999987273, -22.018999999981975]}, 'action': {0.0: [95.91599999999549, 90.52199999998494, 94.67899999999455, 97.85399999999603], 0.02: [95.73999999999303, -6.730000000003295, 61.93399999997664, 83.07999999998405], 0.04: [10.925000000003136, 16.590000000003414, 40.40200000002496, 71.01799999996561], 0.06: [8.394999999994972, -15.765999999999446, 0.49199999999752436, 16.120000000003753], 0.08: [-35.29499999997204, -24.8559999999801, -26.045999999975475, -24.368999999981785], 0.1: [-32.295999999973446, -43.37199999997906, -40.361999999968106, -35.299999999971575], 0.12: [-46.13599999999286, -47.315999999995, -47.3429999999951, -29.20999999997757], 0.14: [-30.949999999975486, -47.73699999999747, -50.00000000000659, -44.570999999982284], 0.16: [-41.38599999997002, -43.93299999998027, -50.00000000000659, -50.00000000000659], 0.18: [-33.82399999997483, -50.00000000000659, -50.00000000000659, -48.787000000001264], 0.2: [-47.47100000000125, -48.93300000000179, -50.00000000000659, -48.90500000000169]}}\n","5 fgsm (ut): {'goal': {0.0: [96.15899999999641, 91.9079999999858, 94.67499999999218, 97.86699999999608, 97.65899999999573], 0.02: [22.258000000005772, 97.70799999999578, 82.57699999999414, 94.0879999999894, -21.99999999998589], 0.04: [48.803000000003514, 89.92299999998816, 53.64399999999787, 72.11799999997588, -5.254000000005386], 0.06: [64.83999999999416, 31.98200000001243, 44.42200000000861, 39.02700000002122, 61.15099999997613], 0.08: [14.94700000000817, -20.37099999999178, 28.263000000010674, -27.550999999972436, 65.13399999999675], 0.1: [46.48200000000523, -2.097000000001648, 32.91900000001815, -19.259999999989727, -15.061999999988837], 0.12: [4.9959999999991, -16.9359999999941, 42.30700000000571, -12.007999999980289, -18.631999999985087], 0.14: [-7.201000000001517, -43.02399999998121, 26.12500000000593, -22.571999999983436, -43.09399999997767], 0.16: [-16.61299999999516, -48.624000000001544, -4.625000000006548, -25.66999999997782, -46.45099999999095], 0.18: [-24.100999999981983, -47.253999999994775, -25.398999999974123, -26.83099999997652, -47.48499999999647], 0.2: [-21.438999999979533, -45.94199999999023, -7.293000000006254, -33.91599999997448, -47.59899999999603]}, 'action': {0.0: [97.2849999999957, 89.3059999999852, 91.74799999999516, 97.87099999999604, 97.64899999999562], 0.02: [98.0429999999964, -6.530000000006604, 85.0189999999916, 89.86599999998408, -5.27800000000227], 0.04: [87.3519999999903, 5.311999999994893, 65.32899999997043, 64.31899999997071, 80.98299999998702], 0.06: [-5.521000000003792, 1.99999999999807, 20.119000000012488, 53.93299999999821, 93.21899999998875], 0.08: [4.3989999999986855, -16.74199999999219, 27.41100000001276, 70.41599999997378, 79.37199999998403], 0.1: [4.115999999999144, -25.659999999974893, 21.62900000001421, 32.40700000001763, 17.124000000009236], 0.12: [9.755999999994508, -41.45299999997117, 21.477000000011188, 15.397999999998824, 9.83799999999513], 0.14: [20.298000000015016, -42.82399999997682, 9.807000000013735, -9.200000000005327, -5.153000000006914], 0.16: [20.649000000010574, -48.56200000000499, -18.67099999998399, -12.49099999999944, -13.254000000000808], 0.18: [-4.505000000008063, -48.73500000000107, -31.628999999972763, -17.40999999997783, -18.868999999986105], 0.2: [-14.637999999998437, -45.24299999998668, -34.6459999999745, -24.72299999998036, -30.69899999997511]}}\n","5 fgsm (t): {'goal': {0.0: [97.17799999999487, 91.00099999998632, 93.18799999999224, 97.86599999999608, 97.68299999999579], 0.02: [38.48200000001634, 97.69799999999577, 82.52799999998882, 93.20599999998765, -19.04799999998846], 0.04: [30.500000000009777, 86.64599999998681, 58.151999999975445, 52.152999999981745, -19.85499999999123], 0.06: [63.32299999998191, 37.18800000001513, 29.48800000000659, 28.263000000010337, 67.21499999998875], 0.08: [15.1250000000028, -20.151999999984895, 33.181000000017185, -27.187999999977816, 57.19100000000085], 0.1: [37.174000000014566, -12.823999999994797, 42.7380000000109, -26.076999999975875, 0.05799999999907945], 0.12: [5.028999999999612, -4.818000000006438, 44.66100000000657, -15.692999999992855, -12.7629999999987], 0.14: [-4.286000000006214, -36.979999999975455, 33.3140000000109, -12.783999999998917, -38.062999999969776], 0.16: [-5.905000000005804, -48.58000000000506, 0.1329999999986125, -29.30999999997202, -41.20299999996935], 0.18: [-22.57699999998124, -50.00000000000659, -15.778999999986004, -30.274999999971143, -48.77100000000209], 0.2: [-24.368999999984876, -44.4309999999927, -13.191999999987273, -22.018999999981975, -46.53499999999239]}, 'action': {0.0: [95.91599999999549, 90.52199999998494, 94.67899999999455, 97.85399999999603, 97.66999999999571], 0.02: [95.73999999999303, -6.730000000003295, 61.93399999997664, 83.07999999998405, -40.59799999997038], 0.04: [10.925000000003136, 16.590000000003414, 40.40200000002496, 71.01799999996561, -23.052999999979292], 0.06: [8.394999999994972, -15.765999999999446, 0.49199999999752436, 16.120000000003753, -38.874999999968495], 0.08: [-35.29499999997204, -24.8559999999801, -26.045999999975475, -24.368999999981785, -39.45799999997279], 0.1: [-32.295999999973446, -43.37199999997906, -40.361999999968106, -35.299999999971575, -48.8930000000062], 0.12: [-46.13599999999286, -47.315999999995, -47.3429999999951, -29.20999999997757, -50.00000000000659], 0.14: [-30.949999999975486, -47.73699999999747, -50.00000000000659, -44.570999999982284, -48.96100000000275], 0.16: [-41.38599999997002, -43.93299999998027, -50.00000000000659, -50.00000000000659, -48.97100000000193], 0.18: [-33.82399999997483, -50.00000000000659, -50.00000000000659, -48.787000000001264, -50.00000000000659], 0.2: [-47.47100000000125, -48.93300000000179, -50.00000000000659, -48.90500000000169, -45.82999999998778]}}\n","----\n","fgsm (ut): {'goal': {0.0: [96.15899999999641, 91.9079999999858, 94.67499999999218, 97.86699999999608, 97.65899999999573], 0.02: [22.258000000005772, 97.70799999999578, 82.57699999999414, 94.0879999999894, -21.99999999998589], 0.04: [48.803000000003514, 89.92299999998816, 53.64399999999787, 72.11799999997588, -5.254000000005386], 0.06: [64.83999999999416, 31.98200000001243, 44.42200000000861, 39.02700000002122, 61.15099999997613], 0.08: [14.94700000000817, -20.37099999999178, 28.263000000010674, -27.550999999972436, 65.13399999999675], 0.1: [46.48200000000523, -2.097000000001648, 32.91900000001815, -19.259999999989727, -15.061999999988837], 0.12: [4.9959999999991, -16.9359999999941, 42.30700000000571, -12.007999999980289, -18.631999999985087], 0.14: [-7.201000000001517, -43.02399999998121, 26.12500000000593, -22.571999999983436, -43.09399999997767], 0.16: [-16.61299999999516, -48.624000000001544, -4.625000000006548, -25.66999999997782, -46.45099999999095], 0.18: [-24.100999999981983, -47.253999999994775, -25.398999999974123, -26.83099999997652, -47.48499999999647], 0.2: [-21.438999999979533, -45.94199999999023, -7.293000000006254, -33.91599999997448, -47.59899999999603]}, 'action': {0.0: [97.2849999999957, 89.3059999999852, 91.74799999999516, 97.87099999999604, 97.64899999999562], 0.02: [98.0429999999964, -6.530000000006604, 85.0189999999916, 89.86599999998408, -5.27800000000227], 0.04: [87.3519999999903, 5.311999999994893, 65.32899999997043, 64.31899999997071, 80.98299999998702], 0.06: [-5.521000000003792, 1.99999999999807, 20.119000000012488, 53.93299999999821, 93.21899999998875], 0.08: [4.3989999999986855, -16.74199999999219, 27.41100000001276, 70.41599999997378, 79.37199999998403], 0.1: [4.115999999999144, -25.659999999974893, 21.62900000001421, 32.40700000001763, 17.124000000009236], 0.12: [9.755999999994508, -41.45299999997117, 21.477000000011188, 15.397999999998824, 9.83799999999513], 0.14: [20.298000000015016, -42.82399999997682, 9.807000000013735, -9.200000000005327, -5.153000000006914], 0.16: [20.649000000010574, -48.56200000000499, -18.67099999998399, -12.49099999999944, -13.254000000000808], 0.18: [-4.505000000008063, -48.73500000000107, -31.628999999972763, -17.40999999997783, -18.868999999986105], 0.2: [-14.637999999998437, -45.24299999998668, -34.6459999999745, -24.72299999998036, -30.69899999997511]}}\n","fgsm (t): {'goal': {0.0: [97.17799999999487, 91.00099999998632, 93.18799999999224, 97.86599999999608, 97.68299999999579], 0.02: [38.48200000001634, 97.69799999999577, 82.52799999998882, 93.20599999998765, -19.04799999998846], 0.04: [30.500000000009777, 86.64599999998681, 58.151999999975445, 52.152999999981745, -19.85499999999123], 0.06: [63.32299999998191, 37.18800000001513, 29.48800000000659, 28.263000000010337, 67.21499999998875], 0.08: [15.1250000000028, -20.151999999984895, 33.181000000017185, -27.187999999977816, 57.19100000000085], 0.1: [37.174000000014566, -12.823999999994797, 42.7380000000109, -26.076999999975875, 0.05799999999907945], 0.12: [5.028999999999612, -4.818000000006438, 44.66100000000657, -15.692999999992855, -12.7629999999987], 0.14: [-4.286000000006214, -36.979999999975455, 33.3140000000109, -12.783999999998917, -38.062999999969776], 0.16: [-5.905000000005804, -48.58000000000506, 0.1329999999986125, -29.30999999997202, -41.20299999996935], 0.18: [-22.57699999998124, -50.00000000000659, -15.778999999986004, -30.274999999971143, -48.77100000000209], 0.2: [-24.368999999984876, -44.4309999999927, -13.191999999987273, -22.018999999981975, -46.53499999999239]}, 'action': {0.0: [95.91599999999549, 90.52199999998494, 94.67899999999455, 97.85399999999603, 97.66999999999571], 0.02: [95.73999999999303, -6.730000000003295, 61.93399999997664, 83.07999999998405, -40.59799999997038], 0.04: [10.925000000003136, 16.590000000003414, 40.40200000002496, 71.01799999996561, -23.052999999979292], 0.06: [8.394999999994972, -15.765999999999446, 0.49199999999752436, 16.120000000003753, -38.874999999968495], 0.08: [-35.29499999997204, -24.8559999999801, -26.045999999975475, -24.368999999981785, -39.45799999997279], 0.1: [-32.295999999973446, -43.37199999997906, -40.361999999968106, -35.299999999971575, -48.8930000000062], 0.12: [-46.13599999999286, -47.315999999995, -47.3429999999951, -29.20999999997757, -50.00000000000659], 0.14: [-30.949999999975486, -47.73699999999747, -50.00000000000659, -44.570999999982284, -48.96100000000275], 0.16: [-41.38599999997002, -43.93299999998027, -50.00000000000659, -50.00000000000659, -48.97100000000193], 0.18: [-33.82399999997483, -50.00000000000659, -50.00000000000659, -48.787000000001264, -50.00000000000659], 0.2: [-47.47100000000125, -48.93300000000179, -50.00000000000659, -48.90500000000169, -45.82999999998778]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c-fYUkxf5QF1"},"source":["Solved after 2564 episodes!\n","0 noise_hrl: {'both': {0.0: [94.51999999999411], 0.05: [77.73399999999089], 0.1: [62.49999999999135], 0.15: [64.97399999999148], 0.2: [48.31499999999807], 0.25: [12.52299999999834], 0.3: [-0.6109999999991379], 0.35: [-13.03800000000245], 0.4: [-19.677999999987172], 0.45: [-22.515999999976557], 0.5: [-23.790999999986585]}, 'action_only': {0.0: [96.03099999999391], 0.05: [77.5119999999846], 0.1: [73.84899999997789], 0.15: [54.39599999998985], 0.2: [44.23100000000369], 0.25: [32.61500000002243], 0.3: [2.1159999999987322], 0.35: [-14.945000000000137], 0.4: [-27.69299999997554], 0.45: [-25.040999999978858], 0.5: [-27.306999999979066]}, 'goal_only': {0.0: [97.48999999999606], 0.05: [91.30599999999794], 0.1: [81.75099999999196], 0.15: [71.89499999997756], 0.2: [81.89899999998096], 0.25: [76.84999999998966], 0.3: [77.25799999998907], 0.35: [73.86299999997289], 0.4: [74.97599999998678], 0.45: [72.03799999997823], 0.5: [69.99199999998356]}, 'both_same': {0.0: [95.85899999999512], 0.05: [69.76699999998402], 0.1: [55.15299999999188], 0.15: [48.93500000000657], 0.2: [31.406000000013943], 0.25: [13.855999999995424], 0.3: [15.421000000000479], 0.35: [7.552000000005685], 0.4: [-10.321000000004597], 0.45: [-19.07799999998664], 0.5: [-13.299999999990808]}}\n","6 noise_hrl: {'both': {0.0: [87.86099999998967, 94.68799999999457, -0.7399999999995339, 97.85799999999603, 96.187999999994, 40.563000000018434], 0.05: [18.52800000000326, 85.08399999998132, 3.9380000000035262, 85.43899999998497, 56.00399999998277, 75.66299999997736], 0.1: [8.384999999997586, 79.18999999998522, -4.991000000002036, 73.28199999998044, 52.31899999998814, 52.647999999994035], 0.15: [2.2269999999981445, 78.49499999998652, 10.846999999995345, 62.248999999976874, 52.656999999981544, 24.558000000016197], 0.2: [-8.079000000006069, 81.55699999998232, -7.719999999999308, 53.37499999999752, 47.77500000000174, 13.27000000000003], 0.25: [-29.808999999977168, 65.760999999974, -13.81799999999499, 28.01900000001038, 32.07700000002334, 11.439000000009614], 0.3: [-26.73699999997784, 45.11800000001087, -29.160999999976315, 12.801000000005542, 27.57900000001563, 8.894999999993901], 0.35: [-31.33599999997336, 10.566999999991904, -33.420999999973475, 1.9070000000003509, 20.259000000014645, 10.676000000005661], 0.4: [-35.671999999969586, 2.198999999995465, -32.81299999997248, 0.3749999999995863, 6.655999999996588, -4.912999999999815], 0.45: [-37.993999999971855, -0.8820000000009628, -36.8719999999737, -16.232999999983637, 13.763000000003421, -3.557000000003212], 0.5: [-41.08099999996774, -0.8850000000001097, -26.623999999973506, -18.87699999998581, -4.220000000005752, -14.017000000000246]}, 'action_only': {0.0: [92.34099999998766, 94.68999999999454, 1.9210000000001626, 97.86199999999606, 97.66399999999571, 37.643000000021466], 0.05: [10.818999999998814, 83.88699999998595, 4.689999999996502, 93.28199999999353, 57.04399999997311, 75.21099999998277], 0.1: [11.324000000007302, 79.42199999998529, 5.91499999999706, 90.39899999998755, 61.019999999989295, 43.70300000001355], 0.15: [7.674999999993611, 73.54199999998376, 4.018999999995917, 65.47399999997893, 48.516999999992976, 25.56000000001888], 0.2: [-2.761000000001606, 76.35499999997819, 17.087000000018623, 39.38200000001894, 52.45199999999775, 10.282999999994331], 0.25: [-17.065999999985884, 69.30099999998907, -3.8460000000022068, 5.648999999998402, 26.549000000013567, 3.9279999999953827], 0.3: [-14.445999999990159, 44.63800000001339, -14.41799999998965, -7.584000000005559, 5.600999999999053, -16.609999999998426], 0.35: [-34.75299999997355, 26.645000000014583, -23.104999999978126, -24.01899999997655, 5.521999999996938, -19.320999999989727], 0.4: [-43.2259999999774, 6.745999999993341, -37.65999999997381, -24.269999999979053, -21.434999999990758, -20.17099999998139], 0.45: [-40.476999999971184, 2.9919999999979634, -41.51099999997138, -22.635999999977116, -31.249999999979313, -29.529999999974816], 0.5: [-44.65999999998488, -8.47800000000474, -32.58699999997404, -28.638999999971407, -27.55699999997569, -32.73199999997101]}, 'goal_only': {0.0: [92.57099999998736, 97.63699999999564, 3.830999999998346, 97.85799999999607, 97.67399999999571, 37.471000000023814], 0.05: [89.137999999989, 96.1629999999963, 20.71400000000297, 89.68599999998553, 97.68699999999579, 66.9649999999773], 0.1: [86.4759999999864, 96.08899999999618, 50.04199999999733, 88.99399999999258, 97.61999999999561, 72.17499999997892], 0.15: [68.9559999999841, 94.50599999999659, 39.52300000001755, 93.9059999999913, 91.68299999999104, 67.66399999997721], 0.2: [51.01599999999133, 97.09799999999494, 53.64999999999424, 96.0809999999939, 85.59399999999269, 59.779999999978514], 0.25: [34.939000000014445, 89.48199999999594, 45.37100000000683, 96.95699999999391, 80.26899999998587, 58.64900000000308], 0.3: [40.4490000000168, 91.4759999999881, 40.285000000009305, 93.58799999998955, 66.79399999997987, 66.85199999999126], 0.35: [32.03000000001023, 96.23199999999356, 50.15900000000849, 93.81999999998936, 80.16999999998733, 79.13199999998966], 0.4: [49.59699999998716, 96.07899999999302, 43.88200000000962, 94.16599999999129, 82.3799999999879, 83.93599999998241], 0.45: [45.65700000000616, 95.39599999999102, 51.323999999994605, 93.47799999998813, 82.38399999998292, 79.6169999999791], 0.5: [45.86500000000504, 95.1909999999909, 48.0640000000039, 90.40899999998601, 87.68699999998445, 73.87399999997689]}, 'both_same': {0.0: [92.49999999998747, 96.1499999999939, 5.644999999995011, 97.85999999999603, 97.66499999999569, 25.14800000001758], 0.05: [36.53200000001549, 87.51499999999245, 12.757000000001488, 91.37199999998839, 53.85700000000113, 73.98599999997374], 0.1: [11.3809999999961, 84.79299999998233, 26.19600000001604, 76.96799999997437, 63.12599999998738, 53.82400000000198], 0.15: [-0.296000000001824, 78.49899999998844, 13.524000000003799, 62.940999999978, 49.22899999999972, 30.89500000001775], 0.2: [-20.264999999992277, 81.39899999998056, -7.880000000003701, 49.380999999997414, 48.41600000000588, 12.589000000006038], 0.25: [-26.395999999981893, 65.74799999998028, -12.696999999994969, 46.444999999999276, 30.243000000013318, 17.603000000009658], 0.3: [-28.698999999971665, 51.077999999995754, -12.946999999997281, 23.95900000001548, 24.562000000020248, 3.2049999999963346], 0.35: [-30.66799999997565, 36.45400000001939, -25.814999999977413, 23.235000000017063, 20.59900000001025, -12.154999999994521], 0.4: [-27.3279999999785, 12.342999999997224, -29.078999999972797, -1.5610000000018076, 23.22500000001262, -4.952000000004371], 0.45: [-31.828999999972343, 4.2679999999944656, -34.478999999969574, -5.965000000006495, 15.30500000000193, -13.189999999997621], 0.5: [-33.30299999997497, -16.213999999988467, -18.493999999987388, 4.618999999995867, 4.9009999999967935, -15.755999999992827]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdQEb3LDjtxd"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointFallEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHVoTScvj8Ru","executionInfo":{"status":"ok","timestamp":1617265780029,"user_tz":-60,"elapsed":428881,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"36318ffb-78bf-49cf-cbc3-0cb584ef4ad3"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    if i == 3:\n","        i += 1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [66.95999999999025], 1.5: [55.811999999996836], 2.0: [98.59399999999691], 2.5: [92.1249999999889], 3.0: [95.14899999999675], 3.5: [97.99699999999622], 4.0: [97.6919999999959], 4.5: [96.70099999999444], 5.0: [94.73899999999036], 5.5: [96.84599999999456], 6.0: [96.70699999999422], 6.5: [95.90199999999327], 7.0: [95.38299999999188]}\n","1 scale: {1.0: [66.95999999999025, 83.9039999999899], 1.5: [55.811999999996836, 7.234999999996528], 2.0: [98.59399999999691, 79.95899999998268], 2.5: [92.1249999999889, 92.03499999999286], 3.0: [95.14899999999675, 98.08899999999632], 3.5: [97.99699999999622, 94.52299999999197], 4.0: [97.6919999999959, 92.5209999999868], 4.5: [96.70099999999444, 91.94599999998684], 5.0: [94.73899999999036, 91.08099999998532], 5.5: [96.84599999999456, 86.04699999998392], 6.0: [96.70699999999422, 59.848999999979796], 6.5: [95.90199999999327, 55.799999999985815], 7.0: [95.38299999999188, 44.612999999998074]}\n","2 scale: {1.0: [66.95999999999025, 83.9039999999899, -11.30399999999676], 1.5: [55.811999999996836, 7.234999999996528, 57.71299999998654], 2.0: [98.59399999999691, 79.95899999998268, 65.90499999997928], 2.5: [92.1249999999889, 92.03499999999286, 77.85699999997607], 3.0: [95.14899999999675, 98.08899999999632, 88.91999999999149], 3.5: [97.99699999999622, 94.52299999999197, 87.56399999999265], 4.0: [97.6919999999959, 92.5209999999868, 94.6769999999963], 4.5: [96.70099999999444, 91.94599999998684, 97.47899999999547], 5.0: [94.73899999999036, 91.08099999998532, 95.77899999999335], 5.5: [96.84599999999456, 86.04699999998392, 97.02299999999462], 6.0: [96.70699999999422, 59.848999999979796, 96.64799999999394], 6.5: [95.90199999999327, 55.799999999985815, 96.35799999999348], 7.0: [95.38299999999188, 44.612999999998074, 96.13299999999307]}\n","4 scale: {1.0: [66.95999999999025, 83.9039999999899, -11.30399999999676, 81.12999999996785], 1.5: [55.811999999996836, 7.234999999996528, 57.71299999998654, 58.977999999976426], 2.0: [98.59399999999691, 79.95899999998268, 65.90499999997928, 97.47299999999545], 2.5: [92.1249999999889, 92.03499999999286, 77.85699999997607, 98.29399999999697], 3.0: [95.14899999999675, 98.08899999999632, 88.91999999999149, 98.21099999999669], 3.5: [97.99699999999622, 94.52299999999197, 87.56399999999265, 98.0509999999964], 4.0: [97.6919999999959, 92.5209999999868, 94.6769999999963, 97.86399999999603], 4.5: [96.70099999999444, 91.94599999998684, 97.47899999999547, 97.63399999999565], 5.0: [94.73899999999036, 91.08099999998532, 95.77899999999335, 97.41899999999538], 5.5: [96.84599999999456, 86.04699999998392, 97.02299999999462, 97.21199999999497], 6.0: [96.70699999999422, 59.848999999979796, 96.64799999999394, 96.97899999999457], 6.5: [95.90199999999327, 55.799999999985815, 96.35799999999348, 96.68499999999408], 7.0: [95.38299999999188, 44.612999999998074, 96.13299999999307, 96.47499999999361]}\n","5 scale: {1.0: [66.95999999999025, 83.9039999999899, -11.30399999999676, 81.12999999996785, 15.02800000000779], 1.5: [55.811999999996836, 7.234999999996528, 57.71299999998654, 58.977999999976426, 59.84999999998756], 2.0: [98.59399999999691, 79.95899999998268, 65.90499999997928, 97.47299999999545, 98.5199999999973], 2.5: [92.1249999999889, 92.03499999999286, 77.85699999997607, 98.29399999999697, 98.37799999999703], 3.0: [95.14899999999675, 98.08899999999632, 88.91999999999149, 98.21099999999669, 98.17699999999667], 3.5: [97.99699999999622, 94.52299999999197, 87.56399999999265, 98.0509999999964, 97.96099999999623], 4.0: [97.6919999999959, 92.5209999999868, 94.6769999999963, 97.86399999999603, 96.21799999999405], 4.5: [96.70099999999444, 91.94599999998684, 97.47899999999547, 97.63399999999565, 97.36699999999526], 5.0: [94.73899999999036, 91.08099999998532, 95.77899999999335, 97.41899999999538, 97.1639999999949], 5.5: [96.84599999999456, 86.04699999998392, 97.02299999999462, 97.21199999999497, 96.92199999999451], 6.0: [96.70699999999422, 59.848999999979796, 96.64799999999394, 96.97899999999457, 96.65799999999399], 6.5: [95.90199999999327, 55.799999999985815, 96.35799999999348, 96.68499999999408, 96.36699999999345], 7.0: [95.38299999999188, 44.612999999998074, 96.13299999999307, 96.47499999999361, 96.08199999999297]}\n","----\n","scale: {1.0: [66.95999999999025, 83.9039999999899, -11.30399999999676, 81.12999999996785, 15.02800000000779], 1.5: [55.811999999996836, 7.234999999996528, 57.71299999998654, 58.977999999976426, 59.84999999998756], 2.0: [98.59399999999691, 79.95899999998268, 65.90499999997928, 97.47299999999545, 98.5199999999973], 2.5: [92.1249999999889, 92.03499999999286, 77.85699999997607, 98.29399999999697, 98.37799999999703], 3.0: [95.14899999999675, 98.08899999999632, 88.91999999999149, 98.21099999999669, 98.17699999999667], 3.5: [97.99699999999622, 94.52299999999197, 87.56399999999265, 98.0509999999964, 97.96099999999623], 4.0: [97.6919999999959, 92.5209999999868, 94.6769999999963, 97.86399999999603, 96.21799999999405], 4.5: [96.70099999999444, 91.94599999998684, 97.47899999999547, 97.63399999999565, 97.36699999999526], 5.0: [94.73899999999036, 91.08099999998532, 95.77899999999335, 97.41899999999538, 97.1639999999949], 5.5: [96.84599999999456, 86.04699999998392, 97.02299999999462, 97.21199999999497, 96.92199999999451], 6.0: [96.70699999999422, 59.848999999979796, 96.64799999999394, 96.97899999999457, 96.65799999999399], 6.5: [95.90199999999327, 55.799999999985815, 96.35799999999348, 96.68499999999408, 96.36699999999345], 7.0: [95.38299999999188, 44.612999999998074, 96.13299999999307, 96.47499999999361, 96.08199999999297]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCngiUhykCBW"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VRa-o8mkMvW","executionInfo":{"status":"ok","timestamp":1617265351115,"user_tz":-60,"elapsed":400902,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"dc87fc9b-8dcb-4faf-b47b-0acbb61f4cee"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    if i == 3:\n","        i += 1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [97.39399999999534], 0.05: [91.5309999999883], 0.1: [84.64699999998994], 0.15: [72.26799999998433], 0.2: [77.86399999998682], 0.25: [54.931999999983994], 0.3: [63.21199999998873], 0.35: [63.46400000000179], 0.4: [47.96500000000046]}\n","1 range: {0.0: [97.39399999999534, 92.41799999998757], 0.05: [91.5309999999883, 94.19399999998937], 0.1: [84.64699999998994, 83.77399999998704], 0.15: [72.26799999998433, 79.78199999998208], 0.2: [77.86399999998682, 77.18399999998209], 0.25: [54.931999999983994, 53.83399999998829], 0.3: [63.21199999998873, 46.61000000000517], 0.35: [63.46400000000179, 32.51000000001554], 0.4: [47.96500000000046, 27.219000000018426]}\n","2 range: {0.0: [97.39399999999534, 92.41799999998757, 93.21299999999279], 0.05: [91.5309999999883, 94.19399999998937, 90.24999999998934], 0.1: [84.64699999998994, 83.77399999998704, 93.18799999999278], 0.15: [72.26799999998433, 79.78199999998208, 88.74199999999351], 0.2: [77.86399999998682, 77.18399999998209, 88.24199999999252], 0.25: [54.931999999983994, 53.83399999998829, 88.73699999998982], 0.3: [63.21199999998873, 46.61000000000517, 94.38799999999186], 0.35: [63.46400000000179, 32.51000000001554, 91.6779999999911], 0.4: [47.96500000000046, 27.219000000018426, 94.63199999999442]}\n","4 range: {0.0: [97.39399999999534, 92.41799999998757, 93.21299999999279, 97.86699999999605], 0.05: [91.5309999999883, 94.19399999998937, 90.24999999998934, 97.85399999999603], 0.1: [84.64699999998994, 83.77399999998704, 93.18799999999278, 97.85599999999604], 0.15: [72.26799999998433, 79.78199999998208, 88.74199999999351, 94.78199999999502], 0.2: [77.86399999998682, 77.18399999998209, 88.24199999999252, 93.6589999999917], 0.25: [54.931999999983994, 53.83399999998829, 88.73699999998982, 92.5179999999938], 0.3: [63.21199999998873, 46.61000000000517, 94.38799999999186, 86.26999999998263], 0.35: [63.46400000000179, 32.51000000001554, 91.6779999999911, 87.7419999999881], 0.4: [47.96500000000046, 27.219000000018426, 94.63199999999442, 78.37499999999127]}\n","5 range: {0.0: [97.39399999999534, 92.41799999998757, 93.21299999999279, 97.86699999999605, 97.64499999999563], 0.05: [91.5309999999883, 94.19399999998937, 90.24999999998934, 97.85399999999603, 94.6989999999922], 0.1: [84.64699999998994, 83.77399999998704, 93.18799999999278, 97.85599999999604, 88.8099999999899], 0.15: [72.26799999998433, 79.78199999998208, 88.74199999999351, 94.78199999999502, 69.61399999998972], 0.2: [77.86399999998682, 77.18399999998209, 88.24199999999252, 93.6589999999917, 57.76599999999569], 0.25: [54.931999999983994, 53.83399999998829, 88.73699999998982, 92.5179999999938, 67.90399999998083], 0.3: [63.21199999998873, 46.61000000000517, 94.38799999999186, 86.26999999998263, 48.72699999999958], 0.35: [63.46400000000179, 32.51000000001554, 91.6779999999911, 87.7419999999881, 62.15200000000238], 0.4: [47.96500000000046, 27.219000000018426, 94.63199999999442, 78.37499999999127, 45.75000000000267]}\n","----\n","range: {0.0: [97.39399999999534, 92.41799999998757, 93.21299999999279, 97.86699999999605, 97.64499999999563], 0.05: [91.5309999999883, 94.19399999998937, 90.24999999998934, 97.85399999999603, 94.6989999999922], 0.1: [84.64699999998994, 83.77399999998704, 93.18799999999278, 97.85599999999604, 88.8099999999899], 0.15: [72.26799999998433, 79.78199999998208, 88.74199999999351, 94.78199999999502, 69.61399999998972], 0.2: [77.86399999998682, 77.18399999998209, 88.24199999999252, 93.6589999999917, 57.76599999999569], 0.25: [54.931999999983994, 53.83399999998829, 88.73699999998982, 92.5179999999938, 67.90399999998083], 0.3: [63.21199999998873, 46.61000000000517, 94.38799999999186, 86.26999999998263, 48.72699999999958], 0.35: [63.46400000000179, 32.51000000001554, 91.6779999999911, 87.7419999999881, 62.15200000000238], 0.4: [47.96500000000046, 27.219000000018426, 94.63199999999442, 78.37499999999127, 45.75000000000267]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fXVuyJCHOES6"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal_raw = agent.select_goal(g_state, False, False)\n","            goal = agent.convert_goal(goal_raw)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmuu11RLSSo1","executionInfo":{"status":"ok","timestamp":1612798729883,"user_tz":-60,"elapsed":63303,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"d32abe58-f333-4cba-d4c1-d7b0a59209b7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","torch.save(episodes, \"PointFall_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RqCEeJ2WSZ3F"},"source":[""],"execution_count":null,"outputs":[]}]}