{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointpush_pheromone.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1618988201650,"user_tz":-60,"elapsed":44804,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"37e57a70-209c-44f2-f878-b17bb3abbb6a"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_push.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1618988201653,"user_tz":-60,"elapsed":2936,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push_pheromone/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.pheromone_paths,\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push_pheromone/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.pheromone_paths = copy.deepcopy(checkpoint['meta_controller'])\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1618988203409,"user_tz":-60,"elapsed":4302,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1618988203429,"user_tz":-60,"elapsed":3937,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1618988203432,"user_tz":-60,"elapsed":3609,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_push import PointPushEnv \n","env = NormalizedEnv(PointPushEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1618988203435,"user_tz":-60,"elapsed":3108,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def plot_durations(episode_durations, goals_done):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*goals_done)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Goals done')\n","    \n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1618988203438,"user_tz":-60,"elapsed":2808,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1618988203441,"user_tz":-60,"elapsed":2653,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1618988203445,"user_tz":-60,"elapsed":2310,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1618988203448,"user_tz":-60,"elapsed":1486,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1618988203781,"user_tz":-60,"elapsed":1428,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 10000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1618988203783,"user_tz":-60,"elapsed":1030,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from operator import itemgetter\n","class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","\n","        # a list of tuple of form (reward, path); keep top 5\n","        self.pheromone_paths = []\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def add_path(self, reward, path):\n","        # prefer higher-reward paths\n","        self.pheromone_paths.append((reward, path))\n","        self.pheromone_paths.sort(key=itemgetter(0), reverse=True)\n","        self.pheromone_paths = self.pheromone_paths[:5] # only keep top 5\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        if warmup or len(self.pheromone_paths) == 0:\n","            return torch.tensor([np.random.uniform(-1.,1.,len(self.goal_dim))], device=device, dtype=torch.float)\n","        \n","        time_index = 3\n","        #cur_t = s_t.squeeze(0)[time_index] # time\n","        cur_pos = s_t.squeeze(0)[self.goal_dim]\n","\n","        goal = torch.tensor([0] * len(self.goal_dim), device=device, dtype=torch.float)\n","\n","        min_rew = -60 # min(self.pheromone_paths, key = lambda t: t[0])[0]\n","        tot_rew = sum([t[0] for t in self.pheromone_paths]) - len(self.pheromone_paths) * min_rew\n","\n","        for rew, path in self.pheromone_paths:\n","            breakdown = tuple(map(torch.stack, zip(*path)))\n","            positions = torch.stack([breakdown[i] for i in self.goal_dim], axis=-1)\n","            chosen_i = torch.argmin(torch.pow(torch.sum(torch.pow(positions - cur_pos, 2), axis=1), 0.5))\n","            \n","            # assume c = 10\n","            # basically, in chosen path, go 10 steps ahead from position closest\n","            # to the currently observed one\n","            chosen_point = path[min(chosen_i + 10, len(path) - 1)]\n","\n","            #chosen_point = path[torch.argmin(torch.abs(breakdown[time_index] - cur_t))]\n","            goal += (rew - min_rew) * chosen_point[self.goal_dim]\n","        \n","        goal /= tot_rew\n","        goal = goal - s_t.squeeze(0)[self.goal_dim] # make goal relative to given position\n","\n","        return goal.unsqueeze(0)\n","\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 5\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        state_seq = None\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        # once episode finishes, append full path to manager\n","        agent.add_path(overall_reward, state_seq)\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1618988214459,"user_tz":-60,"elapsed":7041,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data - eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, targeted):\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            goal = agent.select_goal(og_state, False, False)\n","            state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    goal_temp = agent.h(state, goal, next_og_state)\n","                    next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od4IvIuPuNlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618991715763,"user_tz":-60,"elapsed":3492297,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"97e3e8ea-d243-4af9-edea-7083af7a1145"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","#i = 2\n","#while i < 3:\n","#    #agent = train_model()\n","#    agent = HIRO(n_observations, n_actions).to(device)\n","#    load_model(agent, f\"hiro_{i}\")\n","\n","#    if agent is not None:\n","#        # goal_attack, action_attack, same_noise\n","#        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","#        eval_model(agent, noise_hrl['both'], True, True, False)\n","#        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","#        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","#        print(f\"{i} noise_hrl: {noise_hrl}\")\n","#        i += 1\n","\n","#print(\"----\")\n","#print(f\"noise_hrl: {noise_hrl}\")\n","\n","untargeted = {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.18899999999152, 89.67999999998972, 75.64099999997917, 83.00299999998151, 80.18599999997733, 93.33199999998824], 0.02: [86.62999999998618, 81.11999999998467, -23.037999999984013, 22.224000000013348, 72.01799999998593, 34.57400000001203], 0.04: [-0.5440000000040107, 34.67400000001282, -45.76999999998847, 28.868000000020658, 13.01500000000504, -35.68199999997282], 0.06: [-0.2730000000045658, -37.04599999996998, -44.488999999982894, -42.93199999997945, -50.00000000000659, -50.00000000000659], 0.08: [-24.462999999975608, -50.00000000000659, -42.8689999999761, -47.213000000000086, -50.00000000000659, -50.00000000000659], 0.1: [-40.32199999997019, -50.00000000000659, -36.154999999971665, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.12: [-11.535999999993159, -50.00000000000659, -44.45599999998733, -48.56800000000047, -50.00000000000659, -48.541000000000366], 0.14: [-26.428999999979414, -50.00000000000659, -38.88099999997294, -50.00000000000659, -50.00000000000659, -40.08599999996813], 0.16: [-12.006999999999305, -50.00000000000659, -41.53699999997295, -50.00000000000659, -50.00000000000659, -41.614999999973364], 0.18: [-12.700999999982523, -50.00000000000659, -45.79099999998854, -47.80699999999771, -50.00000000000659, -44.75899999998388], 0.2: [-3.957000000004028, -47.61999999999725, -36.6349999999719, -47.80900000000134, -50.00000000000659, -46.140999999989816]}}\n","targeted = {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.33599999999153, 88.06399999998811, 80.60999999998089, 83.96199999997744, 76.57599999997275], 0.02: [80.5379999999728, 74.82499999997725, 50.530999999998684, 76.39099999997752, 27.687000000005913], 0.04: [-1.5880000000029943, 36.98700000001531, 18.739000000014535, 42.937000000014834, 13.194000000007344], 0.06: [-24.78999999997478, 16.978000000021506, -15.409999999997668, 8.34600000000472, -27.826999999976724], 0.08: [-43.63299999997973, -25.104999999983782, -42.2119999999728, -33.23599999997118, -30.40699999997626], 0.1: [-38.40399999997239, -30.274999999971826, -39.186999999971114, -42.093999999976916, -23.436999999980834], 0.12: [-47.507999999996834, -30.80699999997619, -45.98399999999152, -44.54499999998434, -42.20499999997394], 0.14: [-41.70599999997346, -36.409999999969884, -42.16099999997553, -50.00000000000659, -43.20099999998036], 0.16: [-43.766999999982275, -36.17499999997299, -50.00000000000659, -50.00000000000659, -32.940999999974856], 0.18: [-50.00000000000659, -40.394999999970125, -50.00000000000659, -50.00000000000659, -27.72099999997675], 0.2: [-47.41699999999537, -47.23700000000381, -50.00000000000659, -48.70400000000096, -7.892000000002875]}}\n","\n","i = 5\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        if i != 5:\n","            apply_fgsm(agent, untargeted['action'], False)   \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['action'], True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["5 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.18899999999152, 89.67999999998972, 75.64099999997917, 83.00299999998151, 80.18599999997733, 93.33199999998824], 0.02: [86.62999999998618, 81.11999999998467, -23.037999999984013, 22.224000000013348, 72.01799999998593, 34.57400000001203], 0.04: [-0.5440000000040107, 34.67400000001282, -45.76999999998847, 28.868000000020658, 13.01500000000504, -35.68199999997282], 0.06: [-0.2730000000045658, -37.04599999996998, -44.488999999982894, -42.93199999997945, -50.00000000000659, -50.00000000000659], 0.08: [-24.462999999975608, -50.00000000000659, -42.8689999999761, -47.213000000000086, -50.00000000000659, -50.00000000000659], 0.1: [-40.32199999997019, -50.00000000000659, -36.154999999971665, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.12: [-11.535999999993159, -50.00000000000659, -44.45599999998733, -48.56800000000047, -50.00000000000659, -48.541000000000366], 0.14: [-26.428999999979414, -50.00000000000659, -38.88099999997294, -50.00000000000659, -50.00000000000659, -40.08599999996813], 0.16: [-12.006999999999305, -50.00000000000659, -41.53699999997295, -50.00000000000659, -50.00000000000659, -41.614999999973364], 0.18: [-12.700999999982523, -50.00000000000659, -45.79099999998854, -47.80699999999771, -50.00000000000659, -44.75899999998388], 0.2: [-3.957000000004028, -47.61999999999725, -36.6349999999719, -47.80900000000134, -50.00000000000659, -46.140999999989816]}}\n","5 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.33599999999153, 88.06399999998811, 80.60999999998089, 83.96199999997744, 76.57599999997275, 93.39999999998854], 0.02: [80.5379999999728, 74.82499999997725, 50.530999999998684, 76.39099999997752, 27.687000000005913, 10.245999999998846], 0.04: [-1.5880000000029943, 36.98700000001531, 18.739000000014535, 42.937000000014834, 13.194000000007344, -32.87299999997373], 0.06: [-24.78999999997478, 16.978000000021506, -15.409999999997668, 8.34600000000472, -27.826999999976724, -48.601000000005136], 0.08: [-43.63299999997973, -25.104999999983782, -42.2119999999728, -33.23599999997118, -30.40699999997626, -48.86600000000155], 0.1: [-38.40399999997239, -30.274999999971826, -39.186999999971114, -42.093999999976916, -23.436999999980834, -50.00000000000659], 0.12: [-47.507999999996834, -30.80699999997619, -45.98399999999152, -44.54499999998434, -42.20499999997394, -50.00000000000659], 0.14: [-41.70599999997346, -36.409999999969884, -42.16099999997553, -50.00000000000659, -43.20099999998036, -48.589000000001285], 0.16: [-43.766999999982275, -36.17499999997299, -50.00000000000659, -50.00000000000659, -32.940999999974856, -50.00000000000659], 0.18: [-50.00000000000659, -40.394999999970125, -50.00000000000659, -50.00000000000659, -27.72099999997675, -50.00000000000659], 0.2: [-47.41699999999537, -47.23700000000381, -50.00000000000659, -48.70400000000096, -7.892000000002875, -50.00000000000659]}}\n","6 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.18899999999152, 89.67999999998972, 75.64099999997917, 83.00299999998151, 80.18599999997733, 93.33199999998824, 95.62799999999302], 0.02: [86.62999999998618, 81.11999999998467, -23.037999999984013, 22.224000000013348, 72.01799999998593, 34.57400000001203, 18.10200000001153], 0.04: [-0.5440000000040107, 34.67400000001282, -45.76999999998847, 28.868000000020658, 13.01500000000504, -35.68199999997282, -6.199000000001254], 0.06: [-0.2730000000045658, -37.04599999996998, -44.488999999982894, -42.93199999997945, -50.00000000000659, -50.00000000000659, -24.83299999997721], 0.08: [-24.462999999975608, -50.00000000000659, -42.8689999999761, -47.213000000000086, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.1: [-40.32199999997019, -50.00000000000659, -36.154999999971665, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.12: [-11.535999999993159, -50.00000000000659, -44.45599999998733, -48.56800000000047, -50.00000000000659, -48.541000000000366, -50.00000000000659], 0.14: [-26.428999999979414, -50.00000000000659, -38.88099999997294, -50.00000000000659, -50.00000000000659, -40.08599999996813, -50.00000000000659], 0.16: [-12.006999999999305, -50.00000000000659, -41.53699999997295, -50.00000000000659, -50.00000000000659, -41.614999999973364, -48.680000000000874], 0.18: [-12.700999999982523, -50.00000000000659, -45.79099999998854, -47.80699999999771, -50.00000000000659, -44.75899999998388, -50.00000000000659], 0.2: [-3.957000000004028, -47.61999999999725, -36.6349999999719, -47.80900000000134, -50.00000000000659, -46.140999999989816, -47.368000000000876]}}\n","6 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.33599999999153, 88.06399999998811, 80.60999999998089, 83.96199999997744, 76.57599999997275, 93.39999999998854, 95.76199999999206], 0.02: [80.5379999999728, 74.82499999997725, 50.530999999998684, 76.39099999997752, 27.687000000005913, 10.245999999998846, 13.99600000000639], 0.04: [-1.5880000000029943, 36.98700000001531, 18.739000000014535, 42.937000000014834, 13.194000000007344, -32.87299999997373, -9.221000000005345], 0.06: [-24.78999999997478, 16.978000000021506, -15.409999999997668, 8.34600000000472, -27.826999999976724, -48.601000000005136, -12.903999999998002], 0.08: [-43.63299999997973, -25.104999999983782, -42.2119999999728, -33.23599999997118, -30.40699999997626, -48.86600000000155, -33.25699999997636], 0.1: [-38.40399999997239, -30.274999999971826, -39.186999999971114, -42.093999999976916, -23.436999999980834, -50.00000000000659, 4.691999999995744], 0.12: [-47.507999999996834, -30.80699999997619, -45.98399999999152, -44.54499999998434, -42.20499999997394, -50.00000000000659, -15.919999999993895], 0.14: [-41.70599999997346, -36.409999999969884, -42.16099999997553, -50.00000000000659, -43.20099999998036, -48.589000000001285, -24.90499999997425], 0.16: [-43.766999999982275, -36.17499999997299, -50.00000000000659, -50.00000000000659, -32.940999999974856, -50.00000000000659, -27.33299999997377], 0.18: [-50.00000000000659, -40.394999999970125, -50.00000000000659, -50.00000000000659, -27.72099999997675, -50.00000000000659, -43.382999999983646], 0.2: [-47.41699999999537, -47.23700000000381, -50.00000000000659, -48.70400000000096, -7.892000000002875, -50.00000000000659, -45.127999999994316]}}\n","----\n","fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.18899999999152, 89.67999999998972, 75.64099999997917, 83.00299999998151, 80.18599999997733, 93.33199999998824, 95.62799999999302], 0.02: [86.62999999998618, 81.11999999998467, -23.037999999984013, 22.224000000013348, 72.01799999998593, 34.57400000001203, 18.10200000001153], 0.04: [-0.5440000000040107, 34.67400000001282, -45.76999999998847, 28.868000000020658, 13.01500000000504, -35.68199999997282, -6.199000000001254], 0.06: [-0.2730000000045658, -37.04599999996998, -44.488999999982894, -42.93199999997945, -50.00000000000659, -50.00000000000659, -24.83299999997721], 0.08: [-24.462999999975608, -50.00000000000659, -42.8689999999761, -47.213000000000086, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.1: [-40.32199999997019, -50.00000000000659, -36.154999999971665, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.12: [-11.535999999993159, -50.00000000000659, -44.45599999998733, -48.56800000000047, -50.00000000000659, -48.541000000000366, -50.00000000000659], 0.14: [-26.428999999979414, -50.00000000000659, -38.88099999997294, -50.00000000000659, -50.00000000000659, -40.08599999996813, -50.00000000000659], 0.16: [-12.006999999999305, -50.00000000000659, -41.53699999997295, -50.00000000000659, -50.00000000000659, -41.614999999973364, -48.680000000000874], 0.18: [-12.700999999982523, -50.00000000000659, -45.79099999998854, -47.80699999999771, -50.00000000000659, -44.75899999998388, -50.00000000000659], 0.2: [-3.957000000004028, -47.61999999999725, -36.6349999999719, -47.80900000000134, -50.00000000000659, -46.140999999989816, -47.368000000000876]}}\n","fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [95.33599999999153, 88.06399999998811, 80.60999999998089, 83.96199999997744, 76.57599999997275, 93.39999999998854, 95.76199999999206], 0.02: [80.5379999999728, 74.82499999997725, 50.530999999998684, 76.39099999997752, 27.687000000005913, 10.245999999998846, 13.99600000000639], 0.04: [-1.5880000000029943, 36.98700000001531, 18.739000000014535, 42.937000000014834, 13.194000000007344, -32.87299999997373, -9.221000000005345], 0.06: [-24.78999999997478, 16.978000000021506, -15.409999999997668, 8.34600000000472, -27.826999999976724, -48.601000000005136, -12.903999999998002], 0.08: [-43.63299999997973, -25.104999999983782, -42.2119999999728, -33.23599999997118, -30.40699999997626, -48.86600000000155, -33.25699999997636], 0.1: [-38.40399999997239, -30.274999999971826, -39.186999999971114, -42.093999999976916, -23.436999999980834, -50.00000000000659, 4.691999999995744], 0.12: [-47.507999999996834, -30.80699999997619, -45.98399999999152, -44.54499999998434, -42.20499999997394, -50.00000000000659, -15.919999999993895], 0.14: [-41.70599999997346, -36.409999999969884, -42.16099999997553, -50.00000000000659, -43.20099999998036, -48.589000000001285, -24.90499999997425], 0.16: [-43.766999999982275, -36.17499999997299, -50.00000000000659, -50.00000000000659, -32.940999999974856, -50.00000000000659, -27.33299999997377], 0.18: [-50.00000000000659, -40.394999999970125, -50.00000000000659, -50.00000000000659, -27.72099999997675, -50.00000000000659, -43.382999999983646], 0.2: [-47.41699999999537, -47.23700000000381, -50.00000000000659, -48.70400000000096, -7.892000000002875, -50.00000000000659, -45.127999999994316]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KsVJ9Vr0uEJX"},"source":["Solved after 1257 episodes!\n","0 noise_hrl: {'both': {0.0: [95.2909999999919], 0.05: [78.10399999997867], 0.1: [44.27800000000538], 0.15: [31.345000000021365], 0.2: [10.017999999994569], 0.25: [-5.561000000002012], 0.3: [-18.698999999982536], 0.35: [-25.83199999997743], 0.4: [-27.708999999979145], 0.45: [-22.693999999986094], 0.5: [-27.021999999978583]}, 'action_only': {0.0: [95.30199999999166], 0.05: [81.63899999998573], 0.1: [63.00299999999265], 0.15: [33.00200000000814], 0.2: [25.028000000005797], 0.25: [17.199000000011623], 0.3: [26.37000000002001], 0.35: [11.992000000000896], 0.4: [-0.4559999999992143], 0.45: [-13.065999999994723], 0.5: [-29.70299999997987]}, 'goal_only': {0.0: [95.33499999999157], 0.05: [90.04299999999445], 0.1: [73.14399999997853], 0.15: [54.07999999999349], 0.2: [25.11900000001559], 0.25: [12.64499999999871], 0.3: [8.581999999994709], 0.35: [-10.018000000003735], 0.4: [-24.129999999976963], 0.45: [-30.950999999975465], 0.5: [-35.315999999973094]}, 'both_same': {0.0: [93.81199999998995], 0.05: [76.42699999998078], 0.1: [57.40299999998928], 0.15: [18.770000000001545], 0.2: [2.80199999999555], 0.25: [-4.572000000007936], 0.3: [0.4539999999988095], 0.35: [-12.586999999999215], 0.4: [-16.563999999982933], 0.45: [-30.467999999971653], 0.5: [-18.905999999988236]}}\n","Solved after 1360 episodes!\n","1 noise_hrl: {'both': {0.0: [86.63399999998369], 0.05: [82.4339999999887], 0.1: [48.590999999998445], 0.15: [-6.330000000007147], 0.2: [-12.051000000001752], 0.25: [-18.417999999989856], 0.3: [-26.297999999979716], 0.35: [-22.90399999998143], 0.4: [-20.922999999981734], 0.45: [-29.68999999997741], 0.5: [-32.62399999997568]}, 'action_only': {0.0: [88.52299999998493], 0.05: [83.18199999998681], 0.1: [88.6749999999834], 0.15: [73.4799999999855], 0.2: [64.80199999998943], 0.25: [55.62199999998642], 0.3: [20.42600000001088], 0.35: [-11.299999999992702], 0.4: [-18.852999999979705], 0.45: [-19.789999999987227], 0.5: [-21.36899999997521]}, 'goal_only': {0.0: [91.29999999998626], 0.05: [81.58099999998245], 0.1: [43.439000000014985], 0.15: [20.00900000000222], 0.2: [-2.10600000000459], 0.25: [-4.129000000006505], 0.3: [-0.44200000000631307], 0.35: [-3.101000000000928], 0.4: [-6.752000000003262], 0.45: [-12.037000000005241], 0.5: [-12.719999999996539]}, 'both_same': {0.0: [88.69399999998515], 0.05: [84.79799999998917], 0.1: [58.726999999982375], 0.15: [29.536000000018795], 0.2: [7.198999999998173], 0.25: [-26.13499999998094], 0.3: [-16.663999999985393], 0.35: [-18.52199999997714], 0.4: [-30.104999999975707], 0.45: [-28.27699999997295], 0.5: [-29.499999999978172]}}\n","Solved after 2621 episodes!\n","2 noise_hrl: {'both': {0.0: [81.65099999998483], 0.05: [49.85999999999136], 0.1: [22.49100000001333], 0.15: [-13.738999999998386], 0.2: [-25.728999999977283], 0.25: [-27.732999999973877], 0.3: [-33.13999999997524], 0.35: [-35.65599999997148], 0.4: [-29.472999999974252], 0.45: [-32.170999999973176], 0.5: [-33.186999999969174]}, 'action_only': {0.0: [82.03999999998227], 0.05: [83.99899999998745], 0.1: [57.29799999998582], 0.15: [57.68099999998702], 0.2: [43.00400000001679], 0.25: [33.19900000001503], 0.3: [23.431000000017658], 0.35: [18.658000000008634], 0.4: [6.737999999997974], 0.45: [4.555999999996748], 0.5: [-5.530000000008207]}, 'goal_only': {0.0: [75.19699999998303], 0.05: [38.43900000001104], 0.1: [-3.0650000000038835], 0.15: [-10.748000000007853], 0.2: [-26.215999999983197], 0.25: [-39.665999999970445], 0.3: [-36.41399999997078], 0.35: [-39.26399999997114], 0.4: [-41.12899999997113], 0.45: [-36.7639999999707], 0.5: [-34.48399999997185]}, 'both_same': {0.0: [79.49899999997146], 0.05: [40.52100000001273], 0.1: [10.970999999997346], 0.15: [1.775000000000408], 0.2: [-29.695999999974507], 0.25: [-24.490999999982684], 0.3: [-27.617999999985596], 0.35: [-30.87099999997583], 0.4: [-21.266999999981557], 0.45: [-30.762999999976156], 0.5: [-35.21999999997106]}}\n","Solved after 2506 episodes!\n","Solved after 1324 episodes!\n","4 noise_hrl: {'both': {0.0: [82.86099999997796, 75.23399999997478], 0.05: [78.31099999998806, 62.16699999998617], 0.1: [30.617000000012446, 19.892000000011258], 0.15: [-1.2210000000053645, -22.586999999981728], 0.2: [-23.570999999977563, -31.298999999975678], 0.25: [-23.160999999976134, -36.672999999970976], 0.3: [-41.75299999997135, -35.48099999997301], 0.35: [-40.52799999997053, -33.084999999974045], 0.4: [-45.146999999987564, -37.34599999996906], 0.45: [-43.82799999998813, -40.477999999968], 0.5: [-44.001999999984534, -35.39999999996874]}, 'action_only': {0.0: [81.22799999998507, 72.79999999997713], 0.05: [89.05199999998824, 91.31099999998806], 0.1: [71.9259999999859, 72.30499999997622], 0.15: [54.288999999984114, 60.3739999999854], 0.2: [59.48699999998503, 42.19500000000787], 0.25: [56.025999999992834, 27.151000000020826], 0.3: [42.60500000001395, 15.609000000014621], 0.35: [36.99200000001656, 24.121000000015147], 0.4: [16.721999999999962, 23.24300000002076], 0.45: [23.018000000020237, 9.564999999992487], 0.5: [14.391000000004976, 20.777000000005632]}, 'goal_only': {0.0: [82.85999999997725, 78.71599999998139], 0.05: [79.83699999997657, 51.01599999999151], 0.1: [32.23300000001611, 16.69900000001225], 0.15: [-11.505000000000953, -34.894999999971894], 0.2: [-34.40299999997073, -29.682999999978982], 0.25: [-45.830999999989714, -42.20999999997546], 0.3: [-44.416999999986274, -34.1969999999761], 0.35: [-46.510999999992016, -37.988999999970495], 0.4: [-50.00000000000659, -36.57599999997542], 0.45: [-50.00000000000659, -43.21899999997737], 0.5: [-50.00000000000659, -34.26899999997667]}, 'both_same': {0.0: [80.10399999998602, 70.68999999998108], 0.05: [74.40699999999315, 46.68199999999807], 0.1: [36.307000000017425, 18.212000000001336], 0.15: [9.20200000000499, -16.53499999998371], 0.2: [-9.064000000004832, -21.708999999982115], 0.25: [-28.87299999997671, -31.757999999975638], 0.3: [-39.68999999997099, -35.03499999997118], 0.35: [-35.945999999973395, -26.709999999977757], 0.4: [-36.7849999999695, -26.615999999981142], 0.45: [-37.229999999969, -27.818999999977986], 0.5: [-41.885999999978864, -28.858999999979964]}}\n","Solved after 2153 episodes!\n","Solved after 1102 episodes!\n","6 noise_hrl: {'both': {0.0: [93.59099999998845, 95.82699999999268], 0.05: [51.135999999978964, 25.124000000013538], 0.1: [-3.143000000003118, 5.1549999999992835], 0.15: [-24.03099999997523, -14.66699999999627], 0.2: [-19.47699999998875, -3.477000000002735], 0.25: [-27.6629999999764, -5.881000000002434], 0.3: [-20.65099999997652, -14.382999999983559], 0.35: [-39.82999999996838, -15.63699999999387], 0.4: [-42.39499999997662, -19.943999999991235], 0.45: [-35.70699999997134, -30.742999999973076], 0.5: [-31.962999999976546, -32.9909999999714]}, 'action_only': {0.0: [93.60699999998842, 95.48499999999072], 0.05: [85.97899999998273, 50.60600000000608], 0.1: [74.31699999998244, 27.577000000017225], 0.15: [56.92299999998438, 12.710000000005227], 0.2: [38.09700000001057, 24.54500000001763], 0.25: [20.017000000006522, 42.660000000012175], 0.3: [3.1530000000005725, 40.35600000001543], 0.35: [8.061999999995978, 18.135000000001487], 0.4: [2.0989999999998266, -3.8560000000063726], 0.45: [2.586999999999339, -3.2130000000009757], 0.5: [-10.934000000000491, -24.534999999976478]}, 'goal_only': {0.0: [93.91699999999013, 95.6009999999929], 0.05: [56.24699999999941, 41.92300000000236], 0.1: [18.88300000000399, -2.1600000000025203], 0.15: [-5.391000000005116, -4.570000000006349], 0.2: [-15.927999999981886, -13.373999999999848], 0.25: [-33.416999999974216, -14.340999999990078], 0.3: [-41.385999999971155, -18.89999999999105], 0.35: [-33.943999999975084, -21.92799999997983], 0.4: [-37.42599999997123, -26.471999999983495], 0.45: [-35.70099999996956, -18.90099999998466], 0.5: [-36.912999999972456, -23.31499999998268]}, 'both_same': {0.0: [93.33499999998924, 95.807999999993], 0.05: [41.81300000000162, 5.360999999998208], 0.1: [6.667999999997147, -1.8880000000038668], 0.15: [-13.700999999994508, -10.656999999996817], 0.2: [-33.60599999997231, 0.9379999999962118], 0.25: [-29.283999999974498, -19.04299999997892], 0.3: [-28.807999999981057, -13.233000000000207], 0.35: [-32.12299999997394, -16.607999999986767], 0.4: [-38.43399999997072, -30.036999999973823], 0.45: [-28.691999999977444, -32.42299999997433], 0.5: [-25.25299999997437, -32.53799999997977]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mit6CCnJob4o"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointPushEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxjTe0z5csNW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617102290704,"user_tz":-60,"elapsed":1909980,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"4178317e-bbcd-4bfb-9930-e9e64d6add16"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [-5.231000000001368], 1.5: [24.51400000001438], 2.0: [80.13999999998688], 2.5: [61.24399999997684], 3.0: [86.9359999999865], 3.5: [93.56499999998947], 4.0: [93.77999999999241], 4.5: [94.09699999998963], 5.0: [93.19999999998832], 5.5: [93.25999999998812], 6.0: [92.19599999998658], 6.5: [91.98799999998619], 7.0: [91.98599999998616]}\n","1 scale: {1.0: [-5.231000000001368, -41.4489999999788], 1.5: [24.51400000001438, -15.471999999991793], 2.0: [80.13999999998688, 50.61200000000366], 2.5: [61.24399999997684, -25.99699999997498], 3.0: [86.9359999999865, 15.413000000012413], 3.5: [93.56499999998947, 40.74700000001278], 4.0: [93.77999999999241, 91.10499999998774], 4.5: [94.09699999998963, 91.35999999998494], 5.0: [93.19999999998832, 90.33699999998349], 5.5: [93.25999999998812, 92.07499999998652], 6.0: [92.19599999998658, 91.15699999998552], 6.5: [91.98799999998619, 82.28099999997453], 7.0: [91.98599999998616, 89.50099999998233]}\n","2 scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364]}\n","3 scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514, -12.76899999999743], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362, 28.74500000001892], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084, 53.087999999988], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097, 45.078999999998885], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261, 81.13599999998416], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816, 86.11799999998047], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008, 80.17399999997667], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806, 75.66699999998173], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774, 47.07300000000723], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666, 67.87099999998105], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602, 37.65400000001714], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858, 56.134999999993454], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364, -23.166999999977193]}\n","4 scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514, -12.76899999999743, -43.22599999997967], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362, 28.74500000001892, -18.66299999999167], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084, 53.087999999988, 17.53200000002023], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097, 45.078999999998885, 59.791999999986835], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261, 81.13599999998416, 72.70899999996905], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816, 86.11799999998047, 67.32499999998531], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008, 80.17399999997667, 80.57699999998226], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806, 75.66699999998173, 79.50199999998291], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774, 47.07300000000723, 72.92999999997699], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666, 67.87099999998105, 78.8779999999813], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602, 37.65400000001714, 81.75499999998151], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858, 56.134999999993454, 73.3919999999714], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364, -23.166999999977193, 69.45499999997952]}\n","5 scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514, -12.76899999999743, -43.22599999997967, -41.447999999973426], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362, 28.74500000001892, -18.66299999999167, 5.2859999999985305], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084, 53.087999999988, 17.53200000002023, 44.30700000001033], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097, 45.078999999998885, 59.791999999986835, 66.06199999997527], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261, 81.13599999998416, 72.70899999996905, 91.31699999998959], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816, 86.11799999998047, 67.32499999998531, 93.94699999998934], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008, 80.17399999997667, 80.57699999998226, 93.24099999998766], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806, 75.66699999998173, 79.50199999998291, 93.17399999998739], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774, 47.07300000000723, 72.92999999997699, 92.1889999999857], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666, 67.87099999998105, 78.8779999999813, 87.49899999998019], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602, 37.65400000001714, 81.75499999998151, 86.38099999997941], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858, 56.134999999993454, 73.3919999999714, 86.94799999997906], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364, -23.166999999977193, 69.45499999997952, 87.83399999998026]}\n","6 scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514, -12.76899999999743, -43.22599999997967, -41.447999999973426, 42.914000000004805], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362, 28.74500000001892, -18.66299999999167, 5.2859999999985305, 49.40099999999517], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084, 53.087999999988, 17.53200000002023, 44.30700000001033, 88.55599999998843], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097, 45.078999999998885, 59.791999999986835, 66.06199999997527, 92.82999999998943], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261, 81.13599999998416, 72.70899999996905, 91.31699999998959, 93.97399999998959], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816, 86.11799999998047, 67.32499999998531, 93.94699999998934, 82.36299999999277], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008, 80.17399999997667, 80.57699999998226, 93.24099999998766, 95.69099999999219], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806, 75.66699999998173, 79.50199999998291, 93.17399999998739, 96.12799999999352], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774, 47.07300000000723, 72.92999999997699, 92.1889999999857, 95.24099999999143], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666, 67.87099999998105, 78.8779999999813, 87.49899999998019, 84.8579999999886], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602, 37.65400000001714, 81.75499999998151, 86.38099999997941, 93.12699999999015], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858, 56.134999999993454, 73.3919999999714, 86.94799999997906, 82.75399999998237], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364, -23.166999999977193, 69.45499999997952, 87.83399999998026, 60.150999999993466]}\n","----\n","scale: {1.0: [-5.231000000001368, -41.4489999999788, -41.671999999976514, -12.76899999999743, -43.22599999997967, -41.447999999973426, 42.914000000004805], 1.5: [24.51400000001438, -15.471999999991793, -11.579999999997362, 28.74500000001892, -18.66299999999167, 5.2859999999985305, 49.40099999999517], 2.0: [80.13999999998688, 50.61200000000366, 50.32100000000084, 53.087999999988, 17.53200000002023, 44.30700000001033, 88.55599999998843], 2.5: [61.24399999997684, -25.99699999997498, 64.69999999998097, 45.078999999998885, 59.791999999986835, 66.06199999997527, 92.82999999998943], 3.0: [86.9359999999865, 15.413000000012413, 72.59899999998261, 81.13599999998416, 72.70899999996905, 91.31699999998959, 93.97399999998959], 3.5: [93.56499999998947, 40.74700000001278, 81.8459999999816, 86.11799999998047, 67.32499999998531, 93.94699999998934, 82.36299999999277], 4.0: [93.77999999999241, 91.10499999998774, 83.82899999998008, 80.17399999997667, 80.57699999998226, 93.24099999998766, 95.69099999999219], 4.5: [94.09699999998963, 91.35999999998494, 92.68399999998806, 75.66699999998173, 79.50199999998291, 93.17399999998739, 96.12799999999352], 5.0: [93.19999999998832, 90.33699999998349, 91.16099999998774, 47.07300000000723, 72.92999999997699, 92.1889999999857, 95.24099999999143], 5.5: [93.25999999998812, 92.07499999998652, 92.26899999998666, 67.87099999998105, 78.8779999999813, 87.49899999998019, 84.8579999999886], 6.0: [92.19599999998658, 91.15699999998552, 91.67599999998602, 37.65400000001714, 81.75499999998151, 86.38099999997941, 93.12699999999015], 6.5: [91.98799999998619, 82.28099999997453, 91.8829999999858, 56.134999999993454, 73.3919999999714, 86.94799999997906, 82.75399999998237], 7.0: [91.98599999998616, 89.50099999998233, 88.41199999998364, -23.166999999977193, 69.45499999997952, 87.83399999998026, 60.150999999993466]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTjUVh0r8nx9"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZYgmaK7PhX1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617103359171,"user_tz":-60,"elapsed":1068443,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"fce08fb8-7628-4ce7-f0c6-0589d26823fd"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [95.22699999999162], 0.05: [93.84499999999167], 0.1: [89.4839999999894], 0.15: [89.49099999998742], 0.2: [72.07999999998204], 0.25: [74.93799999999125], 0.3: [45.68000000000614], 0.35: [41.407000000009646], 0.4: [24.016000000002734]}\n","1 range: {0.0: [95.22699999999162, 88.05799999998773], 0.05: [93.84499999999167, 74.29299999997713], 0.1: [89.4839999999894, 82.50699999998378], 0.15: [89.49099999998742, 79.86399999998241], 0.2: [72.07999999998204, 78.475999999983], 0.25: [74.93799999999125, 72.68499999999194], 0.3: [45.68000000000614, 75.27999999997718], 0.35: [41.407000000009646, 64.39199999999022], 0.4: [24.016000000002734, 55.416999999982934]}\n","2 range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491]}\n","3 range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847, 83.2469999999764], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874, 86.27699999998161], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067, 85.11999999998334], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852, 77.60399999997964], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518, 74.22799999998692], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524, 76.73399999997046], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572, 70.07799999998076], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587, 49.96699999999185], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491, 56.41499999998734]}\n","4 range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847, 83.2469999999764, 68.45599999998876], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874, 86.27699999998161, 78.28899999997454], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067, 85.11999999998334, 70.12699999997685], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852, 77.60399999997964, 70.23699999997207], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518, 74.22799999998692, 65.74799999998059], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524, 76.73399999997046, 69.75299999997152], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572, 70.07799999998076, 63.83799999998798], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587, 49.96699999999185, 64.52099999999213], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491, 56.41499999998734, 59.77799999997901]}\n","5 range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847, 83.2469999999764, 68.45599999998876, 93.76799999999014], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874, 86.27699999998161, 78.28899999997454, 93.96899999999022], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067, 85.11999999998334, 70.12699999997685, 91.91799999998734], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852, 77.60399999997964, 70.23699999997207, 88.40199999998252], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518, 74.22799999998692, 65.74799999998059, 81.18999999998526], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524, 76.73399999997046, 69.75299999997152, 77.6529999999898], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572, 70.07799999998076, 63.83799999998798, 71.34899999999597], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587, 49.96699999999185, 64.52099999999213, 69.05699999998372], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491, 56.41499999998734, 59.77799999997901, 70.92799999998557]}\n","6 range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847, 83.2469999999764, 68.45599999998876, 93.76799999999014, 95.28699999999219], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874, 86.27699999998161, 78.28899999997454, 93.96899999999022, 95.97399999999303], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067, 85.11999999998334, 70.12699999997685, 91.91799999998734, 95.87199999999262], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852, 77.60399999997964, 70.23699999997207, 88.40199999998252, 87.20599999998889], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518, 74.22799999998692, 65.74799999998059, 81.18999999998526, 88.15399999999329], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524, 76.73399999997046, 69.75299999997152, 77.6529999999898, 75.63299999998799], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572, 70.07799999998076, 63.83799999998798, 71.34899999999597, 69.41499999998807], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587, 49.96699999999185, 64.52099999999213, 69.05699999998372, 32.67100000001164], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491, 56.41499999998734, 59.77799999997901, 70.92799999998557, 27.248000000019925]}\n","----\n","range: {0.0: [95.22699999999162, 88.05799999998773, 88.1219999999847, 83.2469999999764, 68.45599999998876, 93.76799999999014, 95.28699999999219], 0.05: [93.84499999999167, 74.29299999997713, 88.43099999998874, 86.27699999998161, 78.28899999997454, 93.96899999999022, 95.97399999999303], 0.1: [89.4839999999894, 82.50699999998378, 70.98699999998067, 85.11999999998334, 70.12699999997685, 91.91799999998734, 95.87199999999262], 0.15: [89.49099999998742, 79.86399999998241, 75.2909999999852, 77.60399999997964, 70.23699999997207, 88.40199999998252, 87.20599999998889], 0.2: [72.07999999998204, 78.475999999983, 75.63799999997518, 74.22799999998692, 65.74799999998059, 81.18999999998526, 88.15399999999329], 0.25: [74.93799999999125, 72.68499999999194, 54.62200000000524, 76.73399999997046, 69.75299999997152, 77.6529999999898, 75.63299999998799], 0.3: [45.68000000000614, 75.27999999997718, 48.44199999999572, 70.07799999998076, 63.83799999998798, 71.34899999999597, 69.41499999998807], 0.35: [41.407000000009646, 64.39199999999022, 38.61500000001587, 49.96699999999185, 64.52099999999213, 69.05699999998372, 32.67100000001164], 0.4: [24.016000000002734, 55.416999999982934, 52.80699999998491, 56.41499999998734, 59.77799999997901, 70.92799999998557, 27.248000000019925]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6g9omuyeT51"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz-OF0Zl6zp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612798920385,"user_tz":-60,"elapsed":83452,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"4aea5aff-c52d-4252-c444-7dbcb3043205"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","episodes.pop(5 - 1)\n","episodes.pop(11 - 2)\n","\n","torch.save(episodes, \"PointPush_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVujCiQrP3hh"},"source":[""],"execution_count":null,"outputs":[]}]}