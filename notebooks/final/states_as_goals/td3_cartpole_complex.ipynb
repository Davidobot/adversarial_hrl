{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"td3_cartpole_complex.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QM04UTO98Nf2","executionInfo":{"status":"ok","timestamp":1610016768985,"user_tz":-60,"elapsed":79017,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"30151f0c-fe85-4df5-c5d7-588834f0e7ee"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive/')\r\n","\r\n","#!cp \"/content/drive/My Drive/Dissertation/preprocessing.py\" .\r\n","#!cp -r \"/content/drive/My Drive/Dissertation/gym_maze\" .\r\n","!cp \"/content/drive/My Drive/Dissertation/envs/continuous_complex_cartpole.py\" ."],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4bwBIEXm8gt7","executionInfo":{"status":"ok","timestamp":1610016785344,"user_tz":-60,"elapsed":2017,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\r\n","def save_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/hiro_cartpole_rerun/{name}\" \r\n","\r\n","    torch.save({\r\n","      'controller': {\r\n","          'critic': model.critic.state_dict(),\r\n","          'actor': model.actor.state_dict(),\r\n","      }\r\n","    }, path)\r\n","\r\n","import copy\r\n","def load_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/hiro_cartpole_rerun/{name}\" \r\n","    checkpoint = torch.load(path)\r\n","\r\n","    model.critic.load_state_dict(checkpoint['controller']['critic'])\r\n","    model.critic_target = copy.deepcopy(model.critic)\r\n","    \r\n","    model.actor.load_state_dict(checkpoint['controller']['actor'])\r\n","    model.actor_target = copy.deepcopy(model.actor)\r\n","\r\n","    model.eval()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1610016792662,"user_tz":-60,"elapsed":6166,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1610016792674,"user_tz":-60,"elapsed":3491,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1610016792680,"user_tz":-60,"elapsed":1993,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from continuous_complex_cartpole import ContinuousCartPoleEnv \r\n","env = NormalizedEnv(ContinuousCartPoleEnv())"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1610016796611,"user_tz":-60,"elapsed":1556,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def plot_durations(episode_durations, actions):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*actions)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1610016798142,"user_tz":-60,"elapsed":1886,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1610016799133,"user_tz":-60,"elapsed":1658,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1610016800678,"user_tz":-60,"elapsed":1015,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1610016803354,"user_tz":-60,"elapsed":1203,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.head = nn.Linear(128, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l2 = nn.Linear(128, 128)\n","        self.l3 = nn.Linear(128, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l5 = nn.Linear(128, 128)\n","        self.l6 = nn.Linear(128, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1610016807085,"user_tz":-60,"elapsed":1272,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        #Create replay buffer\n","        self.memory = ReplayMemory(2000000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 5000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup=True, decay_epsilon=True):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI","executionInfo":{"status":"ok","timestamp":1610016932664,"user_tz":-60,"elapsed":2510,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["import time\n","SAVE_OFFSET = 0\n","\n","def train_model():\n","    global SAVE_OFFSET\n","\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = TD3(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 200\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 2000 # M\n","    episode_durations = []\n","    actions = [(0,0)]\n","\n","    steps = 0\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        episode_steps = 0\n","        done = False\n","        while not done:        \n","            # agent pick action ...\n","            action = agent.select_action(state, i_episode <= warmup)\n","\n","            # env response with next_observation, reward, terminate_info\n","            action_i = action.detach().cpu().squeeze(0).numpy()\n","\n","            observation, reward, done, info = env.step(action_i)\n","            steps += 1\n","\n","            #actions.append((steps, action_i[0]))\n","                \n","            if max_episode_length and episode_steps >= max_episode_length - 1:\n","                done = True\n","            episode_steps += 1 \n","                \n","            extrinsic_reward = torch.tensor([reward], device=device)\n","            \n","            overall_reward += reward\n","            \n","            next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            # agent observe and update policy\n","            agent.observe(state, action, next_state, extrinsic_reward, done)           \n","            state = next_state\n","            \n","            if i_episode > warmup:\n","                agent.update_policy()\n","\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, actions)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 200 == 0:\n","                print(f\"Episode {i_episode}: {np.mean(dur[-100:])}\")\n","            if np.mean(dur[-100:]) >= 300:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"td3_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","    return None"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrKiOTksSvYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610010217698,"user_tz":-60,"elapsed":2329105,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"321af97d-39e6-488c-d522-1ec200d773f4"},"source":["i = 1\r\n","\r\n","while i < 10:\r\n","    agent = train_model()\r\n","\r\n","    if agent is not None:\r\n","        print(i)\r\n","        i += 1"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Episode 200: 10.77\n","Episode 400: 10.04\n","Solved after 550 episodes!\n","1\n","Episode 200: 11.52\n","Episode 400: 14.47\n","Solved after 470 episodes!\n","2\n","Episode 200: 10.52\n","Episode 400: 9.55\n","Solved after 592 episodes!\n","3\n","Episode 200: 12.15\n","Episode 400: 9.67\n","Episode 600: 151.45\n","Solved after 632 episodes!\n","4\n","Episode 200: 10.01\n","Episode 400: 13.68\n","Solved after 506 episodes!\n","5\n","Episode 200: 12.92\n","Episode 400: 12.29\n","Solved after 497 episodes!\n","6\n","Episode 200: 11.95\n","Episode 400: 29.33\n","Episode 600: 265.42\n","Solved after 644 episodes!\n","7\n","Episode 200: 12.32\n","Episode 400: 28.18\n","Solved after 542 episodes!\n","8\n","Episode 200: 9.92\n","Episode 400: 9.57\n","Solved after 539 episodes!\n","9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ewRAF108B9Ia","executionInfo":{"status":"ok","timestamp":1610016997509,"user_tz":-60,"elapsed":877,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\r\n","def eval_model(agent, episode_durations):\r\n","    agent.eval()\r\n","    agent.is_training = False\r\n","\r\n","    max_episode_length = 200\r\n","    num_episodes = 100\r\n","\r\n","    for noise in np.arange(0,0.31,0.03):\r\n","\r\n","        overall_reward = 0\r\n","        for i_episode in range(num_episodes):\r\n","            observation = env.reset()\r\n","            # unsqueeze adds batch dimension\r\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","            episode_steps = 0\r\n","            done = False\r\n","            while not done:\r\n","                state = state + state_max * torch.FloatTensor(state.shape).uniform_(-noise/2, noise/2).to(device)\r\n","                state = state.float()\r\n","\r\n","                action = agent.select_action(state, False, False)\r\n","                action_i = action.detach().cpu().squeeze(0).numpy()\r\n","\r\n","                observation, reward, done, info = env.step(action_i) \r\n","                \r\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\r\n","                    done = True\r\n","                \r\n","                episode_steps += 1 \r\n","\r\n","                overall_reward += reward\r\n","\r\n","                state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","        episode_durations[noise].append(overall_reward / num_episodes)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgdtuIjSK-Aa"},"source":["state_min = torch.from_numpy(env.observation_space.low).to(device)\r\n","def fgsm_attack(data, eps, data_grad):\r\n","    sign_data_grad = data_grad.sign()\r\n","\r\n","    perturbed_data = data + eps * sign_data_grad * state_max\r\n","\r\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\r\n","\r\n","    return clipped_perturbed_data\r\n","\r\n","def fgsm_action(state, agent, eps, target, targetted):\r\n","    state = state.clone().detach().requires_grad_(True)\r\n","\r\n","    # initial forward pass\r\n","    action = agent.actor(state)\r\n","    action = torch.clamp(action, -1., 1.)\r\n","\r\n","    if not targetted:\r\n","        loss = F.mse_loss(action, target)\r\n","    else:\r\n","        loss = F.mse_loss(action, target if action > 0 else -target)\r\n","    agent.actor.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = state.grad.data\r\n","    # perturb state\r\n","    state_p = fgsm_attack(state, eps, data_grad).float()\r\n","    return agent.select_action(state_p, False, False)\r\n","\r\n","def apply_fgsm(agent, episode_durations, targetted):\r\n","    TARGET_ACTION = torch.tensor([[1.0]], device=device, dtype=torch.float)\r\n","\r\n","    agent.eval()\r\n","\r\n","    max_episode_length = 500\r\n","    agent.is_training = False\r\n","\r\n","    num_episodes = 100\r\n","\r\n","    for eps in np.arange(0.0, 0.031, 0.0025):\r\n","\r\n","        overall_reward = 0\r\n","        for i_episode in range(num_episodes):\r\n","            observation = env.reset()\r\n","            # unsqueeze adds batch dimension\r\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","            episode_steps = 0\r\n","            done = False\r\n","            while not done:\r\n","                action = fgsm_action(state, agent, eps, TARGET_ACTION, targetted)\r\n","                action_i = action.detach().cpu().squeeze(0).numpy()\r\n","\r\n","                observation, reward, done, info = env.step(action_i)\r\n","                \r\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\r\n","                    done = True\r\n","                \r\n","                episode_steps += 1 \r\n","\r\n","                overall_reward += reward\r\n","\r\n","                state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wfh59tBCA--"},"source":["def plot_norms(episode_durations):\r\n","    plt.figure(2, figsize=(10,10))\r\n","    \r\n","    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\r\n","    \r\n","    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\r\n","    plt.xlabel('L2 Norm')\r\n","    plt.ylabel('Average Reward')\r\n","    \r\n","    mu = np.mean(ys, axis=1)\r\n","    plt.plot(x, mu)\r\n","    stds = np.std(ys, axis = 1)\r\n","    plt.fill_between(x, mu + stds , mu - stds, alpha=0.2)\r\n","        \r\n","    plt.pause(0.001)  # pause a bit so that plots are updated\r\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jazaVnJNErwK","executionInfo":{"status":"ok","timestamp":1610038530768,"user_tz":-60,"elapsed":11322548,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"a45c56da-119f-4b33-bebe-1694cda79e22"},"source":["episodes = {}\r\n","for l2norm in np.arange(0,0.31,0.03):\r\n","    episodes[l2norm] = []\r\n","\r\n","fgsm_t = {}\r\n","fgsm_ut = {}\r\n","for eps in np.arange(0.0, 0.031, 0.0025):\r\n","    fgsm_t[eps] = []\r\n","    fgsm_ut[eps] = []\r\n","\r\n","n_observations = env.observation_space.shape[0]\r\n","n_actions = env.action_space.shape[0]\r\n","\r\n","i = 0\r\n","while i < 10:\r\n","    agent = train_model()\r\n","    #agent = TD3(n_observations, n_actions).to(device)\r\n","    #load_model(agent, f\"td3_{i}\")\r\n","\r\n","    if agent is not None:\r\n","        eval_model(agent, episodes)\r\n","        #apply_fgsm(agent, fgsm_t, True)\r\n","        #apply_fgsm(agent, fgsm_ut, False)\r\n","        print(f\"{i} noise: {episodes}\")\r\n","        #print(f\"{i} fgsm (t): {fgsm_t}\")\r\n","        #print(f\"{i} fgsm (ut): {fgsm_ut}\")\r\n","        i += 1\r\n","\r\n","print(\"---\")\r\n","print(f\"noise: {episodes}\")\r\n","#print(f\"fgsm (t): {fgsm_t}\")\r\n","#print(f\"fgsm (ut): {fgsm_ut}\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Episode 200: 10.96\n","Episode 400: 95.29\n","Episode 600: 264.47\n","Episode 800: 224.66\n","Episode 1000: 241.36\n","Episode 1200: 221.02\n","Episode 1400: 228.68\n","Episode 1600: 208.19\n","Episode 1800: 203.11\n","Episode 200: 10.65\n","Episode 400: 12.45\n","Episode 600: 254.14\n","Episode 800: 252.97\n","Episode 1000: 260.18\n","Episode 1200: 252.68\n","Episode 1400: 251.42\n","Episode 1600: 251.61\n","Episode 1800: 261.06\n","Episode 200: 10.87\n","Episode 400: 9.75\n","Episode 600: 199.73\n","Episode 800: 235.22\n","Episode 1000: 270.3\n","Episode 1200: 254.63\n","Episode 1400: 267.88\n","Episode 1600: 255.98\n","Episode 1800: 266.22\n","Episode 200: 10.36\n","Episode 400: 9.3\n","Episode 600: 284.15\n","Solved after 648 episodes!\n","0 noise: {0.0: [300.64], 0.03: [169.8], 0.06: [72.56], 0.09: [46.6], 0.12: [33.88], 0.15: [33.44], 0.18: [26.7], 0.21: [26.3], 0.24: [26.97], 0.27: [25.29], 0.3: [27.38]}\n","Episode 200: 11.1\n","Episode 400: 217.73\n","Episode 600: 252.56\n","Episode 800: 235.86\n","Episode 1000: 244.92\n","Episode 1200: 226.25\n","Episode 1400: 237.87\n","Episode 1600: 263.19\n","Episode 1800: 259.23\n","Episode 200: 10.52\n","Episode 400: 9.79\n","Episode 600: 106.27\n","Episode 800: 219.47\n","Episode 1000: 228.16\n","Episode 1200: 219.63\n","Episode 1400: 278.5\n","Episode 1600: 280.95\n","Episode 1800: 246.72\n","Episode 200: 10.82\n","Episode 400: 9.33\n","Episode 600: 222.3\n","Episode 800: 243.57\n","Episode 1000: 281.54\n","Episode 1200: 287.08\n","Solved after 1345 episodes!\n","1 noise: {0.0: [300.64, 295.84], 0.03: [169.8, 141.31], 0.06: [72.56, 51.87], 0.09: [46.6, 34.89], 0.12: [33.88, 29.43], 0.15: [33.44, 27.97], 0.18: [26.7, 25.33], 0.21: [26.3, 26.57], 0.24: [26.97, 29.64], 0.27: [25.29, 26.59], 0.3: [27.38, 25.38]}\n","Episode 200: 10.52\n","Episode 400: 15.24\n","Episode 600: 239.49\n","Episode 800: 239.0\n","Episode 1000: 256.14\n","Solved after 1156 episodes!\n","2 noise: {0.0: [300.64, 295.84, 266.15], 0.03: [169.8, 141.31, 59.83], 0.06: [72.56, 51.87, 36.68], 0.09: [46.6, 34.89, 26.73], 0.12: [33.88, 29.43, 30.68], 0.15: [33.44, 27.97, 28.92], 0.18: [26.7, 25.33, 25.93], 0.21: [26.3, 26.57, 26.99], 0.24: [26.97, 29.64, 26.59], 0.27: [25.29, 26.59, 24.82], 0.3: [27.38, 25.38, 23.27]}\n","Episode 200: 12.55\n","Episode 400: 9.28\n","Episode 600: 126.57\n","Episode 800: 253.44\n","Episode 1000: 283.45\n","Episode 1200: 200.66\n","Episode 1400: 222.2\n","Episode 1600: 213.69\n","Episode 1800: 254.39\n","Episode 200: 10.31\n","Episode 400: 9.38\n","Episode 600: 35.37\n","Episode 800: 299.23\n","Solved after 804 episodes!\n","3 noise: {0.0: [300.64, 295.84, 266.15, 328.21], 0.03: [169.8, 141.31, 59.83, 118.11], 0.06: [72.56, 51.87, 36.68, 43.46], 0.09: [46.6, 34.89, 26.73, 21.38], 0.12: [33.88, 29.43, 30.68, 16.93], 0.15: [33.44, 27.97, 28.92, 15.12], 0.18: [26.7, 25.33, 25.93, 12.25], 0.21: [26.3, 26.57, 26.99, 12.66], 0.24: [26.97, 29.64, 26.59, 12.2], 0.27: [25.29, 26.59, 24.82, 11.67], 0.3: [27.38, 25.38, 23.27, 11.44]}\n","Episode 200: 10.93\n","Episode 400: 95.8\n","Episode 600: 213.76\n","Episode 800: 202.14\n","Solved after 933 episodes!\n","4 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55]}\n","Episode 200: 11.37\n","Episode 400: 9.93\n","Episode 600: 188.1\n","Episode 800: 249.79\n","Episode 1000: 221.65\n","Episode 1200: 262.88\n","Episode 1400: 250.39\n","Episode 1600: 278.12\n","Episode 1800: 275.33\n","Episode 200: 15.54\n","Episode 400: 274.88\n","Solved after 438 episodes!\n","5 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18]}\n","Episode 200: 10.67\n","Episode 400: 56.01\n","Solved after 482 episodes!\n","6 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86, 395.97], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85, 51.99], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07, 37.64], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22, 31.12], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84, 29.06], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54, 28.06], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88, 24.68], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58, 23.04], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02, 24.46], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02, 24.85], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18, 26.84]}\n","Episode 200: 10.37\n","Episode 400: 155.02\n","Episode 600: 230.64\n","Episode 800: 206.33\n","Solved after 953 episodes!\n","7 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86, 395.97, 355.04], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85, 51.99, 198.27], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07, 37.64, 51.44], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22, 31.12, 40.41], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84, 29.06, 35.65], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54, 28.06, 30.99], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88, 24.68, 30.74], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58, 23.04, 32.14], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02, 24.46, 32.79], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02, 24.85, 30.52], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18, 26.84, 29.74]}\n","Episode 200: 11.02\n","Episode 400: 9.31\n","Episode 600: 282.4\n","Episode 800: 200.46\n","Episode 1000: 211.27\n","Episode 1200: 267.8\n","Solved after 1328 episodes!\n","8 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86, 395.97, 355.04, 320.83], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85, 51.99, 198.27, 163.99], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07, 37.64, 51.44, 76.22], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22, 31.12, 40.41, 39.71], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84, 29.06, 35.65, 34.21], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54, 28.06, 30.99, 33.55], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88, 24.68, 30.74, 33.95], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58, 23.04, 32.14, 27.87], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02, 24.46, 32.79, 25.38], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02, 24.85, 30.52, 26.36], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18, 26.84, 29.74, 24.32]}\n","Episode 200: 11.04\n","Episode 400: 17.04\n","Episode 600: 217.45\n","Episode 800: 202.15\n","Episode 1000: 262.68\n","Episode 1200: 297.05\n","Solved after 1208 episodes!\n","9 noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86, 395.97, 355.04, 320.83, 311.23], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85, 51.99, 198.27, 163.99, 84.78], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07, 37.64, 51.44, 76.22, 36.18], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22, 31.12, 40.41, 39.71, 31.1], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84, 29.06, 35.65, 34.21, 25.35], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54, 28.06, 30.99, 33.55, 28.32], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88, 24.68, 30.74, 33.95, 25.88], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58, 23.04, 32.14, 27.87, 26.91], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02, 24.46, 32.79, 25.38, 22.6], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02, 24.85, 30.52, 26.36, 24.33], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18, 26.84, 29.74, 24.32, 24.61]}\n","---\n","noise: {0.0: [300.64, 295.84, 266.15, 328.21, 334.5, 276.86, 395.97, 355.04, 320.83, 311.23], 0.03: [169.8, 141.31, 59.83, 118.11, 301.07, 189.85, 51.99, 198.27, 163.99, 84.78], 0.06: [72.56, 51.87, 36.68, 43.46, 240.14, 68.07, 37.64, 51.44, 76.22, 36.18], 0.09: [46.6, 34.89, 26.73, 21.38, 125.83, 39.22, 31.12, 40.41, 39.71, 31.1], 0.12: [33.88, 29.43, 30.68, 16.93, 60.95, 34.84, 29.06, 35.65, 34.21, 25.35], 0.15: [33.44, 27.97, 28.92, 15.12, 46.26, 32.54, 28.06, 30.99, 33.55, 28.32], 0.18: [26.7, 25.33, 25.93, 12.25, 58.84, 26.88, 24.68, 30.74, 33.95, 25.88], 0.21: [26.3, 26.57, 26.99, 12.66, 41.68, 26.58, 23.04, 32.14, 27.87, 26.91], 0.24: [26.97, 29.64, 26.59, 12.2, 35.41, 27.02, 24.46, 32.79, 25.38, 22.6], 0.27: [25.29, 26.59, 24.82, 11.67, 34.92, 27.02, 24.85, 30.52, 26.36, 24.33], 0.3: [27.38, 25.38, 23.27, 11.44, 29.55, 25.18, 26.84, 29.74, 24.32, 24.61]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g2OC9xKleOJI"},"source":[""],"execution_count":null,"outputs":[]}]}