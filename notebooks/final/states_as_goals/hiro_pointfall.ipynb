{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointfall.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1618854050872,"user_tz":-60,"elapsed":80221,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"7f6bc724-d1bd-45f4-dead-fb0bc85f741e"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_fall.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1618854051832,"user_tz":-60,"elapsed":940,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_2/{name}\" \n","\n","    torch.save({\n","      'meta_controller': {\n","          'critic': model.meta_controller.critic.state_dict(),\n","          'actor': model.meta_controller.actor.state_dict(),\n","      },\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_2/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1618854055440,"user_tz":-60,"elapsed":4527,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1618854055446,"user_tz":-60,"elapsed":4518,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1618854055456,"user_tz":-60,"elapsed":4515,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_fall import PointFallEnv \n","env = NormalizedEnv(PointFallEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1618854055462,"user_tz":-60,"elapsed":4504,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def plot_durations(episode_durations, goals_done):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*goals_done)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Goals done')\n","    \n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1618854055465,"user_tz":-60,"elapsed":4494,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1618854055467,"user_tz":-60,"elapsed":4415,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1618854055470,"user_tz":-60,"elapsed":4405,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1618854055472,"user_tz":-60,"elapsed":3330,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1618854055474,"user_tz":-60,"elapsed":2653,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 20000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1618854055478,"user_tz":-60,"elapsed":1377,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        self.meta_controller = TD3(nb_states, len(self.goal_dim), True).to(device)\n","        self.max_goal_dist = torch.from_numpy(np.array([2., 3.])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0.5, 1.5])).to(device)\n","        #self.meta_controller.depsilon = 1.0 / 10000\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.update_policy(self.off_policy_corrections)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    # correct goals to allow for use in experience replay\n","    def off_policy_corrections(self, sgoals, states, actions, candidate_goals=8):\n","        first_s = [s[0] for s in states] # First x\n","        last_s = [s[-1] for s in states] # Last x\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # diff = 1\n","        diff_goal = (np.array(last_s) - np.array(first_s))[:, np.newaxis, :self.goal_dimen]\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # original = 1\n","        # random = candidate_goals\n","        scale = self.max_goal_dist.cpu().numpy()\n","        original_goal = np.array(sgoals)[:, np.newaxis, :]\n","        random_goals = np.random.normal(loc=diff_goal, scale=.5*scale,\n","                                        size=(BATCH_SIZE, candidate_goals, original_goal.shape[-1]))\n","        random_goals = random_goals.clip(-scale, scale)\n","\n","        # Shape: (batch_size, 10, subgoal_dim)\n","        candidates = np.concatenate([original_goal, diff_goal, random_goals], axis=1)\n","        #states = np.array(states)[:, :-1, :]\n","        actions = np.array(actions)\n","        seq_len = len(states[0])\n","\n","        # For ease\n","        new_batch_sz = seq_len * BATCH_SIZE\n","        action_dim = actions[0][0].shape\n","        obs_dim = states[0][0].shape\n","        ncands = candidates.shape[1]\n","\n","        true_actions = actions.reshape((new_batch_sz,) + action_dim)\n","        observations = states.reshape((new_batch_sz,) + obs_dim)\n","        goal_shape = (new_batch_sz, self.goal_dimen)\n","        # observations = get_obs_tensor(observations, sg_corrections=True)\n","\n","        # batched_candidates = np.tile(candidates, [seq_len, 1, 1])\n","        # batched_candidates = batched_candidates.transpose(1, 0, 2)\n","\n","        policy_actions = np.zeros((ncands, new_batch_sz) + action_dim)\n","\n","        observations = torch.from_numpy(observations).to(device)\n","        for c in range(ncands):\n","            subgoal = candidates[:,c]\n","            candidate = (subgoal + states[:, 0, :self.goal_dimen])[:, None] - states[:, :, :self.goal_dimen]\n","            candidate = candidate.reshape(*goal_shape)\n","            policy_actions[c] = self.controller.actor(torch.cat([observations, torch.from_numpy(candidate).to(device)], 1).float()).detach().cpu().numpy()\n","\n","        difference = (policy_actions - true_actions)\n","        difference = np.where(difference != -np.inf, difference, 0)\n","        difference = difference.reshape((ncands, BATCH_SIZE, seq_len) + action_dim).transpose(1, 0, 2, 3)\n","\n","        logprob = -0.5*np.sum(np.linalg.norm(difference, axis=-1)**2, axis=-1)\n","        max_indices = np.argmax(logprob, axis=-1)\n","\n","        return torch.from_numpy(candidates[np.arange(BATCH_SIZE), max_indices]).to(device).float()\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done, state_seq, action_seq)\n","\n","    def select_goal(self, s_t, warmup, decay_epsilon):\n","        return self.meta_controller.select_action(s_t, warmup, decay_epsilon) * self.max_goal_dist + self.goal_offset\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 7\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 200\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 400 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_fall_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1618854079389,"user_tz":-60,"elapsed":6598,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data - eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    #g_state = torch.tensor(g_state, requires_grad=True)\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    # initial forward pass\n","    goal = agent.meta_controller.actor(g_state)\n","    goal = torch.clamp(goal, -1., 1.)\n","\n","    loss = F.mse_loss(goal, target)\n","\n","    if targeted:\n","        # initial forward pass\n","        goal = agent.meta_controller.actor(g_state)\n","        goal = torch.clamp(goal, -1., 1.)\n","\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = agent.meta_controller.critic.Q1(g_state, agent.meta_controller.actor(g_state)).mean()\n","\n","    agent.meta_controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal = agent.select_goal(og_state, False, False)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhC6f7N6sJoa","executionInfo":{"status":"ok","timestamp":1618883686191,"user_tz":-60,"elapsed":26880577,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"e9182555-d1ad-432c-bfc0-ab0c0cbd73ca"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","\"\"\"\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\"\"\"\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [96.20799999999399], 0.02: [-50.00000000000659], 0.04: [-48.90900000000266], 0.06: [-16.856999999993175], 0.08: [-48.55100000000041], 0.1: [-31.740999999973997], 0.12: [-27.17099999997843], 0.14: [-46.59699999999375], 0.16: [-46.304999999990414], 0.18: [-44.2169999999864], 0.2: [-45.37199999998611]}, 'action': {0.0: [96.20899999999399], 0.02: [-45.726999999989445], 0.04: [-42.810999999978684], 0.06: [-13.15799999999498], 0.08: [-16.96999999998711], 0.1: [-4.460000000006593], 0.12: [-24.32699999997807], 0.14: [-41.384999999973424], 0.16: [-32.792999999971734], 0.18: [-41.16699999997657], 0.2: [-45.48399999999259]}}\n","0 fgsm (t): {'goal': {0.0: [94.73399999999226], 0.02: [-50.00000000000659], 0.04: [-50.00000000000659], 0.06: [-31.047999999979282], 0.08: [-40.023999999971096], 0.1: [-45.70899999998938], 0.12: [-45.77099999999415], 0.14: [-50.00000000000659], 0.16: [-50.00000000000659], 0.18: [-50.00000000000659], 0.2: [-50.00000000000659]}, 'action': {0.0: [96.2119999999964], 0.02: [-36.248999999972305], 0.04: [-34.72999999997458], 0.06: [-39.332999999971044], 0.08: [-28.638999999976953], 0.1: [-40.51699999997026], 0.12: [-48.86300000000154], 0.14: [-42.027999999972124], 0.16: [-31.533999999972952], 0.18: [-22.98999999997989], 0.2: [-26.624999999976417]}}\n","1 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735], 0.02: [-50.00000000000659, 3.821000000000433], 0.04: [-48.90900000000266, 7.2969999999987705], 0.06: [-16.856999999993175, 5.735999999995805], 0.08: [-48.55100000000041, -35.67599999997124], 0.1: [-31.740999999973997, -3.7940000000073986], 0.12: [-27.17099999997843, -2.328000000004174], 0.14: [-46.59699999999375, 14.53800000000905], 0.16: [-46.304999999990414, 19.923000000015804], 0.18: [-44.2169999999864, 17.839000000013087], 0.2: [-45.37199999998611, -0.5790000000016484]}, 'action': {0.0: [96.20899999999399, 91.38099999999147], 0.02: [-45.726999999989445, -16.24199999999257], 0.04: [-42.810999999978684, -17.232999999983686], 0.06: [-13.15799999999498, -25.85399999997424], 0.08: [-16.96999999998711, -21.478999999987888], 0.1: [-4.460000000006593, -1.8570000000014604], 0.12: [-24.32699999997807, 0.595999999994832], 0.14: [-41.384999999973424, -29.087999999975896], 0.16: [-32.792999999971734, -33.412999999973636], 0.18: [-41.16699999997657, -30.487999999972153], 0.2: [-45.48399999999259, -0.005000000003609284]}}\n","1 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912], 0.02: [-50.00000000000659, 64.40399999997936], 0.04: [-50.00000000000659, -26.554999999974072], 0.06: [-31.047999999979282, -50.00000000000659], 0.08: [-40.023999999971096, -48.58200000000333], 0.1: [-45.70899999998938, -50.00000000000659], 0.12: [-45.77099999999415, -50.00000000000659], 0.14: [-50.00000000000659, -46.52799999999577], 0.16: [-50.00000000000659, -47.171999999999024], 0.18: [-50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659]}, 'action': {0.0: [96.2119999999964, 91.35999999999154], 0.02: [-36.248999999972305, 21.818000000019325], 0.04: [-34.72999999997458, -19.844999999979883], 0.06: [-39.332999999971044, -22.698999999976344], 0.08: [-28.638999999976953, -32.4979999999702], 0.1: [-40.51699999997026, -44.11599999998063], 0.12: [-48.86300000000154, -46.82899999999433], 0.14: [-42.027999999972124, -48.89200000000274], 0.16: [-31.533999999972952, -50.00000000000659], 0.18: [-22.98999999997989, -50.00000000000659], 0.2: [-26.624999999976417, -50.00000000000659]}}\n","2 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616]}}\n","2 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203]}}\n","3 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128, 96.49999999999245], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827, -22.266999999980797], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486, -25.53699999997545], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995, -20.94299999998473], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659, -26.22799999997859], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659, -26.857999999978393], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659, -37.23699999997146], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659, -19.572999999983992], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659, -17.714999999992028], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659, -10.192000000004215], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659, -6.581000000008188]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984, 92.87899999999465], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251, 1.0339999999937228], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659, -50.00000000000659], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001, -32.89699999997614], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209, 42.47500000001655], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506, 13.451000000003669], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717, 5.882999999993021], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158, 12.900000000000905], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609, 32.720000000016945], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975, 8.587999999996248], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616, -20.75899999998627]}}\n","3 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416, 94.32899999999343], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336, 23.666000000018666], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766, 30.08600000001798], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659, 1.2359999999966758], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585, -4.46100000000693], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461, -26.50299999997753], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951, -36.27499999997121], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446, -33.216999999977304], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479, -45.59199999999325], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569, -47.81999999999684], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276, -45.216999999990094]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189, 95.44199999999375], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423, -21.798999999976854], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843, -15.933999999996209], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744, 59.440999999964006], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705, -48.86200000000245], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767, -46.73400000000107], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966, -44.621999999984745], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365, -46.71599999999343], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167, -42.523999999973974], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543, -45.7379999999878], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203, -47.858000000006065]}}\n","4 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128, 96.49999999999245, 97.1189999999948], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827, -22.266999999980797, -50.00000000000659], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486, -25.53699999997545, -24.944999999978933], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995, -20.94299999998473, -50.00000000000659], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659, -26.22799999997859, -50.00000000000659], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659, -26.857999999978393, -50.00000000000659], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659, -37.23699999997146, -50.00000000000659], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659, -19.572999999983992, -48.9760000000065], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659, -17.714999999992028, -50.00000000000659], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659, -10.192000000004215, -50.00000000000659], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659, -6.581000000008188, -50.00000000000659]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984, 92.87899999999465, 95.640999999993], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251, 1.0339999999937228, -47.45899999999552], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659, -50.00000000000659, -15.406999999994113], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001, -32.89699999997614, -33.83299999996932], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209, 42.47500000001655, -35.93399999996941], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506, 13.451000000003669, -29.982999999970207], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717, 5.882999999993021, -27.356999999977432], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158, 12.900000000000905, -39.19599999996803], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609, 32.720000000016945, -44.73399999998379], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975, 8.587999999996248, -42.35999999997902], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616, -20.75899999998627, -35.287999999968854]}}\n","4 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416, 94.32899999999343, 97.14999999999485], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336, 23.666000000018666, -25.83799999997881], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766, 30.08600000001798, -28.673999999978037], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659, 1.2359999999966758, -4.4450000000054795], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585, -4.46100000000693, -22.061999999983037], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461, -26.50299999997753, -39.17199999997146], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951, -36.27499999997121, -38.3289999999688], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446, -33.216999999977304, -48.763000000005725], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479, -45.59199999999325, -47.38299999999524], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569, -47.81999999999684, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276, -45.216999999990094, -50.00000000000659]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189, 95.44199999999375, 97.11999999999489], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423, -21.798999999976854, -38.33699999997388], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843, -15.933999999996209, 5.420999999998697], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744, 59.440999999964006, 1.5079999999947904], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705, -48.86200000000245, -14.52799999998906], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767, -46.73400000000107, -34.171999999973224], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966, -44.621999999984745, -31.733999999975403], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365, -46.71599999999343, -36.45199999996893], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167, -42.523999999973974, -41.35299999997053], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543, -45.7379999999878, -43.071999999982374], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203, -47.858000000006065, -46.011999999995034]}}\n","5 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128, 96.49999999999245, 97.1189999999948, 96.41099999999356], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827, -22.266999999980797, -50.00000000000659, 68.15399999997717], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486, -25.53699999997545, -24.944999999978933, 60.12499999998774], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995, -20.94299999998473, -50.00000000000659, 41.6290000000022], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659, -26.22799999997859, -50.00000000000659, 6.991999999992787], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659, -26.857999999978393, -50.00000000000659, -4.687000000004897], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659, -37.23699999997146, -50.00000000000659, -31.217999999975742], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659, -19.572999999983992, -48.9760000000065, -23.15999999997866], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659, -17.714999999992028, -50.00000000000659, -26.550999999976188], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659, -10.192000000004215, -50.00000000000659, -37.798999999971834], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659, -6.581000000008188, -50.00000000000659, -14.953999999997976]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984, 92.87899999999465, 95.640999999993, 96.43299999999361], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251, 1.0339999999937228, -47.45899999999552, 59.7539999999909], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659, -50.00000000000659, -15.406999999994113, 25.211000000010696], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001, -32.89699999997614, -33.83299999996932, 79.02799999998616], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209, 42.47500000001655, -35.93399999996941, 24.119000000008832], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506, 13.451000000003669, -29.982999999970207, -36.46499999997313], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717, 5.882999999993021, -27.356999999977432, 5.362999999994241], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158, 12.900000000000905, -39.19599999996803, 23.69800000001671], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609, 32.720000000016945, -44.73399999998379, 6.028999999997403], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975, 8.587999999996248, -42.35999999997902, 17.643000000004555], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616, -20.75899999998627, -35.287999999968854, -13.96199999999394]}}\n","5 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416, 94.32899999999343, 97.14999999999485, 96.43399999999357], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336, 23.666000000018666, -25.83799999997881, -47.085999999994726], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766, 30.08600000001798, -28.673999999978037, -46.41199999999535], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659, 1.2359999999966758, -4.4450000000054795, -32.976999999973685], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585, -4.46100000000693, -22.061999999983037, -14.396999999999778], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461, -26.50299999997753, -39.17199999997146, -19.440999999985852], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951, -36.27499999997121, -38.3289999999688, -31.121999999976737], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446, -33.216999999977304, -48.763000000005725, -18.996999999986084], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479, -45.59199999999325, -47.38299999999524, -33.65199999997756], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569, -47.81999999999684, -50.00000000000659, -29.294999999977495], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276, -45.216999999990094, -50.00000000000659, -30.323999999977715]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189, 95.44199999999375, 97.11999999999489, 96.37799999999346], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423, -21.798999999976854, -38.33699999997388, 9.24299999999361], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843, -15.933999999996209, 5.420999999998697, 55.44999999998566], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744, 59.440999999964006, 1.5079999999947904, -36.1629999999717], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705, -48.86200000000245, -14.52799999998906, -38.12699999996832], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767, -46.73400000000107, -34.171999999973224, -27.399999999973705], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966, -44.621999999984745, -31.733999999975403, -44.44699999998361], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365, -46.71599999999343, -36.45199999996893, -31.30599999997446], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167, -42.523999999973974, -41.35299999997053, -31.11299999997375], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543, -45.7379999999878, -43.071999999982374, -22.908999999984793], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203, -47.858000000006065, -46.011999999995034, -28.78799999997183]}}\n","6 fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128, 96.49999999999245, 97.1189999999948, 96.41099999999356, 97.8399999999961], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827, -22.266999999980797, -50.00000000000659, 68.15399999997717, -25.727999999983926], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486, -25.53699999997545, -24.944999999978933, 60.12499999998774, 75.55199999997794], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995, -20.94299999998473, -50.00000000000659, 41.6290000000022, 6.262999999995059], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659, -26.22799999997859, -50.00000000000659, 6.991999999992787, -5.355000000001148], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659, -26.857999999978393, -50.00000000000659, -4.687000000004897, -11.83499999999424], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659, -37.23699999997146, -50.00000000000659, -31.217999999975742, 0.06799999999980189], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659, -19.572999999983992, -48.9760000000065, -23.15999999997866, -3.29400000000398], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659, -17.714999999992028, -50.00000000000659, -26.550999999976188, 0.37199999999740646], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659, -10.192000000004215, -50.00000000000659, -37.798999999971834, -14.201999999984155], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659, -6.581000000008188, -50.00000000000659, -14.953999999997976, -24.664999999977297]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984, 92.87899999999465, 95.640999999993, 96.43299999999361, 97.85899999999603], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251, 1.0339999999937228, -47.45899999999552, 59.7539999999909, -18.234999999992098], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659, -50.00000000000659, -15.406999999994113, 25.211000000010696, 0.15599999999580122], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001, -32.89699999997614, -33.83299999996932, 79.02799999998616, 25.943000000019453], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209, 42.47500000001655, -35.93399999996941, 24.119000000008832, 28.785000000014353], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506, 13.451000000003669, -29.982999999970207, -36.46499999997313, 11.162999999991682], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717, 5.882999999993021, -27.356999999977432, 5.362999999994241, 2.5389999999946293], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158, 12.900000000000905, -39.19599999996803, 23.69800000001671, -1.8030000000009048], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609, 32.720000000016945, -44.73399999998379, 6.028999999997403, -7.744000000008466], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975, 8.587999999996248, -42.35999999997902, 17.643000000004555, -24.98499999997765], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616, -20.75899999998627, -35.287999999968854, -13.96199999999394, -34.41799999997101]}}\n","6 fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416, 94.32899999999343, 97.14999999999485, 96.43399999999357, 97.85999999999605], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336, 23.666000000018666, -25.83799999997881, -47.085999999994726, 92.66099999999122], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766, 30.08600000001798, -28.673999999978037, -46.41199999999535, 67.1519999999935], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659, 1.2359999999966758, -4.4450000000054795, -32.976999999973685, 51.66199999999497], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585, -4.46100000000693, -22.061999999983037, -14.396999999999778, -17.518999999990054], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461, -26.50299999997753, -39.17199999997146, -19.440999999985852, -23.617999999978334], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951, -36.27499999997121, -38.3289999999688, -31.121999999976737, -39.70999999996984], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446, -33.216999999977304, -48.763000000005725, -18.996999999986084, -39.385999999968995], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479, -45.59199999999325, -47.38299999999524, -33.65199999997756, -40.59999999996876], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569, -47.81999999999684, -50.00000000000659, -29.294999999977495, -40.99499999996963], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276, -45.216999999990094, -50.00000000000659, -30.323999999977715, -50.00000000000659]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189, 95.44199999999375, 97.11999999999489, 96.37799999999346, 97.84799999999599], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423, -21.798999999976854, -38.33699999997388, 9.24299999999361, -41.60599999996968], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843, -15.933999999996209, 5.420999999998697, 55.44999999998566, 37.96000000001927], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744, 59.440999999964006, 1.5079999999947904, -36.1629999999717, 38.2810000000171], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705, -48.86200000000245, -14.52799999998906, -38.12699999996832, 15.607999999997752], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767, -46.73400000000107, -34.171999999973224, -27.399999999973705, -14.619999999993023], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966, -44.621999999984745, -31.733999999975403, -44.44699999998361, -26.197999999978943], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365, -46.71599999999343, -36.45199999996893, -31.30599999997446, -32.293999999967944], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167, -42.523999999973974, -41.35299999997053, -31.11299999997375, -9.740000000005354], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543, -45.7379999999878, -43.071999999982374, -22.908999999984793, -23.101999999976755], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203, -47.858000000006065, -46.011999999995034, -28.78799999997183, -33.99699999997162]}}\n","----\n","fgsm (ut): {'goal': {0.0: [96.20799999999399, 91.34699999998735, 95.78599999999128, 96.49999999999245, 97.1189999999948, 96.41099999999356, 97.8399999999961], 0.02: [-50.00000000000659, 3.821000000000433, 84.44199999998827, -22.266999999980797, -50.00000000000659, 68.15399999997717, -25.727999999983926], 0.04: [-48.90900000000266, 7.2969999999987705, -1.7089999999998486, -25.53699999997545, -24.944999999978933, 60.12499999998774, 75.55199999997794], 0.06: [-16.856999999993175, 5.735999999995805, -45.602999999988995, -20.94299999998473, -50.00000000000659, 41.6290000000022, 6.262999999995059], 0.08: [-48.55100000000041, -35.67599999997124, -50.00000000000659, -26.22799999997859, -50.00000000000659, 6.991999999992787, -5.355000000001148], 0.1: [-31.740999999973997, -3.7940000000073986, -50.00000000000659, -26.857999999978393, -50.00000000000659, -4.687000000004897, -11.83499999999424], 0.12: [-27.17099999997843, -2.328000000004174, -50.00000000000659, -37.23699999997146, -50.00000000000659, -31.217999999975742, 0.06799999999980189], 0.14: [-46.59699999999375, 14.53800000000905, -50.00000000000659, -19.572999999983992, -48.9760000000065, -23.15999999997866, -3.29400000000398], 0.16: [-46.304999999990414, 19.923000000015804, -50.00000000000659, -17.714999999992028, -50.00000000000659, -26.550999999976188, 0.37199999999740646], 0.18: [-44.2169999999864, 17.839000000013087, -50.00000000000659, -10.192000000004215, -50.00000000000659, -37.798999999971834, -14.201999999984155], 0.2: [-45.37199999998611, -0.5790000000016484, -50.00000000000659, -6.581000000008188, -50.00000000000659, -14.953999999997976, -24.664999999977297]}, 'action': {0.0: [96.20899999999399, 91.38099999999147, 94.17599999998984, 92.87899999999465, 95.640999999993, 96.43299999999361, 97.85899999999603], 0.02: [-45.726999999989445, -16.24199999999257, -0.8130000000035251, 1.0339999999937228, -47.45899999999552, 59.7539999999909, -18.234999999992098], 0.04: [-42.810999999978684, -17.232999999983686, -50.00000000000659, -50.00000000000659, -15.406999999994113, 25.211000000010696, 0.15599999999580122], 0.06: [-13.15799999999498, -25.85399999997424, -14.319999999989001, -32.89699999997614, -33.83299999996932, 79.02799999998616, 25.943000000019453], 0.08: [-16.96999999998711, -21.478999999987888, -33.23399999997209, 42.47500000001655, -35.93399999996941, 24.119000000008832, 28.785000000014353], 0.1: [-4.460000000006593, -1.8570000000014604, -27.75199999997506, 13.451000000003669, -29.982999999970207, -36.46499999997313, 11.162999999991682], 0.12: [-24.32699999997807, 0.595999999994832, -18.658999999981717, 5.882999999993021, -27.356999999977432, 5.362999999994241, 2.5389999999946293], 0.14: [-41.384999999973424, -29.087999999975896, -35.79499999997158, 12.900000000000905, -39.19599999996803, 23.69800000001671, -1.8030000000009048], 0.16: [-32.792999999971734, -33.412999999973636, -44.58799999998609, 32.720000000016945, -44.73399999998379, 6.028999999997403, -7.744000000008466], 0.18: [-41.16699999997657, -30.487999999972153, -37.55299999996975, 8.587999999996248, -42.35999999997902, 17.643000000004555, -24.98499999997765], 0.2: [-45.48399999999259, -0.005000000003609284, -47.759999999996616, -20.75899999998627, -35.287999999968854, -13.96199999999394, -34.41799999997101]}}\n","fgsm (t): {'goal': {0.0: [94.73399999999226, 87.0119999999912, 96.02299999999416, 94.32899999999343, 97.14999999999485, 96.43399999999357, 97.85999999999605], 0.02: [-50.00000000000659, 64.40399999997936, 34.69500000001336, 23.666000000018666, -25.83799999997881, -47.085999999994726, 92.66099999999122], 0.04: [-50.00000000000659, -26.554999999974072, -43.0079999999766, 30.08600000001798, -28.673999999978037, -46.41199999999535, 67.1519999999935], 0.06: [-31.047999999979282, -50.00000000000659, -50.00000000000659, 1.2359999999966758, -4.4450000000054795, -32.976999999973685, 51.66199999999497], 0.08: [-40.023999999971096, -48.58200000000333, -45.314999999991585, -4.46100000000693, -22.061999999983037, -14.396999999999778, -17.518999999990054], 0.1: [-45.70899999998938, -50.00000000000659, -44.64699999998461, -26.50299999997753, -39.17199999997146, -19.440999999985852, -23.617999999978334], 0.12: [-45.77099999999415, -50.00000000000659, -47.3429999999951, -36.27499999997121, -38.3289999999688, -31.121999999976737, -39.70999999996984], 0.14: [-50.00000000000659, -46.52799999999577, -45.962999999991446, -33.216999999977304, -48.763000000005725, -18.996999999986084, -39.385999999968995], 0.16: [-50.00000000000659, -47.171999999999024, -46.25799999999479, -45.59199999999325, -47.38299999999524, -33.65199999997756, -40.59999999996876], 0.18: [-50.00000000000659, -50.00000000000659, -47.25899999999569, -47.81999999999684, -50.00000000000659, -29.294999999977495, -40.99499999996963], 0.2: [-50.00000000000659, -50.00000000000659, -42.20099999997276, -45.216999999990094, -50.00000000000659, -30.323999999977715, -50.00000000000659]}, 'action': {0.0: [96.2119999999964, 91.35999999999154, 95.76599999999189, 95.44199999999375, 97.11999999999489, 96.37799999999346, 97.84799999999599], 0.02: [-36.248999999972305, 21.818000000019325, 12.194000000014423, -21.798999999976854, -38.33699999997388, 9.24299999999361, -41.60599999996968], 0.04: [-34.72999999997458, -19.844999999979883, -26.872999999977843, -15.933999999996209, 5.420999999998697, 55.44999999998566, 37.96000000001927], 0.06: [-39.332999999971044, -22.698999999976344, -27.881999999977744, 59.440999999964006, 1.5079999999947904, -36.1629999999717, 38.2810000000171], 0.08: [-28.638999999976953, -32.4979999999702, -40.9159999999705, -48.86200000000245, -14.52799999998906, -38.12699999996832, 15.607999999997752], 0.1: [-40.51699999997026, -44.11599999998063, -44.55199999998767, -46.73400000000107, -34.171999999973224, -27.399999999973705, -14.619999999993023], 0.12: [-48.86300000000154, -46.82899999999433, -43.506999999982966, -44.621999999984745, -31.733999999975403, -44.44699999998361, -26.197999999978943], 0.14: [-42.027999999972124, -48.89200000000274, -46.253999999991365, -46.71599999999343, -36.45199999996893, -31.30599999997446, -32.293999999967944], 0.16: [-31.533999999972952, -50.00000000000659, -41.83999999997167, -42.523999999973974, -41.35299999997053, -31.11299999997375, -9.740000000005354], 0.18: [-22.98999999997989, -50.00000000000659, -42.62299999997543, -45.7379999999878, -43.071999999982374, -22.908999999984793, -23.101999999976755], 0.2: [-26.624999999976417, -50.00000000000659, -38.80799999997203, -47.858000000006065, -46.011999999995034, -28.78799999997183, -33.99699999997162]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TdQEb3LDjtxd"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointFallEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHVoTScvj8Ru","executionInfo":{"status":"ok","timestamp":1612553556887,"user_tz":-60,"elapsed":34080,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"e788397d-1469-42bb-e485-79c68de8c682"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [98.889999999998], 1.5: [59.70399999999541], 2.0: [-1.3090000000015707], 2.5: [53.52999999999617], 3.0: [62.327999999992215], 3.5: [-48.53100000000033], 4.0: [97.69499999999574], 4.5: [97.55999999999551], 5.0: [97.29799999999518], 5.5: [68.2129999999829], 6.0: [88.95899999998721], 6.5: [72.05699999998856], 7.0: [41.23500000001407]}\n","1 scale: {1.0: [98.889999999998, 34.25200000001467], 1.5: [59.70399999999541, 30.9810000000133], 2.0: [-1.3090000000015707, 46.32800000000274], 2.5: [53.52999999999617, 61.68399999998656], 3.0: [62.327999999992215, 56.54899999998754], 3.5: [-48.53100000000033, 79.5949999999876], 4.0: [97.69499999999574, 94.21599999999293], 4.5: [97.55999999999551, 95.63199999999215], 5.0: [97.29799999999518, 95.90399999999259], 5.5: [68.2129999999829, 95.73799999999235], 6.0: [88.95899999998721, 95.66899999999218], 6.5: [72.05699999998856, 95.57899999999208], 7.0: [41.23500000001407, 95.51799999999189]}\n","2 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963]}\n","3 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792]}\n","4 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106]}\n","5 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888]}\n","6 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811, 35.40100000002162], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627, 52.84699999999012], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937, 54.21699999998684], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392, 79.83299999997604], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881, 97.96499999999625], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406, 97.86199999999607], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353, 97.85099999999606], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245, 97.80899999999593], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307, 97.54299999999543], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248, 97.17699999999505], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926, 96.76199999999422], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986, 96.52199999999375], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888, 96.3909999999935]}\n","7 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811, 35.40100000002162, 34.06000000002042], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627, 52.84699999999012, 41.694000000012494], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937, 54.21699999998684, 93.52999999999261], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392, 79.83299999997604, 75.76599999997384], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881, 97.96499999999625, 96.77899999999474], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406, 97.86199999999607, 88.68299999999454], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353, 97.85099999999606, 97.72399999999575], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245, 97.80899999999593, 89.58599999999194], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307, 97.54299999999543, 60.9219999999863], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248, 97.17699999999505, 66.80599999998385], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926, 96.76199999999422, 67.78999999997991], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986, 96.52199999999375, 82.33899999999278], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888, 96.3909999999935, 71.39899999997591]}\n","8 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811, 35.40100000002162, 34.06000000002042, 1.6849999999984733], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627, 52.84699999999012, 41.694000000012494, 30.266000000020213], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937, 54.21699999998684, 93.52999999999261, 84.90599999997889], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392, 79.83299999997604, 75.76599999997384, 77.2869999999849], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881, 97.96499999999625, 96.77899999999474, 87.9079999999858], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406, 97.86199999999607, 88.68299999999454, 92.28599999998455], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353, 97.85099999999606, 97.72399999999575, 91.954999999992], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245, 97.80899999999593, 89.58599999999194, 95.01799999999261], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307, 97.54299999999543, 60.9219999999863, 93.2789999999855], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248, 97.17699999999505, 66.80599999998385, 92.95599999998916], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926, 96.76199999999422, 67.78999999997991, 94.69699999999082], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986, 96.52199999999375, 82.33899999999278, 94.51399999999036], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888, 96.3909999999935, 71.39899999997591, 94.02799999998955]}\n","9 scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811, 35.40100000002162, 34.06000000002042, 1.6849999999984733, 99.3999999999989], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627, 52.84699999999012, 41.694000000012494, 30.266000000020213, 75.39899999999345], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937, 54.21699999998684, 93.52999999999261, 84.90599999997889, 96.6269999999939], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392, 79.83299999997604, 75.76599999997384, 77.2869999999849, 92.829999999994], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881, 97.96499999999625, 96.77899999999474, 87.9079999999858, 88.15899999999192], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406, 97.86199999999607, 88.68299999999454, 92.28599999998455, 98.35499999999698], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353, 97.85099999999606, 97.72399999999575, 91.954999999992, 96.62599999999476], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245, 97.80899999999593, 89.58599999999194, 95.01799999999261, 13.024000000002086], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307, 97.54299999999543, 60.9219999999863, 93.2789999999855, 67.50699999998437], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248, 97.17699999999505, 66.80599999998385, 92.95599999998916, 91.81599999999416], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926, 96.76199999999422, 67.78999999997991, 94.69699999999082, 97.21999999999501], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986, 96.52199999999375, 82.33899999999278, 94.51399999999036, 95.6089999999953], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888, 96.3909999999935, 71.39899999997591, 94.02799999998955, 91.11999999999472]}\n","----\n","scale: {1.0: [98.889999999998, 34.25200000001467, 22.131000000015888, 16.343000000009525, -38.77599999996931, 6.923999999993811, 35.40100000002162, 34.06000000002042, 1.6849999999984733, 99.3999999999989], 1.5: [59.70399999999541, 30.9810000000133, 20.061000000014484, 37.81500000002205, -26.652999999978256, 8.104999999993627, 52.84699999999012, 41.694000000012494, 30.266000000020213, 75.39899999999345], 2.0: [-1.3090000000015707, 46.32800000000274, 30.92000000001287, 96.21299999999634, 22.46100000000407, 55.7879999999937, 54.21699999998684, 93.52999999999261, 84.90599999997889, 96.6269999999939], 2.5: [53.52999999999617, 61.68399999998656, 79.55099999998957, 97.71199999999578, 20.66500000002135, 73.49599999997392, 79.83299999997604, 75.76599999997384, 77.2869999999849, 92.829999999994], 3.0: [62.327999999992215, 56.54899999998754, 91.04799999999337, 96.1779999999939, 24.088000000014837, 91.6809999999881, 97.96499999999625, 96.77899999999474, 87.9079999999858, 88.15899999999192], 3.5: [-48.53100000000033, 79.5949999999876, 96.73199999999392, 97.55499999999557, 95.2639999999938, 96.67599999999406, 97.86199999999607, 88.68299999999454, 92.28599999998455, 98.35499999999698], 4.0: [97.69499999999574, 94.21599999999293, 95.62499999999186, 93.91499999999606, 97.1199999999948, 96.41199999999353, 97.85099999999606, 97.72399999999575, 91.954999999992, 96.62599999999476], 4.5: [97.55999999999551, 95.63199999999215, 89.58999999999064, 91.1659999999868, 97.19499999999495, 95.52899999999245, 97.80899999999593, 89.58599999999194, 95.01799999999261, 13.024000000002086], 5.0: [97.29799999999518, 95.90399999999259, 96.73999999999414, 88.59999999999128, 97.10499999999477, 96.13499999999307, 97.54299999999543, 60.9219999999863, 93.2789999999855, 67.50699999998437], 5.5: [68.2129999999829, 95.73799999999235, 95.35799999999172, 40.982000000020136, 80.26699999998732, 95.79099999999248, 97.17699999999505, 66.80599999998385, 92.95599999998916, 91.81599999999416], 6.0: [88.95899999998721, 95.66899999999218, 87.71799999998798, -17.640999999985688, 73.85499999999425, 93.9959999999926, 96.76199999999422, 67.78999999997991, 94.69699999999082, 97.21999999999501], 6.5: [72.05699999998856, 95.57899999999208, 92.69199999999032, -29.862999999974594, 48.38399999999803, 91.93599999998986, 96.52199999999375, 82.33899999999278, 94.51399999999036, 95.6089999999953], 7.0: [41.23500000001407, 95.51799999999189, 83.73299999998963, -14.4929999999792, 46.006000000011106, 89.93399999998888, 96.3909999999935, 71.39899999997591, 94.02799999998955, 91.11999999999472]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCngiUhykCBW"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VRa-o8mkMvW","executionInfo":{"status":"ok","timestamp":1612563120290,"user_tz":-60,"elapsed":1433061,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"3729bd47-9db4-47a1-b9a6-ee01dee13a5b"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [96.2159999999958], 0.05: [97.68499999999572], 0.1: [91.77099999999344], 0.15: [88.79299999998759], 0.2: [75.51999999997673], 0.25: [60.41299999998578], 0.3: [39.774000000013636], 0.35: [39.53000000001412], 0.4: [36.4880000000157]}\n","1 range: {0.0: [96.2159999999958, 88.38999999998856], 0.05: [97.68499999999572, 85.58399999999244], 0.1: [91.77099999999344, 89.83399999998994], 0.15: [88.79299999998759, 79.68799999999007], 0.2: [75.51999999997673, 92.9029999999914], 0.25: [60.41299999998578, 82.61599999999568], 0.3: [39.774000000013636, 73.92199999999082], 0.35: [39.53000000001412, 79.82399999998313], 0.4: [36.4880000000157, 75.38899999999657]}\n","2 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588]}\n","3 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292]}\n","4 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371]}\n","5 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688]}\n","6 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246, 97.86499999999606], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014, 97.84399999999603], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994, 97.83599999999602], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275, 91.80599999999343], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278, 84.41699999999639], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047, 87.41599999999498], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425, 75.4279999999893], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077, 72.65399999998222], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688, 63.691999999995275]}\n","7 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246, 97.86499999999606, 96.1889999999963], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014, 97.84399999999603, 96.1529999999964], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994, 97.83599999999602, 94.4489999999942], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275, 91.80599999999343, 78.19299999998572], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278, 84.41699999999639, 50.079999999984736], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047, 87.41599999999498, 55.86299999999184], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425, 75.4279999999893, 33.87000000000875], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077, 72.65399999998222, 38.093000000017334], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688, 63.691999999995275, 22.308000000012967]}\n","8 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246, 97.86499999999606, 96.1889999999963, 93.36499999999265], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014, 97.84399999999603, 96.1529999999964, 83.39899999998669], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994, 97.83599999999602, 94.4489999999942, 71.40399999997742], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275, 91.80599999999343, 78.19299999998572, 66.64499999998554], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278, 84.41699999999639, 50.079999999984736, 69.4009999999812], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047, 87.41599999999498, 55.86299999999184, 71.98699999997736], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425, 75.4279999999893, 33.87000000000875, 58.188000000003704], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077, 72.65399999998222, 38.093000000017334, 53.87899999997003], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688, 63.691999999995275, 22.308000000012967, 54.635999999984115]}\n","9 range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246, 97.86499999999606, 96.1889999999963, 93.36499999999265, 98.09999999999651], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014, 97.84399999999603, 96.1529999999964, 83.39899999998669, 89.24899999999212], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994, 97.83599999999602, 94.4489999999942, 71.40399999997742, 75.90799999998417], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275, 91.80599999999343, 78.19299999998572, 66.64499999998554, 72.79699999999616], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278, 84.41699999999639, 50.079999999984736, 69.4009999999812, 49.03299999998685], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047, 87.41599999999498, 55.86299999999184, 71.98699999997736, 44.73000000000755], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425, 75.4279999999893, 33.87000000000875, 58.188000000003704, 19.192000000009692], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077, 72.65399999998222, 38.093000000017334, 53.87899999997003, 15.000000000007066], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688, 63.691999999995275, 22.308000000012967, 54.635999999984115, 25.27500000001646]}\n","----\n","range: {0.0: [96.2159999999958, 88.38999999998856, 94.42299999999015, 93.97499999999165, 97.18599999999493, 96.11499999999246, 97.86499999999606, 96.1889999999963, 93.36499999999265, 98.09999999999651], 0.05: [97.68499999999572, 85.58399999999244, 95.72899999999288, 94.27999999999321, 97.13599999999485, 93.47399999999014, 97.84399999999603, 96.1529999999964, 83.39899999998669, 89.24899999999212], 0.1: [91.77099999999344, 89.83399999998994, 77.32799999999102, 86.9319999999847, 97.08799999999474, 73.90399999997994, 97.83599999999602, 94.4489999999942, 71.40399999997742, 75.90799999998417], 0.15: [88.79299999998759, 79.68799999999007, 56.46700000000783, 80.59699999999218, 68.31799999998819, 52.461999999974275, 91.80599999999343, 78.19299999998572, 66.64499999998554, 72.79699999999616], 0.2: [75.51999999997673, 92.9029999999914, 61.23499999998827, 84.88599999998803, 85.31799999998744, 37.82300000001278, 84.41699999999639, 50.079999999984736, 69.4009999999812, 49.03299999998685], 0.25: [60.41299999998578, 82.61599999999568, 45.25600000000011, 55.49999999999301, 66.21199999999206, 66.34799999998047, 87.41599999999498, 55.86299999999184, 71.98699999997736, 44.73000000000755], 0.3: [39.774000000013636, 73.92199999999082, 22.933000000011134, 30.6440000000187, 62.91699999998507, 38.29600000001425, 75.4279999999893, 33.87000000000875, 58.188000000003704, 19.192000000009692], 0.35: [39.53000000001412, 79.82399999998313, 7.0079999999979306, 20.55800000000758, 60.520999999991155, 39.71800000001077, 72.65399999998222, 38.093000000017334, 53.87899999997003, 15.000000000007066], 0.4: [36.4880000000157, 75.38899999999657, 12.75500000000588, 11.060000000003292, 43.77999999999371, 34.31600000001688, 63.691999999995275, 22.308000000012967, 54.635999999984115, 25.27500000001646]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDyjEh2LLmH1","executionInfo":{"status":"ok","timestamp":1611740498633,"user_tz":-60,"elapsed":1231,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"a3b425c0-6cde-44ec-c1fb-3e92480fb6cf"},"source":["print(\"\"\"100: -47.828000000000436\n","200: -50.000000000000426\n","300: -33.3530000000004\n","400: -37.87700000000041\n","500: -21.52900000000036\n","600: 28.95899999999979\n","700: 15.003999999999753\n","800: -36.5020000000004\n","900: 24.23599999999979\n","1000: 10.83599999999975\n","1100: 0.1569999999997134\n","1200: 7.607999999999724\n","1300: -16.606000000000343\n","1400: -0.6620000000002919\n","1500: -14.050000000000331\n","1600: -36.74600000000041\n","1700: -26.589000000000375\n","1800: -39.804000000000414\n","1900: -33.7270000000004\n","2000: -39.71300000000041\n","2100: -45.569000000000415\n","2200: -45.557000000000414\n","2300: -33.879000000000396\n","2400: -26.29500000000037\n","2500: -33.82300000000039\n","2600: 20.951999999999774\n","2700: -5.688000000000309\n","2800: 4.562999999999722\n","2900: 65.12499999999991\n","3000: 78.82499999999996\n","3100: 90.40100000000001\n","Solved after 3100 episodes!\"\"\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100: -47.828000000000436\n","200: -50.000000000000426\n","300: -33.3530000000004\n","400: -37.87700000000041\n","500: -21.52900000000036\n","600: 28.95899999999979\n","700: 15.003999999999753\n","800: -36.5020000000004\n","900: 24.23599999999979\n","1000: 10.83599999999975\n","1100: 0.1569999999997134\n","1200: 7.607999999999724\n","1300: -16.606000000000343\n","1400: -0.6620000000002919\n","1500: -14.050000000000331\n","1600: -36.74600000000041\n","1700: -26.589000000000375\n","1800: -39.804000000000414\n","1900: -33.7270000000004\n","2000: -39.71300000000041\n","2100: -45.569000000000415\n","2200: -45.557000000000414\n","2300: -33.879000000000396\n","2400: -26.29500000000037\n","2500: -33.82300000000039\n","2600: 20.951999999999774\n","2700: -5.688000000000309\n","2800: 4.562999999999722\n","2900: 65.12499999999991\n","3000: 78.82499999999996\n","3100: 90.40100000000001\n","Solved after 3100 episodes!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fXVuyJCHOES6"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmuu11RLSSo1","executionInfo":{"status":"ok","timestamp":1612798729883,"user_tz":-60,"elapsed":63303,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"d32abe58-f333-4cba-d4c1-d7b0a59209b7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 10:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","torch.save(episodes, \"PointFall_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RqCEeJ2WSZ3F"},"source":[""],"execution_count":null,"outputs":[]}]}