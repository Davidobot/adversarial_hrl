{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointpush_freeze.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1618854856127,"user_tz":-60,"elapsed":33337,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"8c019682-e959-4951-f23a-9b4ce1cf4f03"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_push.py\" ."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l"},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/exp_point_push/{name}\" \n","\n","    torch.save({\n","      'meta_controller': {\n","          'critic': model.meta_controller.critic.state_dict(),\n","          'actor': model.meta_controller.actor.state_dict(),\n","      },\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/exp_point_push/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs"},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs"},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0"},"source":["from point_push import PointPushEnv \n","env = NormalizedEnv(PointPushEnv(4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4"},"source":["def plot_durations(episode_durations, goals_done):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*goals_done)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Goals done')\n","    \n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6"},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9"},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA"},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD"},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG"},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 10000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8"},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        self.meta_controller = TD3(nb_states, len(self.goal_dim), True).to(device)\n","        self.max_goal_dist = torch.from_numpy(np.array([2.5, 2.5])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0., 1.])).to(device)\n","        #self.meta_controller.depsilon = 1.0 / 10000\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.update_policy(self.off_policy_corrections)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    # correct goals to allow for use in experience replay\n","    def off_policy_corrections(self, sgoals, states, actions, candidate_goals=8):\n","        first_s = [s[0] for s in states] # First x\n","        last_s = [s[-1] for s in states] # Last x\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # diff = 1\n","        diff_goal = (np.array(last_s) - np.array(first_s))[:, np.newaxis, :self.goal_dimen]\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # original = 1\n","        # random = candidate_goals\n","        scale = self.max_goal_dist.cpu().numpy()\n","        original_goal = np.array(sgoals)[:, np.newaxis, :]\n","        random_goals = np.random.normal(loc=diff_goal, scale=.5*scale,\n","                                        size=(BATCH_SIZE, candidate_goals, original_goal.shape[-1]))\n","        random_goals = random_goals.clip(-scale, scale)\n","\n","        # Shape: (batch_size, 10, subgoal_dim)\n","        candidates = np.concatenate([original_goal, diff_goal, random_goals], axis=1)\n","        #states = np.array(states)[:, :-1, :]\n","        actions = np.array(actions)\n","        seq_len = len(states[0])\n","\n","        # For ease\n","        new_batch_sz = seq_len * BATCH_SIZE\n","        action_dim = actions[0][0].shape\n","        obs_dim = states[0][0].shape\n","        ncands = candidates.shape[1]\n","\n","        true_actions = actions.reshape((new_batch_sz,) + action_dim)\n","        observations = states.reshape((new_batch_sz,) + obs_dim)\n","        goal_shape = (new_batch_sz, self.goal_dimen)\n","        # observations = get_obs_tensor(observations, sg_corrections=True)\n","\n","        # batched_candidates = np.tile(candidates, [seq_len, 1, 1])\n","        # batched_candidates = batched_candidates.transpose(1, 0, 2)\n","\n","        policy_actions = np.zeros((ncands, new_batch_sz) + action_dim)\n","\n","        observations = torch.from_numpy(observations).to(device)\n","        for c in range(ncands):\n","            subgoal = candidates[:,c]\n","            candidate = (subgoal + states[:, 0, :self.goal_dimen])[:, None] - states[:, :, :self.goal_dimen]\n","            candidate = candidate.reshape(*goal_shape)\n","            policy_actions[c] = self.controller.actor(torch.cat([observations, torch.from_numpy(candidate).to(device)], 1).float()).detach().cpu().numpy()\n","\n","        difference = (policy_actions - true_actions)\n","        difference = np.where(difference != -np.inf, difference, 0)\n","        difference = difference.reshape((ncands, BATCH_SIZE, seq_len) + action_dim).transpose(1, 0, 2, 3)\n","\n","        logprob = -0.5*np.sum(np.linalg.norm(difference, axis=-1)**2, axis=-1)\n","        max_indices = np.argmax(logprob, axis=-1)\n","\n","        return torch.from_numpy(candidates[np.arange(BATCH_SIZE), max_indices]).to(device).float()\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done, state_seq, action_seq)\n","\n","    def select_goal(self, s_t, warmup, decay_epsilon):\n","        return self.meta_controller.select_action(s_t, warmup, decay_epsilon) * self.max_goal_dist + self.goal_offset\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 6\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 8000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    freeze_ctrl = False\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                if not freeze_ctrl:\n","                    agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup and not freeze_ctrl:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 70 and not freeze_ctrl:\n","                print(f\"Freezing controller at episode {i_episode}!\")\n","                freeze_ctrl = True\n","                agent.controller.eval()\n","                agent.controller.is_training = False\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_freeze_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data - eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    #g_state = torch.tensor(g_state, requires_grad=True)\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    if targeted:\n","        # initial forward pass\n","        goal = agent.meta_controller.actor(g_state)\n","        goal = torch.clamp(goal, -1., 1.)\n","\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = agent.meta_controller.critic.Q1(g_state, agent.meta_controller.actor(g_state)).mean()\n","\n","    agent.meta_controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal = agent.select_goal(og_state, False, False)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od4IvIuPuNlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618881611935,"user_tz":-60,"elapsed":26719143,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"d7800849-6241-4b76-d942-55fdd6e43a5a"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","\"\"\"\n","i = 6\n","while i < 7:\n","    agent = train_model()\n","    #agent = HIRO(n_observations, n_actions).to(device)\n","    #load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\"\"\"\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [93.19899999998853], 0.02: [-6.756000000003089], 0.04: [-28.080999999977436], 0.06: [3.2919999999981338], 0.08: [-18.63499999998598], 0.1: [-31.7009999999735], 0.12: [-26.588999999977915], 0.14: [-44.54499999998538], 0.16: [-40.05699999996838], 0.18: [-44.49299999998746], 0.2: [-37.61399999996956]}, 'action': {0.0: [90.14699999999075], 0.02: [90.27299999998557], 0.04: [-0.3600000000010772], 0.06: [-48.52700000000145], 0.08: [-45.59299999999319], 0.1: [-35.993999999977085], 0.12: [-50.00000000000659], 0.14: [-47.23999999999472], 0.16: [-50.00000000000659], 0.18: [-48.552000000000405], 0.2: [-50.00000000000659]}}\n","0 fgsm (t): {'goal': {0.0: [94.12299999998937], 0.02: [-50.00000000000659], 0.04: [27.760000000020092], 0.06: [-26.59999999998081], 0.08: [-47.18699999999453], 0.1: [-48.615000000000634], 0.12: [-25.9829999999806], 0.14: [-41.18299999996864], 0.16: [-48.83300000000143], 0.18: [-48.77300000000121], 0.2: [-48.69200000000092]}, 'action': {0.0: [90.23899999998504], 0.02: [70.96799999997414], 0.04: [6.7019999999965885], 0.06: [15.715000000012099], 0.08: [-36.28699999997455], 0.1: [-17.328999999985495], 0.12: [-45.990999999990166], 0.14: [-47.35299999999627], 0.16: [-44.9849999999847], 0.18: [-44.66999999998416], 0.2: [-47.517999999996874]}}\n","1 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912], 0.02: [-6.756000000003089, -9.212000000001588], 0.04: [-28.080999999977436, -26.277999999974853], 0.06: [3.2919999999981338, -40.91299999996965], 0.08: [-18.63499999998598, -41.32599999997071], 0.1: [-31.7009999999735, -47.21699999999919], 0.12: [-26.588999999977915, -43.01899999998005], 0.14: [-44.54499999998538, -45.86899999999088], 0.16: [-40.05699999996838, -48.53300000000147], 0.18: [-44.49299999998746, -42.96099999997643], 0.2: [-37.61399999996956, -45.82099999998979]}, 'action': {0.0: [90.14699999999075, 82.37299999998405], 0.02: [90.27299999998557, -50.00000000000659], 0.04: [-0.3600000000010772, -50.00000000000659], 0.06: [-48.52700000000145, -50.00000000000659], 0.08: [-45.59299999999319, -50.00000000000659], 0.1: [-35.993999999977085, -50.00000000000659], 0.12: [-50.00000000000659, -50.00000000000659], 0.14: [-47.23999999999472, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659], 0.18: [-48.552000000000405, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659]}}\n","1 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478], 0.02: [-50.00000000000659, -46.29599999999038], 0.04: [27.760000000020092, -48.5370000000049], 0.06: [-26.59999999998081, -50.00000000000659], 0.08: [-47.18699999999453, -50.00000000000659], 0.1: [-48.615000000000634, -50.00000000000659], 0.12: [-25.9829999999806, -36.81799999997131], 0.14: [-41.18299999996864, -42.751999999981535], 0.16: [-48.83300000000143, -42.91399999998258], 0.18: [-48.77300000000121, -45.83099999999151], 0.2: [-48.69200000000092, -42.98799999997852]}, 'action': {0.0: [90.23899999998504, 77.58099999998711], 0.02: [70.96799999997414, -50.00000000000659], 0.04: [6.7019999999965885, -50.00000000000659], 0.06: [15.715000000012099, -50.00000000000659], 0.08: [-36.28699999997455, -50.00000000000659], 0.1: [-17.328999999985495, -50.00000000000659], 0.12: [-45.990999999990166, -44.78099999998769], 0.14: [-47.35299999999627, -50.00000000000659], 0.16: [-44.9849999999847, -50.00000000000659], 0.18: [-44.66999999998416, -50.00000000000659], 0.2: [-47.517999999996874, -43.36399999998074]}}\n","2 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093]}}\n","2 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434]}}\n","3 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492, 93.27599999999264], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953, 40.63000000001528], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132, -45.864999999994495], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754, -45.67299999998925], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708, -50.00000000000659], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797, -43.11399999997996], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723, -34.67999999997547], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832, -20.02199999998497], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087, -35.52599999996864], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882, -45.99899999999044], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224, -34.57599999997424]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611, 93.29299999999462], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886, 30.13600000000689], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659, -35.540999999975305], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659, -44.31199999998766], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659, -7.986000000001044], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659, -41.443999999974125], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.79399999997032], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093, -46.00699999999502]}}\n","3 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618, 96.26999999999411], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177, 4.244999999997593], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364, -41.542999999973006], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099, -25.883999999975995], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055, -33.16699999997317], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066, -4.297000000004139], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659, -15.481999999984852], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368, -48.72000000000104], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659, -50.00000000000659], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659, -50.00000000000659], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659, -50.00000000000659]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613, 97.67099999999577], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659, -0.7700000000009528], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547, -35.57599999997064], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812, -38.76799999996931], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175, -40.5039999999705], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659, -40.5529999999677], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659, -50.00000000000659], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007, -48.841000000002595], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659, -38.39099999997127], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834, -46.28899999999604], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434, -43.64399999997892]}}\n","4 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492, 93.27599999999264, 97.96999999999623], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953, 40.63000000001528, 74.90099999997989], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132, -45.864999999994495, 61.57899999999898], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754, -45.67299999998925, -11.280000000003245], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708, -50.00000000000659, -50.00000000000659], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797, -43.11399999997996, -50.00000000000659], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723, -34.67999999997547, -50.00000000000659], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832, -20.02199999998497, -50.00000000000659], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087, -35.52599999996864, -50.00000000000659], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882, -45.99899999999044, -50.00000000000659], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224, -34.57599999997424, -50.00000000000659]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611, 93.29299999999462, 97.88199999999604], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886, 30.13600000000689, 87.63999999999338], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659, -35.540999999975305, 7.975999999997766], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659, -44.31199999998766, -47.08699999999417], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659, -7.986000000001044, -48.54400000000123], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659, -41.443999999974125, -50.00000000000659], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.79399999997032, -50.00000000000659], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093, -46.00699999999502, -47.28599999999489]}}\n","4 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618, 96.26999999999411, 97.96199999999624], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177, 4.244999999997593, 98.51999999999718], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364, -41.542999999973006, -10.193000000004435], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099, -25.883999999975995, -17.8179999999811], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055, -33.16699999997317, -42.73799999997676], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066, -4.297000000004139, -50.00000000000659], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659, -15.481999999984852, -50.00000000000659], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368, -48.72000000000104, -50.00000000000659], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659, -50.00000000000659, -41.30699999997064], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659, -50.00000000000659, -48.55000000000126]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613, 97.67099999999577, 97.89399999999608], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659, -0.7700000000009528, -7.722000000003087], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547, -35.57599999997064, -50.00000000000659], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812, -38.76799999996931, -50.00000000000659], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175, -40.5039999999705, -50.00000000000659], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659, -40.5529999999677, -50.00000000000659], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007, -48.841000000002595, -50.00000000000659], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659, -38.39099999997127, -41.98299999997196], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834, -46.28899999999604, -47.388999999995264], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434, -43.64399999997892, -35.781999999971816]}}\n","5 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492, 93.27599999999264, 97.96999999999623, 75.83499999998776], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953, 40.63000000001528, 74.90099999997989, -50.00000000000659], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132, -45.864999999994495, 61.57899999999898, -50.00000000000659], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754, -45.67299999998925, -11.280000000003245, -50.00000000000659], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797, -43.11399999997996, -50.00000000000659, -50.00000000000659], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723, -34.67999999997547, -50.00000000000659, -50.00000000000659], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832, -20.02199999998497, -50.00000000000659, -47.33699999999707], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087, -35.52599999996864, -50.00000000000659, -39.15799999997347], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882, -45.99899999999044, -50.00000000000659, -41.95299999997465], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224, -34.57599999997424, -50.00000000000659, -48.66600000000082]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611, 93.29299999999462, 97.88199999999604, 76.99499999997951], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886, 30.13600000000689, 87.63999999999338, -37.69199999997076], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659, -35.540999999975305, 7.975999999997766, -50.00000000000659], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659, -44.31199999998766, -47.08699999999417, -50.00000000000659], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659, -7.986000000001044, -48.54400000000123, -37.24699999997403], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659, -41.443999999974125, -50.00000000000659, 1.4949999999980719], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.79399999997032, -50.00000000000659, -0.40200000000158204], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 1.3879999999948975], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 5.379999999999605], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 36.36000000002031], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093, -46.00699999999502, -47.28599999999489, 37.76600000001426]}}\n","5 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618, 96.26999999999411, 97.96199999999624, 72.179999999982], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177, 4.244999999997593, 98.51999999999718, -50.00000000000659], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364, -41.542999999973006, -10.193000000004435, -50.00000000000659], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099, -25.883999999975995, -17.8179999999811, -50.00000000000659], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055, -33.16699999997317, -42.73799999997676, -42.82699999997879], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066, -4.297000000004139, -50.00000000000659, -50.00000000000659], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659, -15.481999999984852, -50.00000000000659, -45.77199999998847], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368, -48.72000000000104, -50.00000000000659, -50.00000000000659], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659, -50.00000000000659, -41.30699999997064, -50.00000000000659], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659, -50.00000000000659, -50.00000000000659, -21.54899999997857], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659, -50.00000000000659, -48.55000000000126, -20.95299999998252]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613, 97.67099999999577, 97.89399999999608, 76.4019999999748], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659, -0.7700000000009528, -7.722000000003087, -50.00000000000659], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547, -35.57599999997064, -50.00000000000659, -50.00000000000659], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812, -38.76799999996931, -50.00000000000659, -50.00000000000659], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175, -40.5039999999705, -50.00000000000659, -48.75200000000227], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659, -40.5529999999677, -50.00000000000659, -50.00000000000659], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659, -50.00000000000659, -50.00000000000659, -7.591000000003323], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007, -48.841000000002595, -50.00000000000659, 12.856000000002911], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659, -38.39099999997127, -41.98299999997196, 8.932999999997811], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834, -46.28899999999604, -47.388999999995264, -50.00000000000659], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434, -43.64399999997892, -35.781999999971816, -50.00000000000659]}}\n","6 fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492, 93.27599999999264, 97.96999999999623, 75.83499999998776, 95.08799999999205], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953, 40.63000000001528, 74.90099999997989, -50.00000000000659, -48.60100000000059], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132, -45.864999999994495, 61.57899999999898, -50.00000000000659, -50.00000000000659], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754, -45.67299999998925, -11.280000000003245, -50.00000000000659, 4.443999999996424], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708, -50.00000000000659, -50.00000000000659, -50.00000000000659, -10.15700000000636], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797, -43.11399999997996, -50.00000000000659, -50.00000000000659, 2.7239999999995295], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723, -34.67999999997547, -50.00000000000659, -50.00000000000659, 4.62100000000034], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832, -20.02199999998497, -50.00000000000659, -47.33699999999707, 7.273999999998347], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087, -35.52599999996864, -50.00000000000659, -39.15799999997347, -17.92099999999248], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882, -45.99899999999044, -50.00000000000659, -41.95299999997465, -8.274000000006414], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224, -34.57599999997424, -50.00000000000659, -48.66600000000082, -37.37699999996967]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611, 93.29299999999462, 97.88199999999604, 76.99499999997951, 95.12199999999049], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886, 30.13600000000689, 87.63999999999338, -37.69199999997076, -50.00000000000659], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659, -35.540999999975305, 7.975999999997766, -50.00000000000659, -50.00000000000659], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659, -44.31199999998766, -47.08699999999417, -50.00000000000659, -50.00000000000659], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659, -7.986000000001044, -48.54400000000123, -37.24699999997403, -48.91400000000286], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659, -41.443999999974125, -50.00000000000659, 1.4949999999980719, -48.61100000000062], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.79399999997032, -50.00000000000659, -0.40200000000158204, -45.78499999998852], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 1.3879999999948975, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 5.379999999999605, -47.194999999994565], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 36.36000000002031, -16.47599999999374], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093, -46.00699999999502, -47.28599999999489, 37.76600000001426, -42.67099999998015]}}\n","6 fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618, 96.26999999999411, 97.96199999999624, 72.179999999982, 94.91799999999166], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177, 4.244999999997593, 98.51999999999718, -50.00000000000659, -36.35599999997107], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364, -41.542999999973006, -10.193000000004435, -50.00000000000659, -44.88199999998433], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099, -25.883999999975995, -17.8179999999811, -50.00000000000659, -14.85899999999843], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055, -33.16699999997317, -42.73799999997676, -42.82699999997879, 9.596999999996596], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066, -4.297000000004139, -50.00000000000659, -50.00000000000659, 18.71000000000199], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659, -15.481999999984852, -50.00000000000659, -45.77199999998847, 11.740999999997175], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368, -48.72000000000104, -50.00000000000659, -50.00000000000659, -13.179999999993136], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659, -50.00000000000659, -41.30699999997064, -50.00000000000659, -25.61599999998225], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659, -50.00000000000659, -50.00000000000659, -21.54899999997857, -35.3659999999721], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659, -50.00000000000659, -48.55000000000126, -20.95299999998252, -10.256000000005123]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613, 97.67099999999577, 97.89399999999608, 76.4019999999748, 94.80699999999025], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659, -0.7700000000009528, -7.722000000003087, -50.00000000000659, -1.641000000002162], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547, -35.57599999997064, -50.00000000000659, -50.00000000000659, -44.314999999984536], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812, -38.76799999996931, -50.00000000000659, -50.00000000000659, -44.49899999998748], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175, -40.5039999999705, -50.00000000000659, -48.75200000000227, -38.97299999997164], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659, -40.5529999999677, -50.00000000000659, -50.00000000000659, -41.895999999981875], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659, -50.00000000000659, -50.00000000000659, -7.591000000003323, -16.463999999994893], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007, -48.841000000002595, -50.00000000000659, 12.856000000002911, -18.96099999997842], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659, -38.39099999997127, -41.98299999997196, 8.932999999997811, 9.427999999996082], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834, -46.28899999999604, -47.388999999995264, -50.00000000000659, -20.24299999998352], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434, -43.64399999997892, -35.781999999971816, -50.00000000000659, -22.368999999983945]}}\n","----\n","fgsm (ut): {'goal': {0.0: [93.19899999998853, 80.40899999998912, 91.15999999998492, 93.27599999999264, 97.96999999999623, 75.83499999998776, 95.08799999999205], 0.02: [-6.756000000003089, -9.212000000001588, 83.3019999999953, 40.63000000001528, 74.90099999997989, -50.00000000000659, -48.60100000000059], 0.04: [-28.080999999977436, -26.277999999974853, 90.87299999999132, -45.864999999994495, 61.57899999999898, -50.00000000000659, -50.00000000000659], 0.06: [3.2919999999981338, -40.91299999996965, -42.6779999999754, -45.67299999998925, -11.280000000003245, -50.00000000000659, 4.443999999996424], 0.08: [-18.63499999998598, -41.32599999997071, -38.3389999999708, -50.00000000000659, -50.00000000000659, -50.00000000000659, -10.15700000000636], 0.1: [-31.7009999999735, -47.21699999999919, -31.68699999997797, -43.11399999997996, -50.00000000000659, -50.00000000000659, 2.7239999999995295], 0.12: [-26.588999999977915, -43.01899999998005, -33.3489999999723, -34.67999999997547, -50.00000000000659, -50.00000000000659, 4.62100000000034], 0.14: [-44.54499999998538, -45.86899999999088, -27.76299999997832, -20.02199999998497, -50.00000000000659, -47.33699999999707, 7.273999999998347], 0.16: [-40.05699999996838, -48.53300000000147, -37.57199999997087, -35.52599999996864, -50.00000000000659, -39.15799999997347, -17.92099999999248], 0.18: [-44.49299999998746, -42.96099999997643, -40.57599999996882, -45.99899999999044, -50.00000000000659, -41.95299999997465, -8.274000000006414], 0.2: [-37.61399999996956, -45.82099999998979, -37.029999999971224, -34.57599999997424, -50.00000000000659, -48.66600000000082, -37.37699999996967]}, 'action': {0.0: [90.14699999999075, 82.37299999998405, 91.81999999998611, 93.29299999999462, 97.88199999999604, 76.99499999997951, 95.12199999999049], 0.02: [90.27299999998557, -50.00000000000659, 76.9379999999886, 30.13600000000689, 87.63999999999338, -37.69199999997076, -50.00000000000659], 0.04: [-0.3600000000010772, -50.00000000000659, -50.00000000000659, -35.540999999975305, 7.975999999997766, -50.00000000000659, -50.00000000000659], 0.06: [-48.52700000000145, -50.00000000000659, -50.00000000000659, -44.31199999998766, -47.08699999999417, -50.00000000000659, -50.00000000000659], 0.08: [-45.59299999999319, -50.00000000000659, -50.00000000000659, -7.986000000001044, -48.54400000000123, -37.24699999997403, -48.91400000000286], 0.1: [-35.993999999977085, -50.00000000000659, -50.00000000000659, -41.443999999974125, -50.00000000000659, 1.4949999999980719, -48.61100000000062], 0.12: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -39.79399999997032, -50.00000000000659, -0.40200000000158204, -45.78499999998852], 0.14: [-47.23999999999472, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 1.3879999999948975, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 5.379999999999605, -47.194999999994565], 0.18: [-48.552000000000405, -50.00000000000659, -50.00000000000659, -50.00000000000659, -50.00000000000659, 36.36000000002031, -16.47599999999374], 0.2: [-50.00000000000659, -50.00000000000659, -48.69700000000093, -46.00699999999502, -47.28599999999489, 37.76600000001426, -42.67099999998015]}}\n","fgsm (t): {'goal': {0.0: [94.12299999998937, 80.78199999998478, 91.44799999998618, 96.26999999999411, 97.96199999999624, 72.179999999982, 94.91799999999166], 0.02: [-50.00000000000659, -46.29599999999038, -6.197000000007177, 4.244999999997593, 98.51999999999718, -50.00000000000659, -36.35599999997107], 0.04: [27.760000000020092, -48.5370000000049, -44.37999999998364, -41.542999999973006, -10.193000000004435, -50.00000000000659, -44.88199999998433], 0.06: [-26.59999999998081, -50.00000000000659, -48.71200000000099, -25.883999999975995, -17.8179999999811, -50.00000000000659, -14.85899999999843], 0.08: [-47.18699999999453, -50.00000000000659, -38.68199999997055, -33.16699999997317, -42.73799999997676, -42.82699999997879, 9.596999999996596], 0.1: [-48.615000000000634, -50.00000000000659, -41.680999999972066, -4.297000000004139, -50.00000000000659, -50.00000000000659, 18.71000000000199], 0.12: [-25.9829999999806, -36.81799999997131, -50.00000000000659, -15.481999999984852, -50.00000000000659, -45.77199999998847, 11.740999999997175], 0.14: [-41.18299999996864, -42.751999999981535, -48.62400000000368, -48.72000000000104, -50.00000000000659, -50.00000000000659, -13.179999999993136], 0.16: [-48.83300000000143, -42.91399999998258, -50.00000000000659, -50.00000000000659, -41.30699999997064, -50.00000000000659, -25.61599999998225], 0.18: [-48.77300000000121, -45.83099999999151, -50.00000000000659, -50.00000000000659, -50.00000000000659, -21.54899999997857, -35.3659999999721], 0.2: [-48.69200000000092, -42.98799999997852, -50.00000000000659, -50.00000000000659, -48.55000000000126, -20.95299999998252, -10.256000000005123]}, 'action': {0.0: [90.23899999998504, 77.58099999998711, 91.22599999998613, 97.67099999999577, 97.89399999999608, 76.4019999999748, 94.80699999999025], 0.02: [70.96799999997414, -50.00000000000659, -50.00000000000659, -0.7700000000009528, -7.722000000003087, -50.00000000000659, -1.641000000002162], 0.04: [6.7019999999965885, -50.00000000000659, -47.13299999999547, -35.57599999997064, -50.00000000000659, -50.00000000000659, -44.314999999984536], 0.06: [15.715000000012099, -50.00000000000659, -42.9719999999812, -38.76799999996931, -50.00000000000659, -50.00000000000659, -44.49899999998748], 0.08: [-36.28699999997455, -50.00000000000659, -47.511000000001175, -40.5039999999705, -50.00000000000659, -48.75200000000227, -38.97299999997164], 0.1: [-17.328999999985495, -50.00000000000659, -50.00000000000659, -40.5529999999677, -50.00000000000659, -50.00000000000659, -41.895999999981875], 0.12: [-45.990999999990166, -44.78099999998769, -50.00000000000659, -50.00000000000659, -50.00000000000659, -7.591000000003323, -16.463999999994893], 0.14: [-47.35299999999627, -50.00000000000659, -48.6330000000007, -48.841000000002595, -50.00000000000659, 12.856000000002911, -18.96099999997842], 0.16: [-44.9849999999847, -50.00000000000659, -50.00000000000659, -38.39099999997127, -41.98299999997196, 8.932999999997811, 9.427999999996082], 0.18: [-44.66999999998416, -50.00000000000659, -44.323999999987834, -46.28899999999604, -47.388999999995264, -50.00000000000659, -20.24299999998352], 0.2: [-47.517999999996874, -43.36399999998074, -41.66399999997434, -43.64399999997892, -35.781999999971816, -50.00000000000659, -22.368999999983945]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6hvBzNIw4NIi"},"source":["Solved after 3632 episodes!\n","0 noise_hrl: {'both': {0.0: [91.6169999999889], 0.05: [4.3909999999943965], 0.1: [-13.06200000000104], 0.15: [-16.02699999998649], 0.2: [-30.081999999977683], 0.25: [-38.66399999996852], 0.3: [-32.41799999997416], 0.35: [-38.8839999999724], 0.4: [-33.40199999997421], 0.45: [-29.158999999978697], 0.5: [-35.521999999973595]}, 'action_only': {0.0: [88.34199999998839], 0.05: [2.7139999999962616], 0.1: [11.539000000002018], 0.15: [-10.021000000006067], 0.2: [-22.952999999983728], 0.25: [-35.94799999996916], 0.3: [-32.0839999999746], 0.35: [-40.47399999996869], 0.4: [-39.056999999971566], 0.45: [-47.45400000000055], 0.5: [-39.488999999971696]}, 'goal_only': {0.0: [91.6299999999885], 0.05: [22.151000000015497], 0.1: [17.961000000003864], 0.15: [14.963000000005446], 0.2: [0.23599999999973187], 0.25: [-8.852000000002521], 0.3: [-3.4670000000002803], 0.35: [-5.9900000000062645], 0.4: [-7.506000000005943], 0.45: [-23.24399999998162], 0.5: [-1.7720000000020357]}, 'both_same': {0.0: [93.54999999998726], 0.05: [1.4820000000008142], 0.1: [-21.385999999983447], 0.15: [-18.7449999999904], 0.2: [-25.991999999980667], 0.25: [-30.60899999997152], 0.3: [-35.08999999997221], 0.35: [-36.249999999972225], 0.4: [-39.40599999996959], 0.45: [-41.88999999997276], 0.5: [-32.59699999997149]}}\n","Freezing controller at episode 818!\n","Solved after 1101 episodes!\n","1\n","Freezing controller at episode 1851!\n","Solved after 1867 episodes!\n","2\n","Freezing controller at episode 2193!\n","Solved after 2231 episodes!\n","3\n","Freezing controller at episode 399!\n","Solved after 413 episodes!\n","4\n","Freezing controller at episode 1418!\n","Solved after 1926 episodes!\n","5 noise_hrl: {'both': {0.0: [73.07799999999051, 91.12699999998574, 93.23699999999299, 97.88399999999612, 72.61699999997971], 0.05: [-1.4370000000000809, 22.928000000011767, 20.556000000019484, 46.513000000004055, 12.59500000000035], 0.1: [-16.576999999993337, -4.8370000000012805, -0.22400000000150652, -11.547000000004939, -9.440000000003288], 0.15: [-34.16499999997281, -23.35899999998167, -2.0829999999993443, -19.041999999984448, -5.232000000006845], 0.2: [-18.10399999998752, -7.636000000004905, -8.895000000001414, -35.36199999997311, -8.283000000004524], 0.25: [-29.00699999997538, -22.17299999997951, -21.979999999973998, -35.56299999997133, -27.31599999997992], 0.3: [-25.716999999974107, -29.78999999997701, -23.74399999997638, -34.34899999997415, -21.546999999986678], 0.35: [-25.065999999974878, -20.34699999998387, -20.418999999985818, -40.382999999971155, -22.086999999975557], 0.4: [-22.101999999978215, -25.028999999972047, -24.482999999985125, -43.27099999997954, -35.82999999997318], 0.45: [-33.67199999997055, -20.40699999998481, -22.83499999998473, -28.209999999976606, -29.513999999977692], 0.5: [-27.229999999976208, -17.756999999992992, -22.977999999978135, -33.83199999997309, -22.69999999998444]}, 'action_only': {0.0: [77.74199999998348, 91.68599999998558, 93.20699999999277, 97.96599999999619, 62.72999999997741], 0.05: [61.660999999992576, 29.517000000018335, 26.582000000009543, 50.96199999998754, 56.26499999999746], 0.1: [20.70200000000523, 3.783999999998489, 13.109999999997367, 0.4039999999944049, 15.109000000001236], 0.15: [3.518999999999521, -28.368999999977795, -3.6650000000024208, -19.10699999999364, -10.774000000004458], 0.2: [-3.9360000000035824, -24.239999999979972, -20.060999999983295, -39.67399999997117, -11.026999999993569], 0.25: [-11.634999999988542, -35.30999999997225, -7.630000000005308, -36.81499999997567, -11.171999999995561], 0.3: [-7.036000000005539, -35.132999999972846, -13.861999999992886, -38.257999999971155, -26.96899999998037], 0.35: [-11.58700000000008, -31.389999999971806, -17.658999999991593, -42.72799999997672, -33.03599999997113], 0.4: [-7.849000000007347, -32.416999999977605, -9.038000000003201, -32.59699999997267, -31.683999999974148], 0.45: [-10.72200000000302, -27.364999999974017, -13.438999999988352, -38.69599999997198, -36.34199999996994], 0.5: [-10.289000000005988, -22.340999999983833, -22.43099999997751, -40.50899999997002, -42.867999999976995]}, 'goal_only': {0.0: [70.39199999998719, 91.14199999998573, 96.22899999999639, 97.87799999999618, 85.44499999998445], 0.05: [8.844999999995483, 79.78399999999209, 34.23300000001106, 96.71199999999719, 42.50600000001279], 0.1: [-4.663000000000556, 80.61399999998515, 5.847999999999738, 77.62199999998337, 44.51600000000882], 0.15: [-12.926999999998687, 58.56499999999761, 24.348000000006696, 35.69300000000811, 32.499000000012146], 0.2: [-26.702999999979895, 63.43799999999815, 21.42300000001203, 15.267000000022188, 18.36000000000339], 0.25: [-25.80099999997772, 60.99699999998591, 2.610999999998728, 11.950999999999272, 13.556000000013722], 0.3: [-28.071999999977315, 47.1410000000063, 6.8639999999980645, 4.66399999999656, -10.583999999984155], 0.35: [-26.608999999973566, 31.715000000013177, -0.5310000000006809, 1.298999999998083, -6.764000000003009], 0.4: [-20.850999999981365, 15.6260000000172, -8.113000000001426, 4.217999999999579, -1.4490000000012628], 0.45: [-27.064999999981964, 27.18200000000728, 3.651999999995551, 12.707999999997543, -9.920000000004519], 0.5: [-12.192000000001526, 12.636999999997197, 1.4409999999987482, -8.950999999998828, -16.04299999997997]}, 'both_same': {0.0: [73.70099999998871, 91.50399999998581, 96.07899999999367, 97.88499999999608, 65.91399999999459], 0.05: [-16.507999999995835, 18.149000000009472, 13.518000000003415, 36.07800000000847, 11.356999999999402], 0.1: [-23.158999999979205, -8.30900000000367, 14.27200000000863, -11.534999999990227, -6.405000000003424], 0.15: [-24.353999999983117, -25.11799999998029, -19.565999999979027, -39.69499999997389, -12.24400000000007], 0.2: [-25.655999999980715, -13.097000000000808, -8.430999999998669, -32.445999999972805, -12.980999999990939], 0.25: [-17.30699999999501, -17.93499999998301, -14.873999999993952, -29.845999999972346, -17.456999999987996], 0.3: [-9.77700000000297, -14.690999999996418, -10.154000000006924, -39.99299999996935, -9.183000000003394], 0.35: [-16.008999999995144, -20.777999999978658, -13.295999999997107, -26.78399999997284, -26.49799999998337], 0.4: [-9.854000000000065, -14.0239999999864, -12.945999999997605, -26.194999999975067, -29.11999999997717], 0.45: [-23.475999999979486, -17.211999999991168, -15.182999999986464, -31.572999999974055, -35.669999999971274], 0.5: [-16.155999999983067, -18.711999999990926, -12.45599999999823, -18.253999999996022, -33.86599999997265]}}\n","Freezing controller at episode 1173!\n","Solved after 1287 episodes!\n","6 noise_hrl: {'both': {0.0: [95.25999999999117], 0.05: [37.68600000001403], 0.1: [8.468999999997664], 0.15: [3.865999999996545], 0.2: [-4.149000000003081], 0.25: [-5.437000000004015], 0.3: [-8.841000000005378], 0.35: [-11.0690000000052], 0.4: [-11.321000000002034], 0.45: [-16.928999999989816], 0.5: [-8.639000000005874]}, 'action_only': {0.0: [94.81599999999071], 0.05: [83.95999999998975], 0.1: [47.33800000000497], 0.15: [34.67300000001742], 0.2: [6.1779999999988435], 0.25: [5.217999999996758], 0.3: [7.775999999998115], 0.35: [-12.588000000000001], 0.4: [-11.287999999995954], 0.45: [-17.98099999997391], 0.5: [-16.94299999997881]}, 'goal_only': {0.0: [94.81799999999053], 0.05: [40.49400000001496], 0.1: [41.940000000012], 0.15: [14.718000000008084], 0.2: [31.25700000000713], 0.25: [20.02600000001346], 0.3: [16.66800000001042], 0.35: [21.031000000005825], 0.4: [-0.570000000004633], 0.45: [2.606999999998042], 0.5: [6.037999999999372]}, 'both_same': {0.0: [94.94199999999115], 0.05: [29.055000000009645], 0.1: [5.436999999998239], 0.15: [-6.471000000002391], 0.2: [-19.151999999984856], 0.25: [-13.666999999984048], 0.3: [-21.03299999998408], 0.35: [-9.06600000000279], 0.4: [-28.753999999977708], 0.45: [-20.2839999999785], 0.5: [-21.233999999980348]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mit6CCnJob4o"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointPushEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxjTe0z5csNW","executionInfo":{"status":"ok","timestamp":1617095122790,"user_tz":-60,"elapsed":1771965,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"94cb561d-00b9-4f20-f1de-67d07a02471f"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [-29.4009999999749], 1.5: [7.290999999993865], 2.0: [92.56699999999057], 2.5: [64.59399999999593], 3.0: [79.48499999998562], 3.5: [92.51199999998809], 4.0: [92.66099999998919], 4.5: [89.98599999998599], 5.0: [85.0989999999866], 5.5: [56.81099999999491], 6.0: [10.787000000009137], 6.5: [-13.617999999997183], 7.0: [-42.84899999997716]}\n","1 scale: {1.0: [-29.4009999999749, -50.00000000000659], 1.5: [7.290999999993865, -14.503999999998525], 2.0: [92.56699999999057, 9.576999999997687], 2.5: [64.59399999999593, 54.61600000000165], 3.0: [79.48499999998562, 80.51299999998629], 3.5: [92.51199999998809, 94.23099999999378], 4.0: [92.66099999998919, 80.34999999998911], 4.5: [89.98599999998599, 93.61199999998911], 5.0: [85.0989999999866, 95.20999999999283], 5.5: [56.81099999999491, 88.34799999998756], 6.0: [10.787000000009137, 91.6879999999866], 6.5: [-13.617999999997183, 81.6769999999829], 7.0: [-42.84899999997716, 58.78999999999369]}\n","2 scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343]}\n","3 scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885, -41.961999999976435], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751, -29.302999999973846], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751, -32.45899999997052], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355, 5.881999999997261], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546, -42.735999999976464], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942, 45.37800000000601], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624, 94.65099999999659], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018, 91.8029999999889], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092, 88.78199999998928], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835, 42.970000000006685], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448, 11.972000000003426], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683, -24.93299999997509], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343, -41.155999999975776]}\n","4 scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885, -41.961999999976435, -50.00000000000659], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751, -29.302999999973846, -50.00000000000659], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751, -32.45899999997052, -42.63399999997865], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355, 5.881999999997261, 21.542000000013946], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546, -42.735999999976464, 89.69799999997967], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942, 45.37800000000601, 97.81299999999597], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624, 94.65099999999659, 97.96499999999618], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018, 91.8029999999889, 97.72199999999586], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092, 88.78199999998928, 97.53999999999542], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835, 42.970000000006685, 97.50299999999552], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448, 11.972000000003426, 97.40399999999534], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683, -24.93299999997509, 97.28799999999518], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343, -41.155999999975776, 97.30899999999518]}\n","5 scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885, -41.961999999976435, -50.00000000000659, -11.076999999990301], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751, -29.302999999973846, -50.00000000000659, 9.679999999994527], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751, -32.45899999997052, -42.63399999997865, 50.077999999994056], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355, 5.881999999997261, 21.542000000013946, 95.05899999999114], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546, -42.735999999976464, 89.69799999997967, 94.12299999999568], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942, 45.37800000000601, 97.81299999999597, 79.97499999998621], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624, 94.65099999999659, 97.96499999999618, 70.43099999998405], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018, 91.8029999999889, 97.72199999999586, 80.19699999998488], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092, 88.78199999998928, 97.53999999999542, 79.70599999998724], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835, 42.970000000006685, 97.50299999999552, 50.30899999998893], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448, 11.972000000003426, 97.40399999999534, 21.685000000019617], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683, -24.93299999997509, 97.28799999999518, -21.500999999983424], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343, -41.155999999975776, 97.30899999999518, -31.03899999997404]}\n","6 scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885, -41.961999999976435, -50.00000000000659, -11.076999999990301, 22.88400000001117], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751, -29.302999999973846, -50.00000000000659, 9.679999999994527, 15.200000000002028], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751, -32.45899999997052, -42.63399999997865, 50.077999999994056, 58.91799999998561], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355, 5.881999999997261, 21.542000000013946, 95.05899999999114, 88.79599999998908], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546, -42.735999999976464, 89.69799999997967, 94.12299999999568, 82.42999999998682], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942, 45.37800000000601, 97.81299999999597, 79.97499999998621, 92.46399999998924], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624, 94.65099999999659, 97.96499999999618, 70.43099999998405, 94.41499999998922], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018, 91.8029999999889, 97.72199999999586, 80.19699999998488, 93.98999999998945], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092, 88.78199999998928, 97.53999999999542, 79.70599999998724, 93.69399999998896], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835, 42.970000000006685, 97.50299999999552, 50.30899999998893, 86.53999999997943], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448, 11.972000000003426, 97.40399999999534, 21.685000000019617, 87.67299999998698], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683, -24.93299999997509, 97.28799999999518, -21.500999999983424, 83.61599999997973], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343, -41.155999999975776, 97.30899999999518, -31.03899999997404, 89.1359999999854]}\n","----\n","scale: {1.0: [-29.4009999999749, -50.00000000000659, -40.102999999971885, -41.961999999976435, -50.00000000000659, -11.076999999990301, 22.88400000001117], 1.5: [7.290999999993865, -14.503999999998525, 11.66199999999751, -29.302999999973846, -50.00000000000659, 9.679999999994527, 15.200000000002028], 2.0: [92.56699999999057, 9.576999999997687, 35.68800000001751, -32.45899999997052, -42.63399999997865, 50.077999999994056, 58.91799999998561], 2.5: [64.59399999999593, 54.61600000000165, 8.20099999999355, 5.881999999997261, 21.542000000013946, 95.05899999999114, 88.79599999998908], 3.0: [79.48499999998562, 80.51299999998629, 75.81299999998546, -42.735999999976464, 89.69799999997967, 94.12299999999568, 82.42999999998682], 3.5: [92.51199999998809, 94.23099999999378, 93.23099999998942, 45.37800000000601, 97.81299999999597, 79.97499999998621, 92.46399999998924], 4.0: [92.66099999998919, 80.34999999998911, 91.39699999998624, 94.65099999999659, 97.96499999999618, 70.43099999998405, 94.41499999998922], 4.5: [89.98599999998599, 93.61199999998911, 85.85799999998018, 91.8029999999889, 97.72199999999586, 80.19699999998488, 93.98999999998945], 5.0: [85.0989999999866, 95.20999999999283, 60.81499999999092, 88.78199999998928, 97.53999999999542, 79.70599999998724, 93.69399999998896], 5.5: [56.81099999999491, 88.34799999998756, 11.777999999996835, 42.970000000006685, 97.50299999999552, 50.30899999998893, 86.53999999997943], 6.0: [10.787000000009137, 91.6879999999866, -25.094999999978448, 11.972000000003426, 97.40399999999534, 21.685000000019617, 87.67299999998698], 6.5: [-13.617999999997183, 81.6769999999829, -43.03399999997683, -24.93299999997509, 97.28799999999518, -21.500999999983424, 83.61599999997973], 7.0: [-42.84899999997716, 58.78999999999369, -48.59300000000343, -41.155999999975776, 97.30899999999518, -31.03899999997404, 89.1359999999854]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTjUVh0r8nx9"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZYgmaK7PhX1","executionInfo":{"status":"ok","timestamp":1617095970708,"user_tz":-60,"elapsed":847051,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"1b42f725-b5ed-4938-f8ae-58a7102c4874"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [90.16799999998669], 0.05: [81.55399999998917], 0.1: [83.2689999999858], 0.15: [81.77599999998235], 0.2: [86.80899999998466], 0.25: [82.18799999997776], 0.3: [82.27099999998532], 0.35: [84.56499999998044], 0.4: [83.05599999997848]}\n","1 range: {0.0: [90.16799999998669, 77.44799999999073], 0.05: [81.55399999998917, 81.72399999999084], 0.1: [83.2689999999858, 71.80099999997935], 0.15: [81.77599999998235, 69.00499999998226], 0.2: [86.80899999998466, 65.1249999999798], 0.25: [82.18799999997776, 66.72999999998689], 0.3: [82.27099999998532, 48.75799999998345], 0.35: [84.56499999998044, 52.67600000000328], 0.4: [83.05599999997848, 42.66100000000942]}\n","2 range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118]}\n","3 range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471, 93.18499999999513], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564, 83.56799999998822], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186, 73.80699999998964], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052, 62.068999999987966], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252, 54.64899999999656], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119, 44.25300000000354], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765, 42.98400000000898], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545, 23.726000000012906], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118, 20.38000000001654]}\n","4 range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471, 93.18499999999513, 97.92999999999628], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564, 83.56799999998822, 94.90099999999255], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186, 73.80699999998964, 84.51799999998435], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052, 62.068999999987966, 71.22999999998179], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252, 54.64899999999656, 68.210999999983], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119, 44.25300000000354, 63.77799999999005], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765, 42.98400000000898, 71.12799999998808], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545, 23.726000000012906, 54.91900000000068], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118, 20.38000000001654, 48.96000000000208]}\n","5 range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471, 93.18499999999513, 97.92999999999628, 64.36399999998754], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564, 83.56799999998822, 94.90099999999255, 73.94899999997966], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186, 73.80699999998964, 84.51799999998435, 78.23099999999052], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052, 62.068999999987966, 71.22999999998179, 69.25599999998305], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252, 54.64899999999656, 68.210999999983, 67.68499999997502], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119, 44.25300000000354, 63.77799999999005, 56.34199999999181], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765, 42.98400000000898, 71.12799999998808, 47.682999999993626], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545, 23.726000000012906, 54.91900000000068, 34.61500000001509], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118, 20.38000000001654, 48.96000000000208, 28.822000000016953]}\n","6 range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471, 93.18499999999513, 97.92999999999628, 64.36399999998754, 93.59799999998991], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564, 83.56799999998822, 94.90099999999255, 73.94899999997966, 93.6539999999919], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186, 73.80699999998964, 84.51799999998435, 78.23099999999052, 78.6819999999837], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052, 62.068999999987966, 71.22999999998179, 69.25599999998305, 78.80799999998236], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252, 54.64899999999656, 68.210999999983, 67.68499999997502, 71.13299999998105], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119, 44.25300000000354, 63.77799999999005, 56.34199999999181, 56.35099999998511], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765, 42.98400000000898, 71.12799999998808, 47.682999999993626, 56.91299999999752], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545, 23.726000000012906, 54.91900000000068, 34.61500000001509, 41.02300000000299], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118, 20.38000000001654, 48.96000000000208, 28.822000000016953, 48.873999999996286]}\n","----\n","range: {0.0: [90.16799999998669, 77.44799999999073, 89.86699999998471, 93.18499999999513, 97.92999999999628, 64.36399999998754, 93.59799999998991], 0.05: [81.55399999998917, 81.72399999999084, 90.65999999998564, 83.56799999998822, 94.90099999999255, 73.94899999997966, 93.6539999999919], 0.1: [83.2689999999858, 71.80099999997935, 82.93899999998186, 73.80699999998964, 84.51799999998435, 78.23099999999052, 78.6819999999837], 0.15: [81.77599999998235, 69.00499999998226, 81.24099999998052, 62.068999999987966, 71.22999999998179, 69.25599999998305, 78.80799999998236], 0.2: [86.80899999998466, 65.1249999999798, 70.11499999999252, 54.64899999999656, 68.210999999983, 67.68499999997502, 71.13299999998105], 0.25: [82.18799999997776, 66.72999999998689, 47.17000000000119, 44.25300000000354, 63.77799999999005, 56.34199999999181, 56.35099999998511], 0.3: [82.27099999998532, 48.75799999998345, 26.287000000007765, 42.98400000000898, 71.12799999998808, 47.682999999993626, 56.91299999999752], 0.35: [84.56499999998044, 52.67600000000328, 23.599000000018545, 23.726000000012906, 54.91900000000068, 34.61500000001509, 41.02300000000299], 0.4: [83.05599999997848, 42.66100000000942, 4.584999999997118, 20.38000000001654, 48.96000000000208, 28.822000000016953, 48.873999999996286]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6g9omuyeT51"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz-OF0Zl6zp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612798920385,"user_tz":-60,"elapsed":83452,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"4aea5aff-c52d-4252-c444-7dbcb3043205"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","episodes.pop(5 - 1)\n","episodes.pop(11 - 2)\n","\n","torch.save(episodes, \"PointPush_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVujCiQrP3hh"},"source":[""],"execution_count":null,"outputs":[]}]}