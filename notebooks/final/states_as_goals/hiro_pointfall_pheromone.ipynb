{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointfall_pheromone.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617436804049,"user_tz":-60,"elapsed":17668,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"02786c5e-77c1-4be4-f523-d286333105ca"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_fall.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617436804050,"user_tz":-60,"elapsed":2115,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_pheromone/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.pheromone_paths,\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_fall_pheromone/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.pheromone_paths = copy.deepcopy(checkpoint['meta_controller'])\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617436808367,"user_tz":-60,"elapsed":6060,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617436808368,"user_tz":-60,"elapsed":5701,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617436808570,"user_tz":-60,"elapsed":5456,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["from point_fall import PointFallEnv \n","env = NormalizedEnv(PointFallEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617436982267,"user_tz":-60,"elapsed":592,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617436982469,"user_tz":-60,"elapsed":323,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617436982962,"user_tz":-60,"elapsed":562,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617436983218,"user_tz":-60,"elapsed":351,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617436983924,"user_tz":-60,"elapsed":802,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 20000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617436984122,"user_tz":-60,"elapsed":508,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["from operator import itemgetter\n","class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        # a list of tuple of form (reward, path); keep top 5\n","        self.pheromone_paths = []\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def add_path(self, reward, path):\n","        # prefer higher-reward paths\n","        self.pheromone_paths.append((reward, path))\n","        self.pheromone_paths.sort(key=itemgetter(0), reverse=True)\n","        self.pheromone_paths = self.pheromone_paths[:5] # only keep top 5\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        if warmup or len(self.pheromone_paths) == 0:\n","            return torch.tensor([np.random.uniform(-1.,1.,len(self.goal_dim))], device=device, dtype=torch.float)\n","        \n","        time_index = 3\n","        #cur_t = s_t.squeeze(0)[time_index] # time\n","        cur_pos = s_t.squeeze(0)[self.goal_dim]\n","\n","        goal = torch.tensor([0] * len(self.goal_dim), device=device, dtype=torch.float)\n","\n","        min_rew = -60 # min(self.pheromone_paths, key = lambda t: t[0])[0]\n","        tot_rew = sum([t[0] for t in self.pheromone_paths]) - len(self.pheromone_paths) * min_rew\n","\n","        for rew, path in self.pheromone_paths:\n","            breakdown = tuple(map(torch.stack, zip(*path)))\n","            positions = torch.stack([breakdown[i] for i in self.goal_dim], axis=-1)\n","            chosen_i = torch.argmin(torch.pow(torch.sum(torch.pow(positions - cur_pos, 2), axis=1), 0.5))\n","            \n","            # assume c = 10\n","            # basically, in chosen path, go 10 steps ahead from position closest\n","            # to the currently observed one\n","            chosen_point = path[min(chosen_i + 10, len(path) - 1)]\n","\n","            #chosen_point = path[torch.argmin(torch.abs(breakdown[time_index] - cur_t))]\n","            goal += (rew - min_rew) * chosen_point[self.goal_dim]\n","        \n","        goal /= tot_rew\n","        goal = goal - s_t.squeeze(0)[self.goal_dim] # make goal relative to given position\n","\n","        return goal.unsqueeze(0)\n","    \n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 6\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 200\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        state_seq = None\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        # once episode finishes, append full path to manager\n","        agent.add_path(overall_reward, state_seq)\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 400 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_fall_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1617436999086,"user_tz":-60,"elapsed":11107,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, targeted):\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            goal = agent.select_goal(og_state, False, False)\n","            state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    goal_temp = agent.h(state, goal, next_og_state)\n","                    next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhC6f7N6sJoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617448367305,"user_tz":-60,"elapsed":11344052,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"e979f060-e1ed-41ee-ec72-dc2b090cba95"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","#i = 1\n","#while i < 2:\n","#    #agent = train_model()\n","#    agent = HIRO(n_observations, n_actions).to(device)\n","#    load_model(agent, f\"hiro_fall_{i}\")\n","#\n","#    if agent is not None:\n","#        # goal_attack, action_attack, same_noise\n","#        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","#        eval_model(agent, noise_hrl['both'], True, True, False)\n","#        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","#        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","#        print(f\"{i} noise_hrl: {noise_hrl}\")\n","#        i += 1\n","\n","#print(\"----\")\n","#print(f\"noise_hrl: {noise_hrl}\")\n","\n","i = 0\n","while i < 6:\n","    if i == 1:\n","        i += 1\n","        continue\n","\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False)   \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['action'], True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857], 0.02: [77.41899999997504], 0.04: [30.536000000013917], 0.06: [-1.2940000000020424], 0.08: [-6.836000000007285], 0.1: [-24.93699999997825], 0.12: [-44.53199999998483], 0.14: [-48.570000000005024], 0.16: [-48.663000000000814], 0.18: [-50.00000000000659], 0.2: [-50.00000000000659]}}\n","0 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852], 0.02: [63.68999999997289], 0.04: [14.712000000003831], 0.06: [-9.573000000005925], 0.08: [-31.353999999973126], 0.1: [-31.202999999971468], 0.12: [-41.18999999997221], 0.14: [-41.03299999997237], 0.16: [-46.82399999999322], 0.18: [-48.70100000000095], 0.2: [-48.64600000000075]}}\n","2 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857, 94.05599999998984], 0.02: [77.41899999997504, 39.65300000002517], 0.04: [30.536000000013917, -44.600999999986946], 0.06: [-1.2940000000020424, 17.37900000000043], 0.08: [-6.836000000007285, -15.065999999995604], 0.1: [-24.93699999997825, -26.60899999997493], 0.12: [-44.53199999998483, -48.980000000001965], 0.14: [-48.570000000005024, -50.00000000000659], 0.16: [-48.663000000000814, -50.00000000000659], 0.18: [-50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659]}}\n","2 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852, 94.13799999998945], 0.02: [63.68999999997289, 31.348000000014498], 0.04: [14.712000000003831, 17.42700000000507], 0.06: [-9.573000000005925, -25.365999999976697], 0.08: [-31.353999999973126, -41.60399999997176], 0.1: [-31.202999999971468, -47.34699999999511], 0.12: [-41.18999999997221, -50.00000000000659], 0.14: [-41.03299999997237, -50.00000000000659], 0.16: [-46.82399999999322, -50.00000000000659], 0.18: [-48.70100000000095, -50.00000000000659], 0.2: [-48.64600000000075, -50.00000000000659]}}\n","3 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857, 94.05599999998984, 86.79899999998203], 0.02: [77.41899999997504, 39.65300000002517, 79.71699999998862], 0.04: [30.536000000013917, -44.600999999986946, -50.00000000000659], 0.06: [-1.2940000000020424, 17.37900000000043, -50.00000000000659], 0.08: [-6.836000000007285, -15.065999999995604, -19.47999999998803], 0.1: [-24.93699999997825, -26.60899999997493, -38.219999999973695], 0.12: [-44.53199999998483, -48.980000000001965, -50.00000000000659], 0.14: [-48.570000000005024, -50.00000000000659, -45.82499999998981], 0.16: [-48.663000000000814, -50.00000000000659, -50.00000000000659], 0.18: [-50.00000000000659, -50.00000000000659, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659]}}\n","3 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852, 94.13799999998945, 89.71399999998232], 0.02: [63.68999999997289, 31.348000000014498, 64.94299999998469], 0.04: [14.712000000003831, 17.42700000000507, 21.484000000012728], 0.06: [-9.573000000005925, -25.365999999976697, -24.009999999974884], 0.08: [-31.353999999973126, -41.60399999997176, -38.833999999971375], 0.1: [-31.202999999971468, -47.34699999999511, -47.62399999999612], 0.12: [-41.18999999997221, -50.00000000000659, -47.25299999999932], 0.14: [-41.03299999997237, -50.00000000000659, -48.62800000000068], 0.16: [-46.82399999999322, -50.00000000000659, -44.58799999998326], 0.18: [-48.70100000000095, -50.00000000000659, -46.81399999999387], 0.2: [-48.64600000000075, -50.00000000000659, -43.7229999999792]}}\n","4 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857, 94.05599999998984, 86.79899999998203, 95.14599999999095], 0.02: [77.41899999997504, 39.65300000002517, 79.71699999998862, 61.97299999998036], 0.04: [30.536000000013917, -44.600999999986946, -50.00000000000659, 43.32599999999019], 0.06: [-1.2940000000020424, 17.37900000000043, -50.00000000000659, 10.63199999999877], 0.08: [-6.836000000007285, -15.065999999995604, -19.47999999998803, -38.01399999996752], 0.1: [-24.93699999997825, -26.60899999997493, -38.219999999973695, -41.04999999996763], 0.12: [-44.53199999998483, -48.980000000001965, -50.00000000000659, -36.13299999996945], 0.14: [-48.570000000005024, -50.00000000000659, -45.82499999998981, -33.173999999967144], 0.16: [-48.663000000000814, -50.00000000000659, -50.00000000000659, -26.541999999973118], 0.18: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -40.93399999996844], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -33.78899999996815]}}\n","4 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852, 94.13799999998945, 89.71399999998232, 95.0269999999919], 0.02: [63.68999999997289, 31.348000000014498, 64.94299999998469, 92.7779999999889], 0.04: [14.712000000003831, 17.42700000000507, 21.484000000012728, 92.1489999999904], 0.06: [-9.573000000005925, -25.365999999976697, -24.009999999974884, 88.00699999998237], 0.08: [-31.353999999973126, -41.60399999997176, -38.833999999971375, 40.5640000000133], 0.1: [-31.202999999971468, -47.34699999999511, -47.62399999999612, -28.087999999976905], 0.12: [-41.18999999997221, -50.00000000000659, -47.25299999999932, -46.58299999999426], 0.14: [-41.03299999997237, -50.00000000000659, -48.62800000000068, -50.00000000000659], 0.16: [-46.82399999999322, -50.00000000000659, -44.58799999998326, -44.86399999998632], 0.18: [-48.70100000000095, -50.00000000000659, -46.81399999999387, -48.66200000000081], 0.2: [-48.64600000000075, -50.00000000000659, -43.7229999999792, -42.19599999997501]}}\n","5 fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857, 94.05599999998984, 86.79899999998203, 95.14599999999095, 91.57799999998551], 0.02: [77.41899999997504, 39.65300000002517, 79.71699999998862, 61.97299999998036, 91.81399999998595], 0.04: [30.536000000013917, -44.600999999986946, -50.00000000000659, 43.32599999999019, 79.0719999999807], 0.06: [-1.2940000000020424, 17.37900000000043, -50.00000000000659, 10.63199999999877, 32.256000000017245], 0.08: [-6.836000000007285, -15.065999999995604, -19.47999999998803, -38.01399999996752, -21.0899999999845], 0.1: [-24.93699999997825, -26.60899999997493, -38.219999999973695, -41.04999999996763, -18.31899999998378], 0.12: [-44.53199999998483, -48.980000000001965, -50.00000000000659, -36.13299999996945, -18.967999999989487], 0.14: [-48.570000000005024, -50.00000000000659, -45.82499999998981, -33.173999999967144, -32.73899999997027], 0.16: [-48.663000000000814, -50.00000000000659, -50.00000000000659, -26.541999999973118, -40.14699999997142], 0.18: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -40.93399999996844, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -33.78899999996815, -50.00000000000659]}}\n","5 fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852, 94.13799999998945, 89.71399999998232, 95.0269999999919, 91.50599999998538], 0.02: [63.68999999997289, 31.348000000014498, 64.94299999998469, 92.7779999999889, 73.01899999997799], 0.04: [14.712000000003831, 17.42700000000507, 21.484000000012728, 92.1489999999904, 6.4859999999949745], 0.06: [-9.573000000005925, -25.365999999976697, -24.009999999974884, 88.00699999998237, -24.215999999975956], 0.08: [-31.353999999973126, -41.60399999997176, -38.833999999971375, 40.5640000000133, -35.21899999997241], 0.1: [-31.202999999971468, -47.34699999999511, -47.62399999999612, -28.087999999976905, -43.38699999998], 0.12: [-41.18999999997221, -50.00000000000659, -47.25299999999932, -46.58299999999426, -43.546999999987655], 0.14: [-41.03299999997237, -50.00000000000659, -48.62800000000068, -50.00000000000659, -43.4319999999802], 0.16: [-46.82399999999322, -50.00000000000659, -44.58799999998326, -44.86399999998632, -44.055999999982525], 0.18: [-48.70100000000095, -50.00000000000659, -46.81399999999387, -48.66200000000081, -48.634000000000704], 0.2: [-48.64600000000075, -50.00000000000659, -43.7229999999792, -42.19599999997501, -44.54699999998963]}}\n","----\n","fgsm (ut): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.1929999999857, 94.05599999998984, 86.79899999998203, 95.14599999999095, 91.57799999998551], 0.02: [77.41899999997504, 39.65300000002517, 79.71699999998862, 61.97299999998036, 91.81399999998595], 0.04: [30.536000000013917, -44.600999999986946, -50.00000000000659, 43.32599999999019, 79.0719999999807], 0.06: [-1.2940000000020424, 17.37900000000043, -50.00000000000659, 10.63199999999877, 32.256000000017245], 0.08: [-6.836000000007285, -15.065999999995604, -19.47999999998803, -38.01399999996752, -21.0899999999845], 0.1: [-24.93699999997825, -26.60899999997493, -38.219999999973695, -41.04999999996763, -18.31899999998378], 0.12: [-44.53199999998483, -48.980000000001965, -50.00000000000659, -36.13299999996945, -18.967999999989487], 0.14: [-48.570000000005024, -50.00000000000659, -45.82499999998981, -33.173999999967144, -32.73899999997027], 0.16: [-48.663000000000814, -50.00000000000659, -50.00000000000659, -26.541999999973118, -40.14699999997142], 0.18: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -40.93399999996844, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659, -50.00000000000659, -33.78899999996815, -50.00000000000659]}}\n","fgsm (t): {'goal': {0.0: [], 0.02: [], 0.04: [], 0.06: [], 0.08: [], 0.1: [], 0.12: [], 0.14: [], 0.16: [], 0.18: [], 0.2: []}, 'action': {0.0: [91.0069999999852, 94.13799999998945, 89.71399999998232, 95.0269999999919, 91.50599999998538], 0.02: [63.68999999997289, 31.348000000014498, 64.94299999998469, 92.7779999999889, 73.01899999997799], 0.04: [14.712000000003831, 17.42700000000507, 21.484000000012728, 92.1489999999904, 6.4859999999949745], 0.06: [-9.573000000005925, -25.365999999976697, -24.009999999974884, 88.00699999998237, -24.215999999975956], 0.08: [-31.353999999973126, -41.60399999997176, -38.833999999971375, 40.5640000000133, -35.21899999997241], 0.1: [-31.202999999971468, -47.34699999999511, -47.62399999999612, -28.087999999976905, -43.38699999998], 0.12: [-41.18999999997221, -50.00000000000659, -47.25299999999932, -46.58299999999426, -43.546999999987655], 0.14: [-41.03299999997237, -50.00000000000659, -48.62800000000068, -50.00000000000659, -43.4319999999802], 0.16: [-46.82399999999322, -50.00000000000659, -44.58799999998326, -44.86399999998632, -44.055999999982525], 0.18: [-48.70100000000095, -50.00000000000659, -46.81399999999387, -48.66200000000081, -48.634000000000704], 0.2: [-48.64600000000075, -50.00000000000659, -43.7229999999792, -42.19599999997501, -44.54699999998963]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RRO0Zum-ueh6"},"source":["Solved after 2188 episodes!\n","0 noise_hrl: {'both': {0.0: [91.14999999998405], 0.05: [79.29499999998413], 0.1: [67.06299999997272], 0.15: [66.75099999998633], 0.2: [65.29299999997187], 0.25: [70.10999999997367], 0.3: [69.03899999996797], 0.35: [68.90299999997333], 0.4: [59.02899999998925], 0.45: [37.38900000002316], 0.5: [41.19000000002035]}, 'action_only': {0.0: [90.8299999999855], 0.05: [81.12999999998142], 0.1: [78.68399999997833], 0.15: [73.29799999997664], 0.2: [64.69999999998511], 0.25: [41.89100000001744], 0.3: [36.32200000001718], 0.35: [33.518000000017366], 0.4: [32.30900000001845], 0.45: [27.87500000002194], 0.5: [23.13200000001679]}, 'goal_only': {0.0: [90.50499999998341], 0.05: [79.91199999997998], 0.1: [71.54099999997774], 0.15: [50.45499999999405], 0.2: [52.255999999993335], 0.25: [64.18799999996844], 0.3: [61.16999999998588], 0.35: [62.519999999980975], 0.4: [54.405999999974576], 0.45: [50.605999999991944], 0.5: [40.07300000002481]}, 'both_same': {0.0: [91.3149999999862], 0.05: [77.30599999997263], 0.1: [57.20799999997914], 0.15: [57.40599999997899], 0.2: [73.60999999997127], 0.25: [62.46399999997322], 0.3: [64.08899999996703], 0.35: [50.193999999994595], 0.4: [28.695000000018705], 0.45: [21.46100000000863], 0.5: [20.46200000001309]}}\n","Solved after 566 episodes!\n","1 noise_hrl: {'both': {0.0: [76.48099999997548], 0.05: [66.42399999998551], 0.1: [74.93699999996724], 0.15: [36.782000000018904], 0.2: [26.905000000016084], 0.25: [3.232999999994894], 0.3: [1.715000000000258], 0.35: [-3.2350000000003205], 0.4: [-16.95199999999436], 0.45: [-9.302000000000056], 0.5: [-22.68099999997717]}, 'action_only': {0.0: [81.90599999998186], 0.05: [64.78699999998383], 0.1: [75.11099999997964], 0.15: [80.14799999997838], 0.2: [79.4489999999731], 0.25: [79.66899999998031], 0.3: [74.24299999997808], 0.35: [71.39599999997375], 0.4: [46.79400000000997], 0.45: [23.03300000001197], 0.5: [7.067999999995821]}, 'goal_only': {0.0: [72.13599999998291], 0.05: [81.40199999997735], 0.1: [75.3829999999819], 0.15: [47.44400000000806], 0.2: [35.46500000001251], 0.25: [11.798999999998209], 0.3: [-16.87699999998201], 0.35: [-24.64499999997189], 0.4: [-24.263999999976885], 0.45: [-31.794999999969306], 0.5: [-22.558999999977758]}, 'both_same': {0.0: [86.30399999998434], 0.05: [59.75999999999748], 0.1: [50.94400000000248], 0.15: [30.537000000021575], 0.2: [4.668999999996821], 0.25: [-9.064000000005079], 0.3: [-14.91999999999718], 0.35: [-27.673999999978474], 0.4: [-31.97899999997428], 0.45: [-33.903999999973045], 0.5: [-30.797999999973207]}}\n","Solved after 957 episodes!\n","Solved after 2026 episodes!\n","3 noise_hrl: {'both': {0.0: [94.2229999999902, 89.37699999998289], 0.05: [57.017999999999596, 75.04199999996837], 0.1: [-2.9510000000033165, 59.294999999971544], 0.15: [-30.59899999997835, 13.106000000013374], 0.2: [-45.90099999999438, -15.556999999989483], 0.25: [-45.89399999999006, -29.576999999975257], 0.3: [-41.75099999997314, -17.43499999998265], 0.35: [-46.385999999995256, -20.941999999986816], 0.4: [-48.65700000000079, -9.663000000009037], 0.45: [-43.94499999998115, -13.374999999997538], 0.5: [-46.23199999999129, -1.970000000002649]}, 'action_only': {0.0: [93.97199999998942, 89.64899999998318], 0.05: [80.29699999998728, 90.11699999998332], 0.1: [68.43499999997522, 82.77699999998451], 0.15: [58.758999999986266, 62.84499999998334], 0.2: [34.252000000016416, 35.68800000002029], 0.25: [-6.889000000004367, 18.44600000000853], 0.3: [-34.813999999977135, 3.469999999999179], 0.35: [-43.170999999978946, 6.354999999995261], 0.4: [-45.93799999998908, -12.834999999999079], 0.45: [-48.75600000000115, -22.37399999998165], 0.5: [-50.00000000000659, -27.589999999972427]}, 'goal_only': {0.0: [93.63399999998886, 89.6589999999833], 0.05: [76.67699999998851, 51.43299999999118], 0.1: [15.861000000010801, 50.308999999989176], 0.15: [-21.990999999979348, 10.792999999998555], 0.2: [-47.11599999999541, -17.394999999984194], 0.25: [-36.54499999996976, -36.60199999996983], 0.3: [-50.00000000000659, -43.01099999998002], 0.35: [-47.522999999995754, -39.2579999999682], 0.4: [-47.28700000000058, -36.076999999969274], 0.45: [-48.54500000000493, -34.72399999997301], 0.5: [-50.00000000000659, -27.237999999975838]}, 'both_same': {0.0: [93.92799999998986, 89.82199999998213], 0.05: [75.27099999998303, 68.19199999996806], 0.1: [9.72299999999606, 43.01200000002319], 0.15: [-31.905999999974874, 1.6719999999995085], 0.2: [-44.80399999998717, -23.92799999997686], 0.25: [-50.00000000000659, -34.14099999996831], 0.3: [-48.629000000005234, -28.20799999997092], 0.35: [-46.012999999990214, -30.569999999970136], 0.4: [-48.77000000000234, -29.166999999974017], 0.45: [-48.856000000001515, -16.66199999999904], 0.5: [-50.00000000000659, -12.425999999984592]}}\n","Solved after 784 episodes!\n","Solved after 2229 episodes!\n","5 noise_hrl: {'both': {0.0: [94.97999999999246, 91.5919999999854], 0.05: [56.68300000000006, 74.89799999997648], 0.1: [9.994999999994992, 22.664000000011555], 0.15: [-14.511999999985617, -6.061000000007647], 0.2: [-35.6279999999706, -10.002000000003678], 0.25: [-41.36299999996994, -33.816999999968374], 0.3: [-44.02999999998627, -30.34299999997835], 0.35: [-47.41599999999628, -28.75599999997639], 0.4: [-41.851999999973835, -29.12799999997753], 0.45: [-34.55699999997266, -23.937999999984427], 0.5: [-36.359999999973574, -33.53399999997518]}, 'action_only': {0.0: [95.28699999999212, 91.6059999999855], 0.05: [52.99099999997989, 90.55299999998476], 0.1: [25.192000000017558, 88.58299999998157], 0.15: [19.207000000007316, 81.90599999997528], 0.2: [-10.057999999997675, 63.180999999975626], 0.25: [-4.806000000003188, 54.20699999998302], 0.3: [0.2920000000004416, 35.14500000001777], 0.35: [-4.368999999999773, 29.91600000001266], 0.4: [-9.963999999994, 22.199000000007448], 0.45: [-12.001999999999645, 15.421000000003477], 0.5: [-14.597999999988918, 1.3589999999978093]}, 'goal_only': {0.0: [95.42999999999185, 91.60599999998561], 0.05: [79.05399999998485, 67.55199999997275], 0.1: [17.246000000001995, 28.090000000018918], 0.15: [0.997999999997677, -6.254000000005414], 0.2: [-17.184999999976814, -22.133999999982237], 0.25: [-39.11999999996896, -19.756999999983346], 0.3: [-37.3509999999699, -38.76399999996979], 0.35: [-34.30399999997176, -37.72999999997096], 0.4: [-36.83899999996836, -33.82299999996924], 0.45: [-42.673999999977134, -35.832999999972856], 0.5: [-38.47699999996971, -37.97599999997616]}, 'both_same': {0.0: [95.4979999999918, 91.51199999998536], 0.05: [57.62699999999806, 71.3169999999803], 0.1: [24.45200000001455, 23.49500000002157], 0.15: [-22.03299999997608, 0.49099999999980876], 0.2: [-16.436999999991862, -6.54800000000321], 0.25: [-38.18899999996912, -13.08199999999984], 0.3: [-41.67599999997583, -22.320999999988995], 0.35: [-42.86099999997785, -27.821999999984627], 0.4: [-38.64299999996989, -25.055999999976315], 0.45: [-44.16199999999643, -32.56099999996903], 0.5: [-42.28999999997421, -31.143999999971435]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdQEb3LDjtxd"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointFallEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHVoTScvj8Ru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617093822040,"user_tz":-60,"elapsed":1516906,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"2c81359e-ac12-484f-99ec-f7d69528b345"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    if i == 1:\n","        i += 1\n","        continue\n","        \n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [-40.22599999997028], 1.5: [-30.681999999974636], 2.0: [-22.282999999985723], 2.5: [14.147000000007887], 3.0: [79.62599999997866], 3.5: [90.61799999998436], 4.0: [91.16199999998582], 4.5: [90.0439999999843], 5.0: [88.90199999998147], 5.5: [84.26999999997392], 6.0: [84.96199999997742], 6.5: [85.80999999998143], 7.0: [83.16699999997368]}\n","1 scale: {1.0: [-40.22599999997028, -0.6030000000028386], 1.5: [-30.681999999974636, 65.05799999999346], 2.0: [-22.282999999985723, 75.27299999998534], 2.5: [14.147000000007887, 90.1799999999845], 3.0: [79.62599999997866, 84.17599999997967], 3.5: [90.61799999998436, 79.69799999997961], 4.0: [91.16199999998582, 73.58199999998902], 4.5: [90.0439999999843, 74.59699999999022], 5.0: [88.90199999998147, 82.30499999998256], 5.5: [84.26999999997392, 85.76599999997883], 6.0: [84.96199999997742, 85.01999999998006], 6.5: [85.80999999998143, 84.2189999999793], 7.0: [83.16699999997368, 86.61299999997884]}\n","2 scale: {1.0: [-40.22599999997028, -0.6030000000028386, -13.434999999990827], 1.5: [-30.681999999974636, 65.05799999999346, 52.63299999998937], 2.0: [-22.282999999985723, 75.27299999998534, 85.22299999998475], 2.5: [14.147000000007887, 90.1799999999845, 93.86999999998945], 3.0: [79.62599999997866, 84.17599999997967, 93.41199999998904], 3.5: [90.61799999998436, 79.69799999997961, 94.00599999998877], 4.0: [91.16199999998582, 73.58199999998902, 94.02999999998927], 4.5: [90.0439999999843, 74.59699999999022, 92.73999999998829], 5.0: [88.90199999998147, 82.30499999998256, 94.56599999999045], 5.5: [84.26999999997392, 85.76599999997883, 94.29699999999], 6.0: [84.96199999997742, 85.01999999998006, 94.37299999999016], 6.5: [85.80999999998143, 84.2189999999793, 94.17299999998977], 7.0: [83.16699999997368, 86.61299999997884, 94.06999999998959]}\n","3 scale: {1.0: [-40.22599999997028, -0.6030000000028386, -13.434999999990827, -30.55899999997379], 1.5: [-30.681999999974636, 65.05799999999346, 52.63299999998937, 17.475000000005178], 2.0: [-22.282999999985723, 75.27299999998534, 85.22299999998475, 65.54499999998086], 2.5: [14.147000000007887, 90.1799999999845, 93.86999999998945, 91.77199999998585], 3.0: [79.62599999997866, 84.17599999997967, 93.41199999998904, 91.44699999998664], 3.5: [90.61799999998436, 79.69799999997961, 94.00599999998877, 91.61399999998552], 4.0: [91.16199999998582, 73.58199999998902, 94.02999999998927, 89.8669999999838], 4.5: [90.0439999999843, 74.59699999999022, 92.73999999998829, 88.85599999998163], 5.0: [88.90199999998147, 82.30499999998256, 94.56599999999045, 91.41899999998523], 5.5: [84.26999999997392, 85.76599999997883, 94.29699999999, 18.979000000005936], 6.0: [84.96199999997742, 85.01999999998006, 94.37299999999016, -18.182999999992152], 6.5: [85.80999999998143, 84.2189999999793, 94.17299999998977, -44.3759999999835], 7.0: [83.16699999997368, 86.61299999997884, 94.06999999998959, -50.00000000000659]}\n","4 scale: {1.0: [-40.22599999997028, -0.6030000000028386, -13.434999999990827, -30.55899999997379, 26.267000000008338], 1.5: [-30.681999999974636, 65.05799999999346, 52.63299999998937, 17.475000000005178, -5.461000000004297], 2.0: [-22.282999999985723, 75.27299999998534, 85.22299999998475, 65.54499999998086, 88.25099999998291], 2.5: [14.147000000007887, 90.1799999999845, 93.86999999998945, 91.77199999998585, 93.39299999998867], 3.0: [79.62599999997866, 84.17599999997967, 93.41199999998904, 91.44699999998664, 91.1539999999866], 3.5: [90.61799999998436, 79.69799999997961, 94.00599999998877, 91.61399999998552, 93.833999999989], 4.0: [91.16199999998582, 73.58199999998902, 94.02999999998927, 89.8669999999838, 95.49299999999148], 4.5: [90.0439999999843, 74.59699999999022, 92.73999999998829, 88.85599999998163, 94.42799999999046], 5.0: [88.90199999998147, 82.30499999998256, 94.56599999999045, 91.41899999998523, 95.80599999999235], 5.5: [84.26999999997392, 85.76599999997883, 94.29699999999, 18.979000000005936, 93.38499999998665], 6.0: [84.96199999997742, 85.01999999998006, 94.37299999999016, -18.182999999992152, 95.51999999999191], 6.5: [85.80999999998143, 84.2189999999793, 94.17299999998977, -44.3759999999835, 95.3659999999914], 7.0: [83.16699999997368, 86.61299999997884, 94.06999999998959, -50.00000000000659, 95.35699999999161]}\n","5 scale: {1.0: [-40.22599999997028, -0.6030000000028386, -13.434999999990827, -30.55899999997379, 26.267000000008338, -9.154999999998578], 1.5: [-30.681999999974636, 65.05799999999346, 52.63299999998937, 17.475000000005178, -5.461000000004297, 43.31300000001557], 2.0: [-22.282999999985723, 75.27299999998534, 85.22299999998475, 65.54499999998086, 88.25099999998291, 85.30999999998085], 2.5: [14.147000000007887, 90.1799999999845, 93.86999999998945, 91.77199999998585, 93.39299999998867, 91.5529999999855], 3.0: [79.62599999997866, 84.17599999997967, 93.41199999998904, 91.44699999998664, 91.1539999999866, 91.54399999998546], 3.5: [90.61799999998436, 79.69799999997961, 94.00599999998877, 91.61399999998552, 93.833999999989, 91.64899999998559], 4.0: [91.16199999998582, 73.58199999998902, 94.02999999998927, 89.8669999999838, 95.49299999999148, 91.5579999999855], 4.5: [90.0439999999843, 74.59699999999022, 92.73999999998829, 88.85599999998163, 94.42799999999046, 91.14999999998457], 5.0: [88.90199999998147, 82.30499999998256, 94.56599999999045, 91.41899999998523, 95.80599999999235, 90.62399999998438], 5.5: [84.26999999997392, 85.76599999997883, 94.29699999999, 18.979000000005936, 93.38499999998665, 90.56799999998422], 6.0: [84.96199999997742, 85.01999999998006, 94.37299999999016, -18.182999999992152, 95.51999999999191, 90.42399999998398], 6.5: [85.80999999998143, 84.2189999999793, 94.17299999998977, -44.3759999999835, 95.3659999999914, 90.13499999998346], 7.0: [83.16699999997368, 86.61299999997884, 94.06999999998959, -50.00000000000659, 95.35699999999161, 89.87899999998314]}\n","----\n","scale: {1.0: [-40.22599999997028, -0.6030000000028386, -13.434999999990827, -30.55899999997379, 26.267000000008338, -9.154999999998578], 1.5: [-30.681999999974636, 65.05799999999346, 52.63299999998937, 17.475000000005178, -5.461000000004297, 43.31300000001557], 2.0: [-22.282999999985723, 75.27299999998534, 85.22299999998475, 65.54499999998086, 88.25099999998291, 85.30999999998085], 2.5: [14.147000000007887, 90.1799999999845, 93.86999999998945, 91.77199999998585, 93.39299999998867, 91.5529999999855], 3.0: [79.62599999997866, 84.17599999997967, 93.41199999998904, 91.44699999998664, 91.1539999999866, 91.54399999998546], 3.5: [90.61799999998436, 79.69799999997961, 94.00599999998877, 91.61399999998552, 93.833999999989, 91.64899999998559], 4.0: [91.16199999998582, 73.58199999998902, 94.02999999998927, 89.8669999999838, 95.49299999999148, 91.5579999999855], 4.5: [90.0439999999843, 74.59699999999022, 92.73999999998829, 88.85599999998163, 94.42799999999046, 91.14999999998457], 5.0: [88.90199999998147, 82.30499999998256, 94.56599999999045, 91.41899999998523, 95.80599999999235, 90.62399999998438], 5.5: [84.26999999997392, 85.76599999997883, 94.29699999999, 18.979000000005936, 93.38499999998665, 90.56799999998422], 6.0: [84.96199999997742, 85.01999999998006, 94.37299999999016, -18.182999999992152, 95.51999999999191, 90.42399999998398], 6.5: [85.80999999998143, 84.2189999999793, 94.17299999998977, -44.3759999999835, 95.3659999999914, 90.13499999998346], 7.0: [83.16699999997368, 86.61299999997884, 94.06999999998959, -50.00000000000659, 95.35699999999161, 89.87899999998314]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCngiUhykCBW"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VRa-o8mkMvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617094509771,"user_tz":-60,"elapsed":687709,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"e7917a31-98df-4cc5-9883-7891e991955b"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","env = NormalizedEnv(PointFallEnv(4))\n","i = 0\n","while i < 6:\n","    if i == 1:\n","        i += 1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_fall_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [91.0559999999851], 0.05: [91.52499999998567], 0.1: [91.23999999998425], 0.15: [91.29499999998501], 0.2: [88.57199999998473], 0.25: [88.52199999998429], 0.3: [84.70899999998248], 0.35: [90.07999999998634], 0.4: [80.52299999998537]}\n","1 range: {0.0: [91.0559999999851, 80.70299999997947], 0.05: [91.52499999998567, 80.75899999998009], 0.1: [91.23999999998425, 83.51999999998402], 0.15: [91.29499999998501, 79.53999999997903], 0.2: [88.57199999998473, 85.14599999998109], 0.25: [88.52199999998429, 75.16499999998868], 0.3: [84.70899999998248, 83.99999999998253], 0.35: [90.07999999998634, 68.18599999998632], 0.4: [80.52299999998537, 72.64399999998317]}\n","2 range: {0.0: [91.0559999999851, 80.70299999997947, 93.88899999998945], 0.05: [91.52499999998567, 80.75899999998009, 94.04799999998957], 0.1: [91.23999999998425, 83.51999999998402, 94.19099999998997], 0.15: [91.29499999998501, 79.53999999997903, 93.73299999998866], 0.2: [88.57199999998473, 85.14599999998109, 93.9079999999893], 0.25: [88.52199999998429, 75.16499999998868, 91.17599999998635], 0.3: [84.70899999998248, 83.99999999998253, 82.11799999998641], 0.35: [90.07999999998634, 68.18599999998632, 85.41399999999098], 0.4: [80.52299999998537, 72.64399999998317, 75.16499999998987]}\n","3 range: {0.0: [91.0559999999851, 80.70299999997947, 93.88899999998945, 89.2689999999821], 0.05: [91.52499999998567, 80.75899999998009, 94.04799999998957, 89.95299999998306], 0.1: [91.23999999998425, 83.51999999998402, 94.19099999998997, 89.62599999998247], 0.15: [91.29499999998501, 79.53999999997903, 93.73299999998866, 87.23599999998511], 0.2: [88.57199999998473, 85.14599999998109, 93.9079999999893, 85.98999999998337], 0.25: [88.52199999998429, 75.16499999998868, 91.17599999998635, 84.5109999999832], 0.3: [84.70899999998248, 83.99999999998253, 82.11799999998641, 86.2559999999815], 0.35: [90.07999999998634, 68.18599999998632, 85.41399999999098, 82.05499999998847], 0.4: [80.52299999998537, 72.64399999998317, 75.16499999998987, 80.17199999998553]}\n","4 range: {0.0: [91.0559999999851, 80.70299999997947, 93.88899999998945, 89.2689999999821, 95.48299999999148], 0.05: [91.52499999998567, 80.75899999998009, 94.04799999998957, 89.95299999998306, 95.44999999999163], 0.1: [91.23999999998425, 83.51999999998402, 94.19099999998997, 89.62599999998247, 93.33899999999021], 0.15: [91.29499999998501, 79.53999999997903, 93.73299999998866, 87.23599999998511, 86.50299999999132], 0.2: [88.57199999998473, 85.14599999998109, 93.9079999999893, 85.98999999998337, 78.36599999998542], 0.25: [88.52199999998429, 75.16499999998868, 91.17599999998635, 84.5109999999832, 79.97799999998475], 0.3: [84.70899999998248, 83.99999999998253, 82.11799999998641, 86.2559999999815, 67.24199999998605], 0.35: [90.07999999998634, 68.18599999998632, 85.41399999999098, 82.05499999998847, 55.21899999999628], 0.4: [80.52299999998537, 72.64399999998317, 75.16499999998987, 80.17199999998553, 48.33499999999078]}\n","5 range: {0.0: [91.0559999999851, 80.70299999997947, 93.88899999998945, 89.2689999999821, 95.48299999999148, 91.61999999998558], 0.05: [91.52499999998567, 80.75899999998009, 94.04799999998957, 89.95299999998306, 95.44999999999163, 91.47899999998528], 0.1: [91.23999999998425, 83.51999999998402, 94.19099999998997, 89.62599999998247, 93.33899999999021, 91.38299999998509], 0.15: [91.29499999998501, 79.53999999997903, 93.73299999998866, 87.23599999998511, 86.50299999999132, 91.40899999998517], 0.2: [88.57199999998473, 85.14599999998109, 93.9079999999893, 85.98999999998337, 78.36599999998542, 91.39499999998509], 0.25: [88.52199999998429, 75.16499999998868, 91.17599999998635, 84.5109999999832, 79.97799999998475, 88.47899999998684], 0.3: [84.70899999998248, 83.99999999998253, 82.11799999998641, 86.2559999999815, 67.24199999998605, 89.85499999998343], 0.35: [90.07999999998634, 68.18599999998632, 85.41399999999098, 82.05499999998847, 55.21899999999628, 85.19799999998271], 0.4: [80.52299999998537, 72.64399999998317, 75.16499999998987, 80.17199999998553, 48.33499999999078, 83.80699999998103]}\n","----\n","range: {0.0: [91.0559999999851, 80.70299999997947, 93.88899999998945, 89.2689999999821, 95.48299999999148, 91.61999999998558], 0.05: [91.52499999998567, 80.75899999998009, 94.04799999998957, 89.95299999998306, 95.44999999999163, 91.47899999998528], 0.1: [91.23999999998425, 83.51999999998402, 94.19099999998997, 89.62599999998247, 93.33899999999021, 91.38299999998509], 0.15: [91.29499999998501, 79.53999999997903, 93.73299999998866, 87.23599999998511, 86.50299999999132, 91.40899999998517], 0.2: [88.57199999998473, 85.14599999998109, 93.9079999999893, 85.98999999998337, 78.36599999998542, 91.39499999998509], 0.25: [88.52199999998429, 75.16499999998868, 91.17599999998635, 84.5109999999832, 79.97799999998475, 88.47899999998684], 0.3: [84.70899999998248, 83.99999999998253, 82.11799999998641, 86.2559999999815, 67.24199999998605, 89.85499999998343], 0.35: [90.07999999998634, 68.18599999998632, 85.41399999999098, 82.05499999998847, 55.21899999999628, 85.19799999998271], 0.4: [80.52299999998537, 72.64399999998317, 75.16499999998987, 80.17199999998553, 48.33499999999078, 83.80699999998103]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fXVuyJCHOES6"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmuu11RLSSo1","executionInfo":{"status":"ok","timestamp":1612798729883,"user_tz":-60,"elapsed":63303,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"d32abe58-f333-4cba-d4c1-d7b0a59209b7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 6:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","torch.save(episodes, \"PointFall_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RqCEeJ2WSZ3F"},"source":[""],"execution_count":null,"outputs":[]}]}