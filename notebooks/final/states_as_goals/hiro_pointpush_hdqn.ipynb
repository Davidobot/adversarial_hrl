{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointpush_hdqn.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617460034724,"user_tz":-60,"elapsed":17177,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"4171e553-2903-494e-b147-5499267b0027"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_push.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617460034725,"user_tz":-60,"elapsed":8658,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push_hdqn/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.meta_controller.state_dict(),\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push_hdqn/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.load_state_dict(checkpoint['meta_controller'])\n","    model.meta_controller_target = copy.deepcopy(model.meta_controller)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617460037588,"user_tz":-60,"elapsed":11257,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617460037589,"user_tz":-60,"elapsed":11021,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617460037589,"user_tz":-60,"elapsed":10763,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_push import PointPushEnv \n","env = NormalizedEnv(PointPushEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617460037590,"user_tz":-60,"elapsed":10331,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617460037590,"user_tz":-60,"elapsed":10097,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617460038105,"user_tz":-60,"elapsed":10379,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617460038106,"user_tz":-60,"elapsed":9924,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617460038427,"user_tz":-60,"elapsed":10026,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 10000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 100000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.0\n","        self.EPS_DECAY = 10000 # in number of steps\n","        self.TAU = 0.001\n","\n","        self.eps_printed = False\n","\n","        self.policy_update = 2\n","        self.tot_updates = 0\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, warmup, is_training):\n","        if warmup: \n","            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            if eps_threshold <= 0.2 and not self.eps_printed:\n","                self.eps_printed = True\n","                print(\"EPS_THRESHOLD below 0.2\")\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.tot_updates += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        loss_val = loss.item()\n","\n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","\n","        if self.tot_updates % self.policy_update == 0:\n","            soft_update(target, self, self.TAU)\n","\n","        return loss_val"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617460038428,"user_tz":-60,"elapsed":9774,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        learning_rate = 2.5e-4\n","        self.meta_controller = DQN(nb_states, 11 * 11).to(device)\n","        self.meta_controller_optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n","        self.meta_controller_target = DQN(nb_states, 11 * 11, mem_len = 0).to(device)\n","        self.meta_controller_target.eval()\n","\n","        self.max_goal_dist = torch.from_numpy(np.array([2.5, 2.5])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0., 1.])).to(device)\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def action_to_2D(self, a):\n","        x = (a % 11)\n","        y = (a // 11)\n","        return -1.0 + 0.2 * torch.cat([x, y], axis=1).float()\n","\n","    def convert_goal(self, a):\n","        return self.action_to_2D(a) * self.max_goal_dist + self.goal_offset\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        return self.meta_controller.act(s_t, warmup, is_training)\n","      \n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 4\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 12000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal_raw = agent.select_goal(state, i_episode <= warmup, True)\n","            goal = agent.convert_goal(goal_raw)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal_raw, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -47.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    # initial forward pass\n","    goal = agent.meta_controller(g_state)\n","\n","    if targeted:\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = F.mse_loss(goal, -goal) # doing -goal inverts the argmax \n","\n","    agent.meta_controller.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0] * (11 * 11)], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal_raw = agent.select_goal(og_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal_raw = agent.select_goal(state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od4IvIuPuNlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617418841776,"user_tz":-60,"elapsed":52764802,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"f3f603db-1ec3-4cb8-e827-89660d0043c7"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\n","untargeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['both', 'goal_only', 'action_only']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 4\n","while i < 8:\n","    if i == 0:\n","        agent = HIRO(n_observations, n_actions).to(device)\n","        load_model(agent, f\"hiro_{i}\")\n","    else:\n","        agent = train_model()\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\n","#i = 0\n","#while i < 13:\n","#    #agent = train_model()\n","#    agent = HIRO(n_observations, n_actions).to(device)\n","#    load_model(agent, f\"hiro_push_{i}\")\n","\n","#    if agent is not None:   \n","#      apply_fgsm(agent, targeted['both'], True, True, False)\n","#      apply_fgsm(agent, targeted['goal_only'], True, False, False)\n","#      apply_fgsm(agent, targeted['action_only'], False, True, False)   \n","#      print(f\"{i} fgsm (ut): {targeted}\")\n","#      i += 1\n","\n","#print(\"----\")\n","#print(f\"fgsm (ut): {targeted}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100: -46.44100000000042\n","200: -26.193000000000378\n","EPS_THRESHOLD below 0.2\n","300: -21.81700000000036\n","400: -37.493000000000414\n","500: -34.3750000000004\n","600: -35.12900000000041\n","700: -25.10600000000038\n","800: -27.10400000000038\n","900: -28.93700000000038\n","1000: -34.7490000000004\n","1100: -28.71900000000039\n","1200: -34.4300000000004\n","1300: -20.742000000000356\n","1400: -25.256000000000377\n","1500: -37.79300000000041\n","1600: -27.87500000000039\n","1700: -14.842000000000347\n","1800: -25.451000000000377\n","1900: -18.357000000000344\n","2000: -27.702000000000382\n","2100: -22.606000000000357\n","2200: -18.931000000000353\n","2300: 3.1949999999997085\n","2400: -14.388000000000345\n","2500: -6.6610000000003176\n","2600: 3.875999999999715\n","2700: -8.789000000000321\n","2800: 5.215999999999724\n","2900: -2.694000000000301\n","3000: -4.196000000000307\n","3100: -6.3280000000003085\n","3200: -18.801000000000343\n","3300: -0.895000000000296\n","3400: 2.9479999999997206\n","3500: 6.047999999999729\n","3600: -8.469000000000314\n","3700: 19.055999999999763\n","3800: -13.672000000000333\n","3900: -6.35200000000032\n","4000: 5.586999999999723\n","4100: 5.4089999999997325\n","4200: -26.796000000000376\n","4300: 8.542999999999735\n","4400: 19.316999999999762\n","4500: 27.9649999999998\n","4600: 24.618999999999787\n","4700: 21.395999999999773\n","4800: 22.632999999999782\n","4900: 28.8829999999998\n","5000: 44.59599999999985\n","5100: 45.680999999999855\n","5200: 65.10699999999991\n","5300: 55.68099999999987\n","5400: 47.79699999999984\n","5500: 73.82099999999993\n","5600: 49.53499999999986\n","5700: 33.4469999999998\n","5800: 53.057999999999865\n","5900: 61.31299999999987\n","6000: 53.24199999999987\n","6100: 50.017999999999866\n","6200: 64.3689999999999\n","6300: 66.84899999999992\n","6400: 63.40499999999993\n","6500: 64.56499999999991\n","6600: 68.93799999999992\n","6700: 78.80899999999994\n","6800: 81.61199999999997\n","6900: 67.71299999999991\n","7000: 42.430999999999834\n","7100: 78.51299999999996\n","7200: 64.04999999999991\n","7300: 74.69799999999992\n","7400: 78.09599999999995\n","7500: 84.73299999999996\n","7600: 82.61999999999996\n","7700: 83.56999999999998\n","Solved after 7772 episodes!\n","4 noise_hrl: {'both': {0.0: [96.1429999999931], 0.05: [41.2690000000128], 0.1: [32.656000000012774], 0.15: [9.064999999993674], 0.2: [1.0399999999992025], 0.25: [10.563000000001237], 0.3: [6.919999999994555], 0.35: [-12.05199999999988], 0.4: [-22.604999999976442], 0.45: [-23.084999999985776], 0.5: [-30.448999999971566]}, 'action_only': {0.0: [95.71299999999309], 0.05: [45.59700000000473], 0.1: [27.26200000001424], 0.15: [21.205000000012134], 0.2: [3.9089999999933966], 0.25: [-2.372000000001199], 0.3: [-11.418999999998427], 0.35: [-20.84499999997825], 0.4: [-26.629999999971503], 0.45: [-33.95299999996912], 0.5: [-31.98399999997388]}, 'goal_only': {0.0: [96.15199999999308], 0.05: [92.76999999999128], 0.1: [95.00899999999201], 0.15: [95.83599999999142], 0.2: [97.01299999999526], 0.25: [90.95999999999161], 0.3: [92.56399999999427], 0.35: [91.60499999999514], 0.4: [89.95599999998824], 0.45: [84.03199999998795], 0.5: [84.17399999999448]}, 'both_same': {0.0: [95.56899999999173], 0.05: [45.79700000000359], 0.1: [20.786000000004275], 0.15: [22.779000000010566], 0.2: [14.389000000014635], 0.25: [1.5169999999945125], 0.3: [-7.9980000000033655], 0.35: [-14.173999999985387], 0.4: [-18.632999999987398], 0.45: [-21.560999999987487], 0.5: [-29.229999999970957]}}\n","100: -46.480000000000416\n","200: -27.45400000000039\n","EPS_THRESHOLD below 0.2\n","300: -41.177000000000426\n","400: -42.877000000000415\n","500: -36.0790000000004\n","600: -33.91700000000041\n","700: -34.068000000000396\n","800: -24.27200000000037\n","900: -25.510000000000375\n","1000: -14.964000000000341\n","1100: -14.03200000000034\n","1200: -21.33000000000036\n","1300: -27.322000000000376\n","1400: -35.5810000000004\n","1500: -28.122000000000376\n","1600: -18.785000000000352\n","1700: -25.240000000000364\n","1800: -16.02000000000034\n","1900: -15.259000000000333\n","2000: -24.793000000000365\n","2100: -35.1490000000004\n","2200: -33.983000000000395\n","2300: 1.548999999999714\n","2400: -13.416000000000329\n","2500: -16.254000000000335\n","2600: -28.156000000000372\n","2700: -33.2890000000004\n","2800: -19.82700000000035\n","2900: -9.545000000000314\n","3000: -27.533000000000374\n","3100: -27.42400000000038\n","3200: -34.2170000000004\n","3300: -34.083000000000396\n","3400: -19.987000000000354\n","3500: -28.266000000000382\n","3600: -23.85000000000036\n","3700: -11.065000000000328\n","3800: 3.5899999999997236\n","3900: -8.765000000000311\n","4000: 6.768999999999737\n","4100: 15.36399999999976\n","4200: 11.776999999999752\n","4300: 13.70399999999975\n","4400: -21.842000000000358\n","4500: 12.395999999999747\n","4600: -21.191000000000354\n","4700: -35.5470000000004\n","4800: 4.369999999999729\n","4900: 21.46499999999977\n","5000: 49.005999999999865\n","5100: 47.215999999999866\n","5200: -11.586000000000322\n","5300: -30.021000000000384\n","5400: 5.041999999999725\n","5500: 23.37499999999979\n","5600: 5.142999999999727\n","5700: 11.171999999999748\n","5800: -21.399000000000353\n","5900: -4.662000000000299\n","6000: 16.459999999999763\n","6100: 27.301999999999794\n","6200: 25.699999999999786\n","6300: 47.28599999999986\n","6400: 26.627999999999794\n","6500: 39.66699999999983\n","6600: 49.51199999999986\n","6700: 68.90299999999992\n","6800: 60.98899999999989\n","6900: 65.14399999999992\n","7000: 51.265999999999856\n","7100: 51.62199999999988\n","7200: 63.30699999999991\n","7300: 70.8909999999999\n","7400: 58.93899999999989\n","7500: 67.81399999999992\n","7600: 75.31099999999995\n","7700: 75.13099999999993\n","7800: 55.23099999999989\n","7900: 52.54699999999987\n","8000: 63.0869999999999\n","8100: 73.90299999999993\n","8200: 72.31799999999994\n","8300: 76.99599999999994\n","8400: 84.45199999999997\n","8500: 78.46599999999995\n","8600: 80.03999999999995\n","8700: 84.51599999999999\n","8800: 86.10599999999995\n","Solved after 8822 episodes!\n","5 noise_hrl: {'both': {0.0: [96.1429999999931, 98.13499999999645], 0.05: [41.2690000000128, 14.969000000011869], 0.1: [32.656000000012774, -13.789999999993347], 0.15: [9.064999999993674, -19.634999999983094], 0.2: [1.0399999999992025, -11.475999999998827], 0.25: [10.563000000001237, -13.063999999991879], 0.3: [6.919999999994555, -9.648999999998258], 0.35: [-12.05199999999988, -5.283000000003792], 0.4: [-22.604999999976442, -11.820999999997827], 0.45: [-23.084999999985776, -18.585999999991653], 0.5: [-30.448999999971566, -18.719999999990783]}, 'action_only': {0.0: [95.71299999999309, 98.10599999999658], 0.05: [45.59700000000473, 14.79900000000871], 0.1: [27.26200000001424, -25.057999999976655], 0.15: [21.205000000012134, -28.10299999997768], 0.2: [3.9089999999933966, -14.19899999999675], 0.25: [-2.372000000001199, -13.050999999988969], 0.3: [-11.418999999998427, -1.6640000000012913], 0.35: [-20.84499999997825, -4.439000000002938], 0.4: [-26.629999999971503, -12.556999999992918], 0.45: [-33.95299999996912, -15.550999999994817], 0.5: [-31.98399999997388, -9.868000000005646]}, 'goal_only': {0.0: [96.15199999999308, 98.28499999999694], 0.05: [92.76999999999128, 98.30799999999635], 0.1: [95.00899999999201, 98.29899999999687], 0.15: [95.83599999999142, 98.40799999999666], 0.2: [97.01299999999526, 90.94199999999844], 0.25: [90.95999999999161, 82.07299999999212], 0.3: [92.56399999999427, 68.5939999999895], 0.35: [91.60499999999514, 68.64999999998932], 0.4: [89.95599999998824, 65.37299999999139], 0.45: [84.03199999998795, 46.100000000006176], 0.5: [84.17399999999448, 46.14699999999815]}, 'both_same': {0.0: [95.56899999999173, 96.68099999999669], 0.05: [45.79700000000359, 3.0119999999953735], 0.1: [20.786000000004275, -17.799999999996455], 0.15: [22.779000000010566, -18.380999999989058], 0.2: [14.389000000014635, -1.207000000003226], 0.25: [1.5169999999945125, -4.251000000001948], 0.3: [-7.9980000000033655, -12.355999999984649], 0.35: [-14.173999999985387, -16.470999999987818], 0.4: [-18.632999999987398, -22.025999999986553], 0.45: [-21.560999999987487, -14.444999999992708], 0.5: [-29.229999999970957, -21.994999999988707]}}\n","100: -41.43500000000042\n","200: -40.33500000000043\n","EPS_THRESHOLD below 0.2\n","300: -33.4560000000004\n","400: -46.044000000000416\n","500: -36.26900000000042\n","600: -41.02900000000042\n","700: -42.0010000000004\n","800: -31.204000000000388\n","900: -21.888000000000364\n","1000: -26.04400000000037\n","1100: -40.77200000000042\n","1200: -24.580000000000364\n","1300: -26.34400000000037\n","1400: -13.802000000000337\n","1500: -22.371000000000365\n","1600: -26.05900000000037\n","1700: -28.620000000000392\n","1800: -34.3520000000004\n","1900: -24.494000000000366\n","2000: -30.003000000000387\n","2100: -21.905000000000367\n","2200: -14.122000000000344\n","2300: -25.687000000000364\n","2400: -16.32400000000034\n","2500: -17.462000000000348\n","2600: -15.212000000000346\n","2700: -20.32600000000036\n","2800: -19.972000000000353\n","2900: -11.124000000000326\n","3000: 5.923999999999731\n","3100: -17.584000000000344\n","3200: 13.502999999999748\n","3300: 29.26099999999979\n","3400: 25.50699999999979\n","3500: -4.238000000000304\n","3600: -1.8930000000002951\n","3700: 9.32599999999973\n","3800: 47.680999999999855\n","3900: 63.7829999999999\n","4000: 29.525999999999794\n","4100: 56.46599999999987\n","4200: 67.76999999999992\n","4300: 59.64799999999992\n","4400: 63.960999999999906\n","4500: 77.21899999999994\n","4600: 67.8909999999999\n","4700: 74.97099999999996\n","4800: 76.03099999999992\n","4900: 70.42699999999992\n","5000: 79.12799999999996\n","5100: 74.81599999999995\n","5200: 77.14399999999993\n","5300: 83.92399999999996\n","5400: 77.08799999999994\n","Solved after 5457 episodes!\n","6 noise_hrl: {'both': {0.0: [96.1429999999931, 98.13499999999645, 93.7829999999928], 0.05: [41.2690000000128, 14.969000000011869, 16.188000000003534], 0.1: [32.656000000012774, -13.789999999993347, 32.09900000001799], 0.15: [9.064999999993674, -19.634999999983094, 7.805999999995798], 0.2: [1.0399999999992025, -11.475999999998827, 14.737000000010987], 0.25: [10.563000000001237, -13.063999999991879, -16.36499999999324], 0.3: [6.919999999994555, -9.648999999998258, -14.166999999989637], 0.35: [-12.05199999999988, -5.283000000003792, -15.354999999996217], 0.4: [-22.604999999976442, -11.820999999997827, -13.502999999996526], 0.45: [-23.084999999985776, -18.585999999991653, -21.990999999988393], 0.5: [-30.448999999971566, -18.719999999990783, -20.816999999976098]}, 'action_only': {0.0: [95.71299999999309, 98.10599999999658, 95.26199999999324], 0.05: [45.59700000000473, 14.79900000000871, 12.67200000000146], 0.1: [27.26200000001424, -25.057999999976655, 22.236000000018684], 0.15: [21.205000000012134, -28.10299999997768, 13.005000000003038], 0.2: [3.9089999999933966, -14.19899999999675, -4.367000000002723], 0.25: [-2.372000000001199, -13.050999999988969, -12.087000000003455], 0.3: [-11.418999999998427, -1.6640000000012913, -10.876999999997102], 0.35: [-20.84499999997825, -4.439000000002938, -21.595999999981736], 0.4: [-26.629999999971503, -12.556999999992918, -16.447999999982166], 0.45: [-33.95299999996912, -15.550999999994817, -18.254999999984303], 0.5: [-31.98399999997388, -9.868000000005646, -20.011999999992884]}, 'goal_only': {0.0: [96.15199999999308, 98.28499999999694, 95.30699999999737], 0.05: [92.76999999999128, 98.30799999999635, 87.66099999999511], 0.1: [95.00899999999201, 98.29899999999687, 71.66499999998472], 0.15: [95.83599999999142, 98.40799999999666, 76.05899999998461], 0.2: [97.01299999999526, 90.94199999999844, 77.02699999998795], 0.25: [90.95999999999161, 82.07299999999212, 73.38599999998624], 0.3: [92.56399999999427, 68.5939999999895, 70.50799999998975], 0.35: [91.60499999999514, 68.64999999998932, 73.93199999997641], 0.4: [89.95599999998824, 65.37299999999139, 74.64999999999227], 0.45: [84.03199999998795, 46.100000000006176, 55.76899999999276], 0.5: [84.17399999999448, 46.14699999999815, 56.5309999999838]}, 'both_same': {0.0: [95.56899999999173, 96.68099999999669, 93.77699999999142], 0.05: [45.79700000000359, 3.0119999999953735, 12.844000000012903], 0.1: [20.786000000004275, -17.799999999996455, 33.101000000016626], 0.15: [22.779000000010566, -18.380999999989058, 3.902999999997156], 0.2: [14.389000000014635, -1.207000000003226, 1.997000000000214], 0.25: [1.5169999999945125, -4.251000000001948, -5.3970000000040965], 0.3: [-7.9980000000033655, -12.355999999984649, -2.2190000000001233], 0.35: [-14.173999999985387, -16.470999999987818, -14.757999999997743], 0.4: [-18.632999999987398, -22.025999999986553, -24.86299999997766], 0.45: [-21.560999999987487, -14.444999999992708, -15.002999999988168], 0.5: [-29.229999999970957, -21.994999999988707, -18.364999999979247]}}\n","100: -42.78200000000041\n","200: -40.378000000000426\n","EPS_THRESHOLD below 0.2\n","300: -26.216000000000378\n","400: -34.49600000000041\n","500: -21.710000000000363\n","600: -19.192000000000363\n","700: -37.917000000000414\n","800: -20.182000000000357\n","900: -26.806000000000378\n","1000: -27.485000000000372\n","1100: -35.6260000000004\n","1200: -35.4430000000004\n","1300: -22.74100000000036\n","1400: -30.203000000000387\n","1500: -25.77100000000037\n","1600: -19.614000000000356\n","1700: -34.099000000000395\n","1800: -35.866000000000405\n","1900: -38.93300000000042\n","2000: -34.6250000000004\n","2100: -35.354000000000404\n","2200: -21.29500000000038\n","2300: -9.214000000000336\n","2400: -19.260000000000353\n","2500: -6.573000000000348\n","2600: -18.63700000000037\n","2700: -11.283000000000342\n","2800: -23.219000000000364\n","2900: -21.740000000000368\n","3000: -32.239000000000395\n","3100: -28.852000000000377\n","3200: -9.355000000000333\n","3300: -10.251000000000332\n","3400: -27.218000000000373\n","3500: -25.462000000000376\n","3600: -30.13000000000039\n","3700: -15.181000000000346\n","3800: -16.167000000000343\n","3900: 4.277999999999718\n","4000: -3.647000000000314\n","4100: -20.817000000000363\n","4200: -21.059000000000356\n","4300: -14.120000000000331\n","4400: -10.231000000000325\n","4500: -19.425000000000356\n","4600: -14.464000000000338\n","4700: -10.382000000000328\n","4800: -18.393000000000345\n","4900: -15.500000000000346\n","5000: -3.612000000000312\n","5100: -16.331000000000348\n","5200: -2.3460000000003096\n","5300: -0.185000000000298\n","5400: 5.45799999999973\n","5500: 30.093999999999806\n","5600: 26.635999999999783\n","5700: 20.63499999999976\n","5800: 1.240999999999702\n","5900: 29.6869999999998\n","6000: 8.589999999999737\n","6100: 46.54899999999985\n","6200: 57.34999999999989\n","6300: 64.73199999999991\n","6400: 60.34399999999991\n","6500: 66.60499999999992\n","6600: 62.9269999999999\n","6700: 53.66999999999988\n","6800: 45.73399999999985\n","6900: 71.47499999999992\n","7000: 61.058999999999884\n","7100: 58.15299999999988\n","7200: 70.76899999999993\n","7300: 73.99599999999995\n","7400: 74.56999999999994\n","7500: 73.51099999999992\n","7600: 81.63999999999996\n","7700: 68.87099999999992\n","7800: 81.04999999999995\n","7900: 77.76199999999993\n","8000: 82.87299999999996\n","8100: 84.10099999999997\n","8200: 88.70699999999995\n","Solved after 8211 episodes!\n","7 noise_hrl: {'both': {0.0: [96.1429999999931, 98.13499999999645, 93.7829999999928, 43.371000000012764], 0.05: [41.2690000000128, 14.969000000011869, 16.188000000003534, 97.21999999999473], 0.1: [32.656000000012774, -13.789999999993347, 32.09900000001799, 73.49499999997947], 0.15: [9.064999999993674, -19.634999999983094, 7.805999999995798, 32.85100000001008], 0.2: [1.0399999999992025, -11.475999999998827, 14.737000000010987, 11.570000000009548], 0.25: [10.563000000001237, -13.063999999991879, -16.36499999999324, 5.692999999997991], 0.3: [6.919999999994555, -9.648999999998258, -14.166999999989637, -4.13100000000272], 0.35: [-12.05199999999988, -5.283000000003792, -15.354999999996217, -2.5960000000031145], 0.4: [-22.604999999976442, -11.820999999997827, -13.502999999996526, -7.626000000004815], 0.45: [-23.084999999985776, -18.585999999991653, -21.990999999988393, -14.480999999999213], 0.5: [-30.448999999971566, -18.719999999990783, -20.816999999976098, -10.14399999999253]}, 'action_only': {0.0: [95.71299999999309, 98.10599999999658, 95.26199999999324, 37.71800000001806], 0.05: [45.59700000000473, 14.79900000000871, 12.67200000000146, 97.08199999999457], 0.1: [27.26200000001424, -25.057999999976655, 22.236000000018684, 73.6269999999822], 0.15: [21.205000000012134, -28.10299999997768, 13.005000000003038, 21.320000000013874], 0.2: [3.9089999999933966, -14.19899999999675, -4.367000000002723, 17.24800000000297], 0.25: [-2.372000000001199, -13.050999999988969, -12.087000000003455, 4.192999999997368], 0.3: [-11.418999999998427, -1.6640000000012913, -10.876999999997102, -8.735000000001797], 0.35: [-20.84499999997825, -4.439000000002938, -21.595999999981736, -6.50700000000316], 0.4: [-26.629999999971503, -12.556999999992918, -16.447999999982166, -13.035000000000032], 0.45: [-33.95299999996912, -15.550999999994817, -18.254999999984303, 2.501000000001058], 0.5: [-31.98399999997388, -9.868000000005646, -20.011999999992884, -8.174000000008329]}, 'goal_only': {0.0: [96.15199999999308, 98.28499999999694, 95.30699999999737, 36.90200000001727], 0.05: [92.76999999999128, 98.30799999999635, 87.66099999999511, 96.15299999999375], 0.1: [95.00899999999201, 98.29899999999687, 71.66499999998472, 96.80699999999464], 0.15: [95.83599999999142, 98.40799999999666, 76.05899999998461, 97.48299999999506], 0.2: [97.01299999999526, 90.94199999999844, 77.02699999998795, 97.29299999999445], 0.25: [90.95999999999161, 82.07299999999212, 73.38599999998624, 93.3709999999952], 0.3: [92.56399999999427, 68.5939999999895, 70.50799999998975, 91.86599999999646], 0.35: [91.60499999999514, 68.64999999998932, 73.93199999997641, 82.83299999999204], 0.4: [89.95599999998824, 65.37299999999139, 74.64999999999227, 91.59099999998878], 0.45: [84.03199999998795, 46.100000000006176, 55.76899999999276, 79.86699999998373], 0.5: [84.17399999999448, 46.14699999999815, 56.5309999999838, 81.0779999999873]}, 'both_same': {0.0: [95.56899999999173, 96.68099999999669, 93.77699999999142, 47.458000000001945], 0.05: [45.79700000000359, 3.0119999999953735, 12.844000000012903, 94.18099999999133], 0.1: [20.786000000004275, -17.799999999996455, 33.101000000016626, 55.18999999999451], 0.15: [22.779000000010566, -18.380999999989058, 3.902999999997156, 35.871000000015854], 0.2: [14.389000000014635, -1.207000000003226, 1.997000000000214, 7.924999999994616], 0.25: [1.5169999999945125, -4.251000000001948, -5.3970000000040965, 4.044999999992927], 0.3: [-7.9980000000033655, -12.355999999984649, -2.2190000000001233, 6.360999999997257], 0.35: [-14.173999999985387, -16.470999999987818, -14.757999999997743, -5.02000000000602], 0.4: [-18.632999999987398, -22.025999999986553, -24.86299999997766, -11.641000000001457], 0.45: [-21.560999999987487, -14.444999999992708, -15.002999999988168, -6.317000000004302], 0.5: [-29.229999999970957, -21.994999999988707, -18.364999999979247, -3.786000000002772]}}\n","----\n","noise_hrl: {'both': {0.0: [96.1429999999931, 98.13499999999645, 93.7829999999928, 43.371000000012764], 0.05: [41.2690000000128, 14.969000000011869, 16.188000000003534, 97.21999999999473], 0.1: [32.656000000012774, -13.789999999993347, 32.09900000001799, 73.49499999997947], 0.15: [9.064999999993674, -19.634999999983094, 7.805999999995798, 32.85100000001008], 0.2: [1.0399999999992025, -11.475999999998827, 14.737000000010987, 11.570000000009548], 0.25: [10.563000000001237, -13.063999999991879, -16.36499999999324, 5.692999999997991], 0.3: [6.919999999994555, -9.648999999998258, -14.166999999989637, -4.13100000000272], 0.35: [-12.05199999999988, -5.283000000003792, -15.354999999996217, -2.5960000000031145], 0.4: [-22.604999999976442, -11.820999999997827, -13.502999999996526, -7.626000000004815], 0.45: [-23.084999999985776, -18.585999999991653, -21.990999999988393, -14.480999999999213], 0.5: [-30.448999999971566, -18.719999999990783, -20.816999999976098, -10.14399999999253]}, 'action_only': {0.0: [95.71299999999309, 98.10599999999658, 95.26199999999324, 37.71800000001806], 0.05: [45.59700000000473, 14.79900000000871, 12.67200000000146, 97.08199999999457], 0.1: [27.26200000001424, -25.057999999976655, 22.236000000018684, 73.6269999999822], 0.15: [21.205000000012134, -28.10299999997768, 13.005000000003038, 21.320000000013874], 0.2: [3.9089999999933966, -14.19899999999675, -4.367000000002723, 17.24800000000297], 0.25: [-2.372000000001199, -13.050999999988969, -12.087000000003455, 4.192999999997368], 0.3: [-11.418999999998427, -1.6640000000012913, -10.876999999997102, -8.735000000001797], 0.35: [-20.84499999997825, -4.439000000002938, -21.595999999981736, -6.50700000000316], 0.4: [-26.629999999971503, -12.556999999992918, -16.447999999982166, -13.035000000000032], 0.45: [-33.95299999996912, -15.550999999994817, -18.254999999984303, 2.501000000001058], 0.5: [-31.98399999997388, -9.868000000005646, -20.011999999992884, -8.174000000008329]}, 'goal_only': {0.0: [96.15199999999308, 98.28499999999694, 95.30699999999737, 36.90200000001727], 0.05: [92.76999999999128, 98.30799999999635, 87.66099999999511, 96.15299999999375], 0.1: [95.00899999999201, 98.29899999999687, 71.66499999998472, 96.80699999999464], 0.15: [95.83599999999142, 98.40799999999666, 76.05899999998461, 97.48299999999506], 0.2: [97.01299999999526, 90.94199999999844, 77.02699999998795, 97.29299999999445], 0.25: [90.95999999999161, 82.07299999999212, 73.38599999998624, 93.3709999999952], 0.3: [92.56399999999427, 68.5939999999895, 70.50799999998975, 91.86599999999646], 0.35: [91.60499999999514, 68.64999999998932, 73.93199999997641, 82.83299999999204], 0.4: [89.95599999998824, 65.37299999999139, 74.64999999999227, 91.59099999998878], 0.45: [84.03199999998795, 46.100000000006176, 55.76899999999276, 79.86699999998373], 0.5: [84.17399999999448, 46.14699999999815, 56.5309999999838, 81.0779999999873]}, 'both_same': {0.0: [95.56899999999173, 96.68099999999669, 93.77699999999142, 47.458000000001945], 0.05: [45.79700000000359, 3.0119999999953735, 12.844000000012903, 94.18099999999133], 0.1: [20.786000000004275, -17.799999999996455, 33.101000000016626, 55.18999999999451], 0.15: [22.779000000010566, -18.380999999989058, 3.902999999997156, 35.871000000015854], 0.2: [14.389000000014635, -1.207000000003226, 1.997000000000214, 7.924999999994616], 0.25: [1.5169999999945125, -4.251000000001948, -5.3970000000040965, 4.044999999992927], 0.3: [-7.9980000000033655, -12.355999999984649, -2.2190000000001233, 6.360999999997257], 0.35: [-14.173999999985387, -16.470999999987818, -14.757999999997743, -5.02000000000602], 0.4: [-18.632999999987398, -22.025999999986553, -24.86299999997766, -11.641000000001457], 0.45: [-21.560999999987487, -14.444999999992708, -15.002999999988168, -6.317000000004302], 0.5: [-29.229999999970957, -21.994999999988707, -18.364999999979247, -3.786000000002772]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyOkpm70eAIM","executionInfo":{"status":"ok","timestamp":1617448887763,"user_tz":-60,"elapsed":16584681,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"3ee8c04f-7b1b-4896-c23d-1cadcbd1edfc"},"source":["targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","for i in [0, 1, 4, 5, 6]:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 fgsm (ut): {'goal': {0.0: [98.03099999999594], 0.02: [67.78499999998444], 0.04: [15.729000000008236], 0.06: [7.596999999998364], 0.08: [23.9560000000084], 0.1: [5.371999999994931], 0.12: [0.7759999999957947], 0.14: [-12.347000000003339], 0.16: [-43.16099999998398], 0.18: [-50.00000000000659], 0.2: [-48.61100000000062]}, 'action': {0.0: [97.63399999999696], 0.02: [45.09299999998916], 0.04: [-12.89999999999932], 0.06: [-27.618999999973934], 0.08: [-33.438999999973895], 0.1: [-39.112999999971244], 0.12: [-50.00000000000659], 0.14: [-50.00000000000659], 0.16: [-48.82600000000141], 0.18: [-50.00000000000659], 0.2: [-48.758000000001715]}}\n","0 fgsm (t): {'goal': {0.0: [97.87899999999692], 0.02: [78.79099999998121], 0.04: [16.35000000000866], 0.06: [22.161000000009256], 0.08: [-3.3770000000015923], 0.1: [5.5679999999952665], 0.12: [-5.340000000004885], 0.14: [-20.358999999976014], 0.16: [-43.30799999997782], 0.18: [-48.61800000000065], 0.2: [-46.027999999989405]}, 'action': {0.0: [97.705999999996], 0.02: [74.03899999997911], 0.04: [38.257000000015445], 0.06: [60.144999999986204], 0.08: [12.016000000006995], 0.1: [7.055999999994604], 0.12: [-6.92500000000343], 0.14: [-17.50299999998795], 0.16: [-26.69199999997531], 0.18: [-31.572999999974055], 0.2: [-37.61099999997311]}}\n","1 fgsm (ut): {'goal': {0.0: [98.03099999999594, 97.71899999999587], 0.02: [67.78499999998444, 21.337000000009827], 0.04: [15.729000000008236, 15.589000000001228], 0.06: [7.596999999998364, 44.36400000000888], 0.08: [23.9560000000084, 31.568000000017005], 0.1: [5.371999999994931, 14.30500000000442], 0.12: [0.7759999999957947, 14.385000000003785], 0.14: [-12.347000000003339, -21.973999999976677], 0.16: [-43.16099999998398, -41.51699999997163], 0.18: [-50.00000000000659, -44.71799999998828], 0.2: [-48.61100000000062, -42.03299999997328]}, 'action': {0.0: [97.63399999999696, 97.7219999999958], 0.02: [45.09299999998916, 25.223000000018743], 0.04: [-12.89999999999932, -0.6200000000017024], 0.06: [-27.618999999973934, -37.255999999971586], 0.08: [-33.438999999973895, -37.82399999997013], 0.1: [-39.112999999971244, -42.13899999997594], 0.12: [-50.00000000000659, -41.03299999997226], 0.14: [-50.00000000000659, -38.56599999997199], 0.16: [-48.82600000000141, -42.0939999999793], 0.18: [-50.00000000000659, -48.69100000000205], 0.2: [-48.758000000001715, -45.24699999999475]}}\n","1 fgsm (t): {'goal': {0.0: [97.87899999999692, 97.7729999999959], 0.02: [78.79099999998121, 16.14200000001836], 0.04: [16.35000000000866, 37.34600000001765], 0.06: [22.161000000009256, 32.5020000000163], 0.08: [-3.3770000000015923, 41.7370000000174], 0.1: [5.5679999999952665, -1.7360000000015348], 0.12: [-5.340000000004885, 14.289000000003675], 0.14: [-20.358999999976014, -19.816999999989047], 0.16: [-43.30799999997782, -39.335999999970866], 0.18: [-48.61800000000065, -44.8499999999866], 0.2: [-46.027999999989405, -45.57199999998911]}, 'action': {0.0: [97.705999999996, 97.73999999999596], 0.02: [74.03899999997911, 61.90999999999057], 0.04: [38.257000000015445, 68.9239999999755], 0.06: [60.144999999986204, 31.046000000016594], 0.08: [12.016000000006995, 6.129999999997251], 0.1: [7.055999999994604, -0.4320000000019998], 0.12: [-6.92500000000343, -9.845000000005381], 0.14: [-17.50299999998795, -18.459999999991602], 0.16: [-26.69199999997531, -17.60299999998567], 0.18: [-31.572999999974055, -23.603999999978722], 0.2: [-37.61099999997311, -11.238000000000202]}}\n","4 fgsm (ut): {'goal': {0.0: [98.03099999999594, 97.71899999999587, 94.68199999999142], 0.02: [67.78499999998444, 21.337000000009827, 6.107999999994714], 0.04: [15.729000000008236, 15.589000000001228, -48.96100000000303], 0.06: [7.596999999998364, 44.36400000000888, -50.00000000000659], 0.08: [23.9560000000084, 31.568000000017005, -50.00000000000659], 0.1: [5.371999999994931, 14.30500000000442, -50.00000000000659], 0.12: [0.7759999999957947, 14.385000000003785, -50.00000000000659], 0.14: [-12.347000000003339, -21.973999999976677, -50.00000000000659], 0.16: [-43.16099999998398, -41.51699999997163, -50.00000000000659], 0.18: [-50.00000000000659, -44.71799999998828, -50.00000000000659], 0.2: [-48.61100000000062, -42.03299999997328, -50.00000000000659]}, 'action': {0.0: [97.63399999999696, 97.7219999999958, 96.117999999993], 0.02: [45.09299999998916, 25.223000000018743, 11.68700000000506], 0.04: [-12.89999999999932, -0.6200000000017024, -41.046999999971284], 0.06: [-27.618999999973934, -37.255999999971586, -40.78099999997011], 0.08: [-33.438999999973895, -37.82399999997013, -40.61299999997072], 0.1: [-39.112999999971244, -42.13899999997594, -45.40199999999645], 0.12: [-50.00000000000659, -41.03299999997226, -48.90300000000168], 0.14: [-50.00000000000659, -38.56599999997199, -50.00000000000659], 0.16: [-48.82600000000141, -42.0939999999793, -48.90100000000168], 0.18: [-50.00000000000659, -48.69100000000205, -48.86100000000267], 0.2: [-48.758000000001715, -45.24699999999475, -50.00000000000659]}}\n","4 fgsm (t): {'goal': {0.0: [97.87899999999692, 97.7729999999959, 95.97099999999327], 0.02: [78.79099999998121, 16.14200000001836, 10.426000000005843], 0.04: [16.35000000000866, 37.34600000001765, -47.493999999996504], 0.06: [22.161000000009256, 32.5020000000163, -50.00000000000659], 0.08: [-3.3770000000015923, 41.7370000000174, -50.00000000000659], 0.1: [5.5679999999952665, -1.7360000000015348, -50.00000000000659], 0.12: [-5.340000000004885, 14.289000000003675, -50.00000000000659], 0.14: [-20.358999999976014, -19.816999999989047, -50.00000000000659], 0.16: [-43.30799999997782, -39.335999999970866, -50.00000000000659], 0.18: [-48.61800000000065, -44.8499999999866, -48.99200000000201], 0.2: [-46.027999999989405, -45.57199999998911, -48.62700000000068]}, 'action': {0.0: [97.705999999996, 97.73999999999596, 91.34699999998901], 0.02: [74.03899999997911, 61.90999999999057, 90.3979999999876], 0.04: [38.257000000015445, 68.9239999999755, 71.15599999999021], 0.06: [60.144999999986204, 31.046000000016594, 53.60700000000306], 0.08: [12.016000000006995, 6.129999999997251, 50.21800000000973], 0.1: [7.055999999994604, -0.4320000000019998, 25.29200000001187], 0.12: [-6.92500000000343, -9.845000000005381, 6.764999999997969], 0.14: [-17.50299999998795, -18.459999999991602, 14.287000000006469], 0.16: [-26.69199999997531, -17.60299999998567, -2.7520000000026412], 0.18: [-31.572999999974055, -23.603999999978722, -11.756000000002569], 0.2: [-37.61099999997311, -11.238000000000202, -19.175999999982174]}}\n","5 fgsm (ut): {'goal': {0.0: [98.03099999999594, 97.71899999999587, 94.68199999999142, 98.13999999999682], 0.02: [67.78499999998444, 21.337000000009827, 6.107999999994714, 62.40199999999147], 0.04: [15.729000000008236, 15.589000000001228, -48.96100000000303, -2.0490000000011013], 0.06: [7.596999999998364, 44.36400000000888, -50.00000000000659, -18.24399999999145], 0.08: [23.9560000000084, 31.568000000017005, -50.00000000000659, -45.68799999998817], 0.1: [5.371999999994931, 14.30500000000442, -50.00000000000659, -45.663999999989215], 0.12: [0.7759999999957947, 14.385000000003785, -50.00000000000659, -50.00000000000659], 0.14: [-12.347000000003339, -21.973999999976677, -50.00000000000659, -45.6179999999909], 0.16: [-43.16099999998398, -41.51699999997163, -50.00000000000659, -37.14599999996989], 0.18: [-50.00000000000659, -44.71799999998828, -50.00000000000659, -48.724000000001034], 0.2: [-48.61100000000062, -42.03299999997328, -50.00000000000659, -42.80399999997692]}, 'action': {0.0: [97.63399999999696, 97.7219999999958, 96.117999999993, 98.05099999999628], 0.02: [45.09299999998916, 25.223000000018743, 11.68700000000506, 27.928000000006854], 0.04: [-12.89999999999932, -0.6200000000017024, -41.046999999971284, -43.367999999983596], 0.06: [-27.618999999973934, -37.255999999971586, -40.78099999997011, -50.00000000000659], 0.08: [-33.438999999973895, -37.82399999997013, -40.61299999997072, -47.50300000000114], 0.1: [-39.112999999971244, -42.13899999997594, -45.40199999999645, -50.00000000000659], 0.12: [-50.00000000000659, -41.03299999997226, -48.90300000000168, -50.00000000000659], 0.14: [-50.00000000000659, -38.56599999997199, -50.00000000000659, -50.00000000000659], 0.16: [-48.82600000000141, -42.0939999999793, -48.90100000000168, -50.00000000000659], 0.18: [-50.00000000000659, -48.69100000000205, -48.86100000000267, -47.358999999995156], 0.2: [-48.758000000001715, -45.24699999999475, -50.00000000000659, -47.51799999999574]}}\n","5 fgsm (t): {'goal': {0.0: [97.87899999999692, 97.7729999999959, 95.97099999999327, 98.2269999999968], 0.02: [78.79099999998121, 16.14200000001836, 10.426000000005843, 62.38999999998937], 0.04: [16.35000000000866, 37.34600000001765, -47.493999999996504, -5.027000000004879], 0.06: [22.161000000009256, 32.5020000000163, -50.00000000000659, -23.98599999997802], 0.08: [-3.3770000000015923, 41.7370000000174, -50.00000000000659, -43.08199999997687], 0.1: [5.5679999999952665, -1.7360000000015348, -50.00000000000659, -41.61499999997176], 0.12: [-5.340000000004885, 14.289000000003675, -50.00000000000659, -47.15399999999622], 0.14: [-20.358999999976014, -19.816999999989047, -50.00000000000659, -45.991999999989275], 0.16: [-43.30799999997782, -39.335999999970866, -50.00000000000659, -44.25899999998433], 0.18: [-48.61800000000065, -44.8499999999866, -48.99200000000201, -47.108999999998794], 0.2: [-46.027999999989405, -45.57199999998911, -48.62700000000068, -45.89799999999803]}, 'action': {0.0: [97.705999999996, 97.73999999999596, 91.34699999998901, 98.1689999999968], 0.02: [74.03899999997911, 61.90999999999057, 90.3979999999876, -26.333999999986222], 0.04: [38.257000000015445, 68.9239999999755, 71.15599999999021, -36.859999999972416], 0.06: [60.144999999986204, 31.046000000016594, 53.60700000000306, -21.633999999981928], 0.08: [12.016000000006995, 6.129999999997251, 50.21800000000973, -35.96799999997345], 0.1: [7.055999999994604, -0.4320000000019998, 25.29200000001187, -19.10499999998387], 0.12: [-6.92500000000343, -9.845000000005381, 6.764999999997969, -15.43699999999824], 0.14: [-17.50299999998795, -18.459999999991602, 14.287000000006469, -33.72599999996937], 0.16: [-26.69199999997531, -17.60299999998567, -2.7520000000026412, -23.039999999980658], 0.18: [-31.572999999974055, -23.603999999978722, -11.756000000002569, -44.66199999998353], 0.2: [-37.61099999997311, -11.238000000000202, -19.175999999982174, -36.54399999997158]}}\n","6 fgsm (ut): {'goal': {0.0: [98.03099999999594, 97.71899999999587, 94.68199999999142, 98.13999999999682, 95.26699999999491], 0.02: [67.78499999998444, 21.337000000009827, 6.107999999994714, 62.40199999999147, 59.535999999979474], 0.04: [15.729000000008236, 15.589000000001228, -48.96100000000303, -2.0490000000011013, 37.71100000002358], 0.06: [7.596999999998364, 44.36400000000888, -50.00000000000659, -18.24399999999145, 29.20900000002191], 0.08: [23.9560000000084, 31.568000000017005, -50.00000000000659, -45.68799999998817, -2.1220000000027897], 0.1: [5.371999999994931, 14.30500000000442, -50.00000000000659, -45.663999999989215, -7.804000000001326], 0.12: [0.7759999999957947, 14.385000000003785, -50.00000000000659, -50.00000000000659, -6.9080000000072825], 0.14: [-12.347000000003339, -21.973999999976677, -50.00000000000659, -45.6179999999909, -19.411999999981887], 0.16: [-43.16099999998398, -41.51699999997163, -50.00000000000659, -37.14599999996989, -38.15599999997148], 0.18: [-50.00000000000659, -44.71799999998828, -50.00000000000659, -48.724000000001034, -36.314999999971114], 0.2: [-48.61100000000062, -42.03299999997328, -50.00000000000659, -42.80399999997692, -39.51599999996774]}, 'action': {0.0: [97.63399999999696, 97.7219999999958, 96.117999999993, 98.05099999999628, 93.77199999999547], 0.02: [45.09299999998916, 25.223000000018743, 11.68700000000506, 27.928000000006854, 39.36300000002542], 0.04: [-12.89999999999932, -0.6200000000017024, -41.046999999971284, -43.367999999983596, 25.10300000001695], 0.06: [-27.618999999973934, -37.255999999971586, -40.78099999997011, -50.00000000000659, 25.787000000018633], 0.08: [-33.438999999973895, -37.82399999997013, -40.61299999997072, -47.50300000000114, 1.6359999999979475], 0.1: [-39.112999999971244, -42.13899999997594, -45.40199999999645, -50.00000000000659, -45.484999999987394], 0.12: [-50.00000000000659, -41.03299999997226, -48.90300000000168, -50.00000000000659, -50.00000000000659], 0.14: [-50.00000000000659, -38.56599999997199, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.16: [-48.82600000000141, -42.0939999999793, -48.90100000000168, -50.00000000000659, -50.00000000000659], 0.18: [-50.00000000000659, -48.69100000000205, -48.86100000000267, -47.358999999995156, -50.00000000000659], 0.2: [-48.758000000001715, -45.24699999999475, -50.00000000000659, -47.51799999999574, -48.6310000000007]}}\n","6 fgsm (t): {'goal': {0.0: [97.87899999999692, 97.7729999999959, 95.97099999999327, 98.2269999999968, 89.32499999999203], 0.02: [78.79099999998121, 16.14200000001836, 10.426000000005843, 62.38999999998937, 56.953999999998445], 0.04: [16.35000000000866, 37.34600000001765, -47.493999999996504, -5.027000000004879, 36.26400000002714], 0.06: [22.161000000009256, 32.5020000000163, -50.00000000000659, -23.98599999997802, 32.30000000002336], 0.08: [-3.3770000000015923, 41.7370000000174, -50.00000000000659, -43.08199999997687, 11.018999999995856], 0.1: [5.5679999999952665, -1.7360000000015348, -50.00000000000659, -41.61499999997176, -9.589000000008463], 0.12: [-5.340000000004885, 14.289000000003675, -50.00000000000659, -47.15399999999622, 0.9260000000005193], 0.14: [-20.358999999976014, -19.816999999989047, -50.00000000000659, -45.991999999989275, -22.271999999981585], 0.16: [-43.30799999997782, -39.335999999970866, -50.00000000000659, -44.25899999998433, -46.20499999999119], 0.18: [-48.61800000000065, -44.8499999999866, -48.99200000000201, -47.108999999998794, -43.56199999998885], 0.2: [-46.027999999989405, -45.57199999998911, -48.62700000000068, -45.89799999999803, -36.953999999973256]}, 'action': {0.0: [97.705999999996, 97.73999999999596, 91.34699999998901, 98.1689999999968, 92.29599999999449], 0.02: [74.03899999997911, 61.90999999999057, 90.3979999999876, -26.333999999986222, 59.97299999999014], 0.04: [38.257000000015445, 68.9239999999755, 71.15599999999021, -36.859999999972416, 40.58200000002008], 0.06: [60.144999999986204, 31.046000000016594, 53.60700000000306, -21.633999999981928, -35.15699999997034], 0.08: [12.016000000006995, 6.129999999997251, 50.21800000000973, -35.96799999997345, -29.749999999977476], 0.1: [7.055999999994604, -0.4320000000019998, 25.29200000001187, -19.10499999998387, -35.430999999971284], 0.12: [-6.92500000000343, -9.845000000005381, 6.764999999997969, -15.43699999999824, -30.946999999977464], 0.14: [-17.50299999998795, -18.459999999991602, 14.287000000006469, -33.72599999996937, 9.314999999995884], 0.16: [-26.69199999997531, -17.60299999998567, -2.7520000000026412, -23.039999999980658, -6.585999999996728], 0.18: [-31.572999999974055, -23.603999999978722, -11.756000000002569, -44.66199999998353, -9.237000000004834], 0.2: [-37.61099999997311, -11.238000000000202, -19.175999999982174, -36.54399999997158, -12.405999999992364]}}\n","----\n","fgsm (ut): {'goal': {0.0: [98.03099999999594, 97.71899999999587, 94.68199999999142, 98.13999999999682, 95.26699999999491], 0.02: [67.78499999998444, 21.337000000009827, 6.107999999994714, 62.40199999999147, 59.535999999979474], 0.04: [15.729000000008236, 15.589000000001228, -48.96100000000303, -2.0490000000011013, 37.71100000002358], 0.06: [7.596999999998364, 44.36400000000888, -50.00000000000659, -18.24399999999145, 29.20900000002191], 0.08: [23.9560000000084, 31.568000000017005, -50.00000000000659, -45.68799999998817, -2.1220000000027897], 0.1: [5.371999999994931, 14.30500000000442, -50.00000000000659, -45.663999999989215, -7.804000000001326], 0.12: [0.7759999999957947, 14.385000000003785, -50.00000000000659, -50.00000000000659, -6.9080000000072825], 0.14: [-12.347000000003339, -21.973999999976677, -50.00000000000659, -45.6179999999909, -19.411999999981887], 0.16: [-43.16099999998398, -41.51699999997163, -50.00000000000659, -37.14599999996989, -38.15599999997148], 0.18: [-50.00000000000659, -44.71799999998828, -50.00000000000659, -48.724000000001034, -36.314999999971114], 0.2: [-48.61100000000062, -42.03299999997328, -50.00000000000659, -42.80399999997692, -39.51599999996774]}, 'action': {0.0: [97.63399999999696, 97.7219999999958, 96.117999999993, 98.05099999999628, 93.77199999999547], 0.02: [45.09299999998916, 25.223000000018743, 11.68700000000506, 27.928000000006854, 39.36300000002542], 0.04: [-12.89999999999932, -0.6200000000017024, -41.046999999971284, -43.367999999983596, 25.10300000001695], 0.06: [-27.618999999973934, -37.255999999971586, -40.78099999997011, -50.00000000000659, 25.787000000018633], 0.08: [-33.438999999973895, -37.82399999997013, -40.61299999997072, -47.50300000000114, 1.6359999999979475], 0.1: [-39.112999999971244, -42.13899999997594, -45.40199999999645, -50.00000000000659, -45.484999999987394], 0.12: [-50.00000000000659, -41.03299999997226, -48.90300000000168, -50.00000000000659, -50.00000000000659], 0.14: [-50.00000000000659, -38.56599999997199, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.16: [-48.82600000000141, -42.0939999999793, -48.90100000000168, -50.00000000000659, -50.00000000000659], 0.18: [-50.00000000000659, -48.69100000000205, -48.86100000000267, -47.358999999995156, -50.00000000000659], 0.2: [-48.758000000001715, -45.24699999999475, -50.00000000000659, -47.51799999999574, -48.6310000000007]}}\n","fgsm (t): {'goal': {0.0: [97.87899999999692, 97.7729999999959, 95.97099999999327, 98.2269999999968, 89.32499999999203], 0.02: [78.79099999998121, 16.14200000001836, 10.426000000005843, 62.38999999998937, 56.953999999998445], 0.04: [16.35000000000866, 37.34600000001765, -47.493999999996504, -5.027000000004879, 36.26400000002714], 0.06: [22.161000000009256, 32.5020000000163, -50.00000000000659, -23.98599999997802, 32.30000000002336], 0.08: [-3.3770000000015923, 41.7370000000174, -50.00000000000659, -43.08199999997687, 11.018999999995856], 0.1: [5.5679999999952665, -1.7360000000015348, -50.00000000000659, -41.61499999997176, -9.589000000008463], 0.12: [-5.340000000004885, 14.289000000003675, -50.00000000000659, -47.15399999999622, 0.9260000000005193], 0.14: [-20.358999999976014, -19.816999999989047, -50.00000000000659, -45.991999999989275, -22.271999999981585], 0.16: [-43.30799999997782, -39.335999999970866, -50.00000000000659, -44.25899999998433, -46.20499999999119], 0.18: [-48.61800000000065, -44.8499999999866, -48.99200000000201, -47.108999999998794, -43.56199999998885], 0.2: [-46.027999999989405, -45.57199999998911, -48.62700000000068, -45.89799999999803, -36.953999999973256]}, 'action': {0.0: [97.705999999996, 97.73999999999596, 91.34699999998901, 98.1689999999968, 92.29599999999449], 0.02: [74.03899999997911, 61.90999999999057, 90.3979999999876, -26.333999999986222, 59.97299999999014], 0.04: [38.257000000015445, 68.9239999999755, 71.15599999999021, -36.859999999972416, 40.58200000002008], 0.06: [60.144999999986204, 31.046000000016594, 53.60700000000306, -21.633999999981928, -35.15699999997034], 0.08: [12.016000000006995, 6.129999999997251, 50.21800000000973, -35.96799999997345, -29.749999999977476], 0.1: [7.055999999994604, -0.4320000000019998, 25.29200000001187, -19.10499999998387, -35.430999999971284], 0.12: [-6.92500000000343, -9.845000000005381, 6.764999999997969, -15.43699999999824, -30.946999999977464], 0.14: [-17.50299999998795, -18.459999999991602, 14.287000000006469, -33.72599999996937, 9.314999999995884], 0.16: [-26.69199999997531, -17.60299999998567, -2.7520000000026412, -23.039999999980658, -6.585999999996728], 0.18: [-31.572999999974055, -23.603999999978722, -11.756000000002569, -44.66199999998353, -9.237000000004834], 0.2: [-37.61099999997311, -11.238000000000202, -19.175999999982174, -36.54399999997158, -12.405999999992364]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxh9qEUO16Az"},"source":["Solved after 9695 episodes!\n","Solved after 9134 episodes!\n","1 noise_hrl: {'both': {0.0: [97.46999999999527, 97.69699999999574], 0.05: [86.23699999997838, 56.576999999990136], 0.1: [34.21900000001736, 33.15700000001119], 0.15: [-9.768000000006756, 6.446999999996693], 0.2: [-29.58099999997852, -15.552999999994652], 0.25: [-35.631999999971725, -19.1929999999875], 0.3: [-42.43899999997449, -21.05499999999158], 0.35: [-44.99099999998676, -21.065999999985042], 0.4: [-38.34199999997063, -32.733999999971054], 0.45: [-36.190999999971616, -21.699999999978385], 0.5: [-31.057999999972775, -28.448999999974166]}, 'action_only': {0.0: [98.20199999999659, 97.79299999999597], 0.05: [73.04099999998135, 71.98599999999087], 0.1: [38.689000000011454, 32.16200000001837], 0.15: [-0.5350000000020939, -2.0710000000045357], 0.2: [-28.905999999974846, -14.108999999994742], 0.25: [-39.94599999997083, -24.54999999997766], 0.3: [-36.11799999997128, -34.81699999997497], 0.35: [-41.80899999997474, -34.2489999999706], 0.4: [-43.04899999997761, -34.53999999997176], 0.45: [-29.11599999997332, -41.22199999996942], 0.5: [-43.45199999997935, -26.097999999978345]}, 'goal_only': {0.0: [97.53699999999459, 97.6939999999958], 0.05: [98.25999999999671, 98.05999999999632], 0.1: [98.36199999999695, 98.1469999999968], 0.15: [96.85099999999518, 98.12799999999648], 0.2: [95.13499999999601, 91.58299999999376], 0.25: [95.36699999999344, 86.6709999999874], 0.3: [86.22899999998955, 78.81199999998665], 0.35: [85.8729999999934, 74.61299999999528], 0.4: [81.7329999999885, 69.98499999998192], 0.45: [81.7779999999887, 58.76000000000011], 0.5: [70.47899999999181, 71.94199999998554]}, 'both_same': {0.0: [97.86399999999561, 97.66499999999573], 0.05: [90.178999999989, 60.0349999999687], 0.1: [49.79799999999771, 37.18500000001147], 0.15: [10.969999999997752, 14.20000000000422], 0.2: [-23.030999999982704, -4.737000000005985], 0.25: [-40.16399999997147, -19.314999999983023], 0.3: [-40.1219999999681, -36.48299999997436], 0.35: [-40.83799999997313, -31.239999999976366], 0.4: [-33.625999999974, -31.201999999970923], 0.45: [-36.96799999997457, -26.946999999970966], 0.5: [-36.085999999975414, -28.83799999997012]}}\n","Solved after 7390 episodes!\n","Solved after 7360 episodes!\n","3 noise_hrl: {'both': {0.0: [42.21600000002324, 89.2129999999878], 0.05: [44.11300000000775, 59.10999999999389], 0.1: [12.290000000006183, 21.485000000007112], 0.15: [-11.47399999999868, 12.17300000000335], 0.2: [-21.357999999987527, -14.231999999991933], 0.25: [-29.883999999980897, -34.39899999997501], 0.3: [-26.52299999997229, -41.429999999970185], 0.35: [-33.2929999999718, -40.732999999968676], 0.4: [-35.60199999997248, -35.090999999972176], 0.45: [-32.73199999997401, -32.08699999997541], 0.5: [-26.674999999980535, -27.920999999976015]}, 'action_only': {0.0: [44.753000000001094, 96.66499999999478], 0.05: [49.01400000000039, 36.93600000001564], 0.1: [18.860000000005904, 25.844000000010254], 0.15: [1.4450000000000633, 14.938000000018473], 0.2: [-13.440000000000044, -16.726999999979597], 0.25: [-20.431999999992648, -15.856999999995166], 0.3: [-32.033999999973396, -35.18899999997122], 0.35: [-30.409999999977835, -36.757999999972974], 0.4: [-25.24799999998376, -37.96999999997074], 0.45: [-32.09999999997583, -26.849999999977143], 0.5: [-26.56199999997526, -30.726999999975483]}, 'goal_only': {0.0: [46.24700000000135, 95.1619999999954], 0.05: [75.61299999997323, 92.50799999999013], 0.1: [62.96099999998537, 92.63899999999624], 0.15: [45.817000000001755, 91.18499999999054], 0.2: [49.66199999999173, 92.50699999999227], 0.25: [25.193000000019634, 91.01199999998856], 0.3: [42.9039999999893, 80.4069999999889], 0.35: [29.879000000019303, 80.53099999998716], 0.4: [21.221000000011667, 78.42299999997904], 0.45: [44.065000000004495, 69.70099999998529], 0.5: [28.343000000018396, 72.68399999998843]}, 'both_same': {0.0: [35.32200000002367, 95.13299999999538], 0.05: [48.64299999998538, 35.830000000012376], 0.1: [14.840000000015566, 40.69000000001639], 0.15: [4.180999999993211, 15.139000000013262], 0.2: [-3.9700000000009963, -5.374000000005266], 0.25: [-6.674000000001981, -35.676999999971684], 0.3: [-27.005999999975828, -37.28099999997355], 0.35: [-31.894999999972114, -37.451999999972855], 0.4: [-27.572999999979796, -38.20699999996802], 0.45: [-35.51899999996971, -33.39299999997358], 0.5: [-33.37699999997492, -29.324999999976143]}}\n","Solved after 7772 episodes!\n","Solved after 8822 episodes!\n","Solved after 5457 episodes!\n","Solved after 8211 episodes!\n","7 noise_hrl: {'both': {0.0: [96.1429999999931, 98.13499999999645, 93.7829999999928, 43.371000000012764], 0.05: [41.2690000000128, 14.969000000011869, 16.188000000003534, 97.21999999999473], 0.1: [32.656000000012774, -13.789999999993347, 32.09900000001799, 73.49499999997947], 0.15: [9.064999999993674, -19.634999999983094, 7.805999999995798, 32.85100000001008], 0.2: [1.0399999999992025, -11.475999999998827, 14.737000000010987, 11.570000000009548], 0.25: [10.563000000001237, -13.063999999991879, -16.36499999999324, 5.692999999997991], 0.3: [6.919999999994555, -9.648999999998258, -14.166999999989637, -4.13100000000272], 0.35: [-12.05199999999988, -5.283000000003792, -15.354999999996217, -2.5960000000031145], 0.4: [-22.604999999976442, -11.820999999997827, -13.502999999996526, -7.626000000004815], 0.45: [-23.084999999985776, -18.585999999991653, -21.990999999988393, -14.480999999999213], 0.5: [-30.448999999971566, -18.719999999990783, -20.816999999976098, -10.14399999999253]}, 'action_only': {0.0: [95.71299999999309, 98.10599999999658, 95.26199999999324, 37.71800000001806], 0.05: [45.59700000000473, 14.79900000000871, 12.67200000000146, 97.08199999999457], 0.1: [27.26200000001424, -25.057999999976655, 22.236000000018684, 73.6269999999822], 0.15: [21.205000000012134, -28.10299999997768, 13.005000000003038, 21.320000000013874], 0.2: [3.9089999999933966, -14.19899999999675, -4.367000000002723, 17.24800000000297], 0.25: [-2.372000000001199, -13.050999999988969, -12.087000000003455, 4.192999999997368], 0.3: [-11.418999999998427, -1.6640000000012913, -10.876999999997102, -8.735000000001797], 0.35: [-20.84499999997825, -4.439000000002938, -21.595999999981736, -6.50700000000316], 0.4: [-26.629999999971503, -12.556999999992918, -16.447999999982166, -13.035000000000032], 0.45: [-33.95299999996912, -15.550999999994817, -18.254999999984303, 2.501000000001058], 0.5: [-31.98399999997388, -9.868000000005646, -20.011999999992884, -8.174000000008329]}, 'goal_only': {0.0: [96.15199999999308, 98.28499999999694, 95.30699999999737, 36.90200000001727], 0.05: [92.76999999999128, 98.30799999999635, 87.66099999999511, 96.15299999999375], 0.1: [95.00899999999201, 98.29899999999687, 71.66499999998472, 96.80699999999464], 0.15: [95.83599999999142, 98.40799999999666, 76.05899999998461, 97.48299999999506], 0.2: [97.01299999999526, 90.94199999999844, 77.02699999998795, 97.29299999999445], 0.25: [90.95999999999161, 82.07299999999212, 73.38599999998624, 93.3709999999952], 0.3: [92.56399999999427, 68.5939999999895, 70.50799999998975, 91.86599999999646], 0.35: [91.60499999999514, 68.64999999998932, 73.93199999997641, 82.83299999999204], 0.4: [89.95599999998824, 65.37299999999139, 74.64999999999227, 91.59099999998878], 0.45: [84.03199999998795, 46.100000000006176, 55.76899999999276, 79.86699999998373], 0.5: [84.17399999999448, 46.14699999999815, 56.5309999999838, 81.0779999999873]}, 'both_same': {0.0: [95.56899999999173, 96.68099999999669, 93.77699999999142, 47.458000000001945], 0.05: [45.79700000000359, 3.0119999999953735, 12.844000000012903, 94.18099999999133], 0.1: [20.786000000004275, -17.799999999996455, 33.101000000016626, 55.18999999999451], 0.15: [22.779000000010566, -18.380999999989058, 3.902999999997156, 35.871000000015854], 0.2: [14.389000000014635, -1.207000000003226, 1.997000000000214, 7.924999999994616], 0.25: [1.5169999999945125, -4.251000000001948, -5.3970000000040965, 4.044999999992927], 0.3: [-7.9980000000033655, -12.355999999984649, -2.2190000000001233, 6.360999999997257], 0.35: [-14.173999999985387, -16.470999999987818, -14.757999999997743, -5.02000000000602], 0.4: [-18.632999999987398, -22.025999999986553, -24.86299999997766, -11.641000000001457], 0.45: [-21.560999999987487, -14.444999999992708, -15.002999999988168, -6.317000000004302], 0.5: [-29.229999999970957, -21.994999999988707, -18.364999999979247, -3.786000000002772]}}\n","0, 1, 4, 5, 6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mit6CCnJob4o","executionInfo":{"status":"ok","timestamp":1617460894739,"user_tz":-60,"elapsed":1212,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointPushEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxjTe0z5csNW","executionInfo":{"status":"ok","timestamp":1617462377502,"user_tz":-60,"elapsed":1483962,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"9362041f-8c8f-4800-f08c-94a92cba1314"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","for i in [0, 1, 4, 5, 6]:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [7.115999999998565], 1.5: [32.22500000001827], 2.0: [93.18999999999572], 2.5: [97.19199999999768], 3.0: [86.80099999998802], 3.5: [97.22399999999678], 4.0: [97.71099999999596], 4.5: [98.12999999999651], 5.0: [98.15499999999656], 5.5: [97.92099999999614], 6.0: [97.72099999999584], 6.5: [97.5699999999955], 7.0: [97.3289999999953]}\n","1 scale: {1.0: [7.115999999998565, -45.77199999998934], 1.5: [32.22500000001827, 1.0900000000010994], 2.0: [93.18999999999572, 64.20299999998775], 2.5: [97.19199999999768, 95.58999999999287], 3.0: [86.80099999998802, 97.40399999999545], 3.5: [97.22399999999678, 97.7669999999958], 4.0: [97.71099999999596, 97.66299999999578], 4.5: [98.12999999999651, 96.47199999999388], 5.0: [98.15499999999656, 96.19699999999321], 5.5: [97.92099999999614, 96.77899999999428], 6.0: [97.72099999999584, 96.3309999999934], 6.5: [97.5699999999955, 96.12799999999297], 7.0: [97.3289999999953, 95.7909999999924]}\n","4 scale: {1.0: [7.115999999998565, -45.77199999998934, -6.856000000006239], 1.5: [32.22500000001827, 1.0900000000010994, 4.316999999993914], 2.0: [93.18999999999572, 64.20299999998775, 39.56600000001751], 2.5: [97.19199999999768, 95.58999999999287, 84.03699999998794], 3.0: [86.80099999998802, 97.40399999999545, 95.71799999999266], 3.5: [97.22399999999678, 97.7669999999958, 96.18799999999315], 4.0: [97.71099999999596, 97.66299999999578, 94.66999999999136], 4.5: [98.12999999999651, 96.47199999999388, 88.22299999998579], 5.0: [98.15499999999656, 96.19699999999321, 76.28799999998733], 5.5: [97.92099999999614, 96.77899999999428, 77.68399999998468], 6.0: [97.72099999999584, 96.3309999999934, 71.51699999998307], 6.5: [97.5699999999955, 96.12799999999297, 68.40999999997578], 7.0: [97.3289999999953, 95.7909999999924, 36.04100000002058]}\n","5 scale: {1.0: [7.115999999998565, -45.77199999998934, -6.856000000006239, 16.72600000000901], 1.5: [32.22500000001827, 1.0900000000010994, 4.316999999993914, 19.51600000001638], 2.0: [93.18999999999572, 64.20299999998775, 39.56600000001751, 76.5299999999867], 2.5: [97.19199999999768, 95.58999999999287, 84.03699999998794, 97.99999999999609], 3.0: [86.80099999998802, 97.40399999999545, 95.71799999999266, 97.12899999999574], 3.5: [97.22399999999678, 97.7669999999958, 96.18799999999315, 98.3649999999972], 4.0: [97.71099999999596, 97.66299999999578, 94.66999999999136, 98.21499999999651], 4.5: [98.12999999999651, 96.47199999999388, 88.22299999998579, 98.18999999999654], 5.0: [98.15499999999656, 96.19699999999321, 76.28799999998733, 97.7059999999956], 5.5: [97.92099999999614, 96.77899999999428, 77.68399999998468, 96.49499999999345], 6.0: [97.72099999999584, 96.3309999999934, 71.51699999998307, 75.69099999998292], 6.5: [97.5699999999955, 96.12799999999297, 68.40999999997578, -8.689000000002556], 7.0: [97.3289999999953, 95.7909999999924, 36.04100000002058, -47.333999999995065]}\n","6 scale: {1.0: [7.115999999998565, -45.77199999998934, -6.856000000006239, 16.72600000000901, -10.281999999994053], 1.5: [32.22500000001827, 1.0900000000010994, 4.316999999993914, 19.51600000001638, -18.989999999990033], 2.0: [93.18999999999572, 64.20299999998775, 39.56600000001751, 76.5299999999867, -2.852000000004307], 2.5: [97.19199999999768, 95.58999999999287, 84.03699999998794, 97.99999999999609, -48.51500000000027], 3.0: [86.80099999998802, 97.40399999999545, 95.71799999999266, 97.12899999999574, 66.67799999998596], 3.5: [97.22399999999678, 97.7669999999958, 96.18799999999315, 98.3649999999972, 97.99399999999639], 4.0: [97.71099999999596, 97.66299999999578, 94.66999999999136, 98.21499999999651, 89.37599999999084], 4.5: [98.12999999999651, 96.47199999999388, 88.22299999998579, 98.18999999999654, 43.20100000001757], 5.0: [98.15499999999656, 96.19699999999321, 76.28799999998733, 97.7059999999956, 0.2869999999995113], 5.5: [97.92099999999614, 96.77899999999428, 77.68399999998468, 96.49499999999345, -26.341999999979652], 6.0: [97.72099999999584, 96.3309999999934, 71.51699999998307, 75.69099999998292, -48.522000000004844], 6.5: [97.5699999999955, 96.12799999999297, 68.40999999997578, -8.689000000002556, -50.00000000000659], 7.0: [97.3289999999953, 95.7909999999924, 36.04100000002058, -47.333999999995065, -50.00000000000659]}\n","----\n","scale: {1.0: [7.115999999998565, -45.77199999998934, -6.856000000006239, 16.72600000000901, -10.281999999994053], 1.5: [32.22500000001827, 1.0900000000010994, 4.316999999993914, 19.51600000001638, -18.989999999990033], 2.0: [93.18999999999572, 64.20299999998775, 39.56600000001751, 76.5299999999867, -2.852000000004307], 2.5: [97.19199999999768, 95.58999999999287, 84.03699999998794, 97.99999999999609, -48.51500000000027], 3.0: [86.80099999998802, 97.40399999999545, 95.71799999999266, 97.12899999999574, 66.67799999998596], 3.5: [97.22399999999678, 97.7669999999958, 96.18799999999315, 98.3649999999972, 97.99399999999639], 4.0: [97.71099999999596, 97.66299999999578, 94.66999999999136, 98.21499999999651, 89.37599999999084], 4.5: [98.12999999999651, 96.47199999999388, 88.22299999998579, 98.18999999999654, 43.20100000001757], 5.0: [98.15499999999656, 96.19699999999321, 76.28799999998733, 97.7059999999956, 0.2869999999995113], 5.5: [97.92099999999614, 96.77899999999428, 77.68399999998468, 96.49499999999345, -26.341999999979652], 6.0: [97.72099999999584, 96.3309999999934, 71.51699999998307, 75.69099999998292, -48.522000000004844], 6.5: [97.5699999999955, 96.12799999999297, 68.40999999997578, -8.689000000002556, -50.00000000000659], 7.0: [97.3289999999953, 95.7909999999924, 36.04100000002058, -47.333999999995065, -50.00000000000659]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTjUVh0r8nx9","executionInfo":{"status":"ok","timestamp":1617460074668,"user_tz":-60,"elapsed":876,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZYgmaK7PhX1","executionInfo":{"status":"ok","timestamp":1617460893501,"user_tz":-60,"elapsed":818436,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"17ccc08c-2290-4dd0-daf1-4572e0bafcd3"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","for i in [0, 1, 4, 5, 6]:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [98.13799999999655], 0.05: [98.01999999999575], 0.1: [97.54299999999455], 0.15: [98.08199999999626], 0.2: [95.97099999999443], 0.25: [91.63899999999921], 0.3: [88.13699999999224], 0.35: [73.33599999999042], 0.4: [80.5439999999849]}\n","1 range: {0.0: [98.13799999999655, 97.73499999999599], 0.05: [98.01999999999575, 97.4419999999954], 0.1: [97.54299999999455, 97.52999999999537], 0.15: [98.08199999999626, 93.10999999999285], 0.2: [95.97099999999443, 93.16399999999008], 0.25: [91.63899999999921, 76.7709999999792], 0.3: [88.13699999999224, 81.31099999998831], 0.35: [73.33599999999042, 50.285999999997784], 0.4: [80.5439999999849, 42.949000000004396]}\n","4 range: {0.0: [98.13799999999655, 97.73499999999599, 96.12599999999301], 0.05: [98.01999999999575, 97.4419999999954, 83.46699999998113], 0.1: [97.54299999999455, 97.52999999999537, 59.323999999992296], 0.15: [98.08199999999626, 93.10999999999285, 57.4249999999942], 0.2: [95.97099999999443, 93.16399999999008, 27.87500000000848], 0.25: [91.63899999999921, 76.7709999999792, 31.58600000001814], 0.3: [88.13699999999224, 81.31099999998831, 31.034000000016], 0.35: [73.33599999999042, 50.285999999997784, 21.29700000000697], 0.4: [80.5439999999849, 42.949000000004396, 42.1870000000138]}\n","5 range: {0.0: [98.13799999999655, 97.73499999999599, 96.12599999999301, 98.13499999999652], 0.05: [98.01999999999575, 97.4419999999954, 83.46699999998113, 98.16099999999683], 0.1: [97.54299999999455, 97.52999999999537, 59.323999999992296, 94.94099999999248], 0.15: [98.08199999999626, 93.10999999999285, 57.4249999999942, 80.00099999999124], 0.2: [95.97099999999443, 93.16399999999008, 27.87500000000848, 75.5879999999997], 0.25: [91.63899999999921, 76.7709999999792, 31.58600000001814, 60.69899999999754], 0.3: [88.13699999999224, 81.31099999998831, 31.034000000016, 59.55899999998783], 0.35: [73.33599999999042, 50.285999999997784, 21.29700000000697, 38.765000000005706], 0.4: [80.5439999999849, 42.949000000004396, 42.1870000000138, 19.359000000006386]}\n","6 range: {0.0: [98.13799999999655, 97.73499999999599, 96.12599999999301, 98.13499999999652, 96.741999999995], 0.05: [98.01999999999575, 97.4419999999954, 83.46699999998113, 98.16099999999683, 67.14699999999189], 0.1: [97.54299999999455, 97.52999999999537, 59.323999999992296, 94.94099999999248, 50.693999999998475], 0.15: [98.08199999999626, 93.10999999999285, 57.4249999999942, 80.00099999999124, 39.944000000012366], 0.2: [95.97099999999443, 93.16399999999008, 27.87500000000848, 75.5879999999997, 28.556000000004623], 0.25: [91.63899999999921, 76.7709999999792, 31.58600000001814, 60.69899999999754, 34.43500000001151], 0.3: [88.13699999999224, 81.31099999998831, 31.034000000016, 59.55899999998783, 16.681000000012236], 0.35: [73.33599999999042, 50.285999999997784, 21.29700000000697, 38.765000000005706, 10.59399999999581], 0.4: [80.5439999999849, 42.949000000004396, 42.1870000000138, 19.359000000006386, -4.510000000003244]}\n","----\n","range: {0.0: [98.13799999999655, 97.73499999999599, 96.12599999999301, 98.13499999999652, 96.741999999995], 0.05: [98.01999999999575, 97.4419999999954, 83.46699999998113, 98.16099999999683, 67.14699999999189], 0.1: [97.54299999999455, 97.52999999999537, 59.323999999992296, 94.94099999999248, 50.693999999998475], 0.15: [98.08199999999626, 93.10999999999285, 57.4249999999942, 80.00099999999124, 39.944000000012366], 0.2: [95.97099999999443, 93.16399999999008, 27.87500000000848, 75.5879999999997, 28.556000000004623], 0.25: [91.63899999999921, 76.7709999999792, 31.58600000001814, 60.69899999999754, 34.43500000001151], 0.3: [88.13699999999224, 81.31099999998831, 31.034000000016, 59.55899999998783, 16.681000000012236], 0.35: [73.33599999999042, 50.285999999997784, 21.29700000000697, 38.765000000005706, 10.59399999999581], 0.4: [80.5439999999849, 42.949000000004396, 42.1870000000138, 19.359000000006386, -4.510000000003244]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6g9omuyeT51"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz-OF0Zl6zp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612798920385,"user_tz":-60,"elapsed":83452,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"4aea5aff-c52d-4252-c444-7dbcb3043205"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","episodes.pop(5 - 1)\n","episodes.pop(11 - 2)\n","\n","torch.save(episodes, \"PointPush_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVujCiQrP3hh"},"source":[""],"execution_count":null,"outputs":[]}]}