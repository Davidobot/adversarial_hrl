{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointpush.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617262709310,"user_tz":-60,"elapsed":23204,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"9f19810f-1395-4484-cf2e-cc5c9e535116"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_push.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617262710352,"user_tz":-60,"elapsed":1023,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push/{name}\" \n","\n","    torch.save({\n","      'meta_controller': {\n","          'critic': model.meta_controller.critic.state_dict(),\n","          'actor': model.meta_controller.actor.state_dict(),\n","      },\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_push/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617262713301,"user_tz":-60,"elapsed":3954,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617262713304,"user_tz":-60,"elapsed":3943,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617262713307,"user_tz":-60,"elapsed":3935,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_push import PointPushEnv \n","env = NormalizedEnv(PointPushEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617262713309,"user_tz":-60,"elapsed":3928,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617262713311,"user_tz":-60,"elapsed":3920,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617262713314,"user_tz":-60,"elapsed":3914,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617262713316,"user_tz":-60,"elapsed":3906,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617262713318,"user_tz":-60,"elapsed":3898,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 10000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # print losses\n","            #if self.total_it % (50 * 50 if self.is_meta else 500 * 50) == 0:\n","            #    print(f\"{self.is_meta} controller;\\n\\tcritic loss: {critic_loss.item()}\\n\\tactor loss: {actor_loss.item()}\")\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, 2 * self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617262713320,"user_tz":-60,"elapsed":3890,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        self.meta_controller = TD3(nb_states, len(self.goal_dim), True).to(device)\n","        self.max_goal_dist = torch.from_numpy(np.array([2.5, 2.5])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([0., 1.])).to(device)\n","        #self.meta_controller.depsilon = 1.0 / 10000\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 10000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.update_policy(self.off_policy_corrections)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    # correct goals to allow for use in experience replay\n","    def off_policy_corrections(self, sgoals, states, actions, candidate_goals=8):\n","        first_s = [s[0] for s in states] # First x\n","        last_s = [s[-1] for s in states] # Last x\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # diff = 1\n","        diff_goal = (np.array(last_s) - np.array(first_s))[:, np.newaxis, :self.goal_dimen]\n","\n","        # Shape: (batch_size, 1, subgoal_dim)\n","        # original = 1\n","        # random = candidate_goals\n","        scale = self.max_goal_dist.cpu().numpy()\n","        original_goal = np.array(sgoals)[:, np.newaxis, :]\n","        random_goals = np.random.normal(loc=diff_goal, scale=.5*scale,\n","                                        size=(BATCH_SIZE, candidate_goals, original_goal.shape[-1]))\n","        random_goals = random_goals.clip(-scale, scale)\n","\n","        # Shape: (batch_size, 10, subgoal_dim)\n","        candidates = np.concatenate([original_goal, diff_goal, random_goals], axis=1)\n","        #states = np.array(states)[:, :-1, :]\n","        actions = np.array(actions)\n","        seq_len = len(states[0])\n","\n","        # For ease\n","        new_batch_sz = seq_len * BATCH_SIZE\n","        action_dim = actions[0][0].shape\n","        obs_dim = states[0][0].shape\n","        ncands = candidates.shape[1]\n","\n","        true_actions = actions.reshape((new_batch_sz,) + action_dim)\n","        observations = states.reshape((new_batch_sz,) + obs_dim)\n","        goal_shape = (new_batch_sz, self.goal_dimen)\n","        # observations = get_obs_tensor(observations, sg_corrections=True)\n","\n","        # batched_candidates = np.tile(candidates, [seq_len, 1, 1])\n","        # batched_candidates = batched_candidates.transpose(1, 0, 2)\n","\n","        policy_actions = np.zeros((ncands, new_batch_sz) + action_dim)\n","\n","        observations = torch.from_numpy(observations).to(device)\n","        for c in range(ncands):\n","            subgoal = candidates[:,c]\n","            candidate = (subgoal + states[:, 0, :self.goal_dimen])[:, None] - states[:, :, :self.goal_dimen]\n","            candidate = candidate.reshape(*goal_shape)\n","            policy_actions[c] = self.controller.actor(torch.cat([observations, torch.from_numpy(candidate).to(device)], 1).float()).detach().cpu().numpy()\n","\n","        difference = (policy_actions - true_actions)\n","        difference = np.where(difference != -np.inf, difference, 0)\n","        difference = difference.reshape((ncands, BATCH_SIZE, seq_len) + action_dim).transpose(1, 0, 2, 3)\n","\n","        logprob = -0.5*np.sum(np.linalg.norm(difference, axis=-1)**2, axis=-1)\n","        max_indices = np.argmax(logprob, axis=-1)\n","\n","        return torch.from_numpy(candidates[np.arange(BATCH_SIZE), max_indices]).to(device).float()\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done, state_seq, action_seq)\n","\n","    def select_goal(self, s_t, warmup, decay_epsilon):\n","        return self.meta_controller.select_action(s_t, warmup, decay_epsilon) * self.max_goal_dist + self.goal_offset\n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 11\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal = agent.select_goal(state, i_episode <= warmup, True)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_push_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1617262721321,"user_tz":-60,"elapsed":6266,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    #g_state = torch.tensor(g_state, requires_grad=True)\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    # initial forward pass\n","    goal = agent.meta_controller.actor(g_state)\n","    goal = torch.clamp(goal, -1., 1.)\n","\n","    loss = F.mse_loss(goal, target)\n","\n","    if targeted:\n","        # initial forward pass\n","        goal = agent.meta_controller.actor(g_state)\n","        goal = torch.clamp(goal, -1., 1.)\n","\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = agent.meta_controller.critic.Q1(g_state, agent.meta_controller.actor(g_state)).mean()\n","\n","    agent.meta_controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal = agent.select_goal(og_state, False, False)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal = agent.select_goal(state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od4IvIuPuNlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617287979112,"user_tz":-60,"elapsed":25210909,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"d10171eb-3e41-4eda-97a1-9c40bee780c6"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","\"\"\"\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")\n","\"\"\"\n","\n","print(\"start\")\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["start\n","0 fgsm (ut): {'goal': {0.0: [95.39999999999215], 0.02: [-11.078000000003392], 0.04: [-2.73000000000287], 0.06: [12.871000000017554], 0.08: [-14.700999999999826], 0.1: [-30.389999999976354], 0.12: [-28.899999999975577], 0.14: [-29.323999999974134], 0.16: [-45.611999999987894], 0.18: [-45.80399999998859], 0.2: [-44.564999999994235]}, 'action': {0.0: [95.48599999999173], 0.02: [-8.00900000000679], 0.04: [56.44799999998324], 0.06: [17.94300000000507], 0.08: [49.61500000000261], 0.1: [21.900000000014888], 0.12: [-36.290999999973046], 0.14: [-15.446999999991215], 0.16: [-34.925999999970955], 0.18: [-29.57099999997383], 0.2: [-32.2359999999757]}}\n","0 fgsm (t): {'goal': {0.0: [95.75099999999209], 0.02: [24.244000000014683], 0.04: [-50.00000000000659], 0.06: [-50.00000000000659], 0.08: [-47.13399999999907], 0.1: [-33.19799999997098], 0.12: [-50.00000000000659], 0.14: [-40.10799999997379], 0.16: [-50.00000000000659], 0.18: [-48.70000000000095], 0.2: [-50.00000000000659]}, 'action': {0.0: [95.94199999999272], 0.02: [40.92300000001343], 0.04: [-30.467999999974932], 0.06: [-27.434999999977137], 0.08: [-8.421000000002646], 0.1: [-48.60700000000061], 0.12: [-41.833999999972555], 0.14: [-27.479999999979068], 0.16: [-36.21299999997097], 0.18: [-25.05999999998595], 0.2: [-39.362999999975756]}}\n","1 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632], 0.02: [-11.078000000003392, -14.685999999990273], 0.04: [-2.73000000000287, 0.22299999999739767], 0.06: [12.871000000017554, -37.28999999997227], 0.08: [-14.700999999999826, 41.519000000014394], 0.1: [-30.389999999976354, -50.00000000000659], 0.12: [-28.899999999975577, -50.00000000000659], 0.14: [-29.323999999974134, -50.00000000000659], 0.16: [-45.611999999987894, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659], 0.2: [-44.564999999994235, -50.00000000000659]}, 'action': {0.0: [95.48599999999173, 49.14599999998998], 0.02: [-8.00900000000679, -47.48899999999803], 0.04: [56.44799999998324, -48.69600000000093], 0.06: [17.94300000000507, -46.119999999989744], 0.08: [49.61500000000261, -47.42199999999993], 0.1: [21.900000000014888, -50.00000000000659], 0.12: [-36.290999999973046, -50.00000000000659], 0.14: [-15.446999999991215, -50.00000000000659], 0.16: [-34.925999999970955, -35.07399999997133], 0.18: [-29.57099999997383, -50.00000000000659], 0.2: [-32.2359999999757, -50.00000000000659]}}\n","1 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654], 0.02: [24.244000000014683, -39.39899999996785], 0.04: [-50.00000000000659, 71.34899999998744], 0.06: [-50.00000000000659, -29.674999999973224], 0.08: [-47.13399999999907, -50.00000000000659], 0.1: [-33.19799999997098, -50.00000000000659], 0.12: [-50.00000000000659, -50.00000000000659], 0.14: [-40.10799999997379, -50.00000000000659], 0.16: [-50.00000000000659, -50.00000000000659], 0.18: [-48.70000000000095, -50.00000000000659], 0.2: [-50.00000000000659, -50.00000000000659]}, 'action': {0.0: [95.94199999999272, 46.70799999999976], 0.02: [40.92300000001343, 37.46300000001904], 0.04: [-30.467999999974932, 10.896000000010563], 0.06: [-27.434999999977137, 12.977999999997724], 0.08: [-8.421000000002646, 45.44300000000417], 0.1: [-48.60700000000061, -18.034999999978375], 0.12: [-41.833999999972555, -5.1590000000053635], 0.14: [-27.479999999979068, -50.00000000000659], 0.16: [-36.21299999997097, -50.00000000000659], 0.18: [-25.05999999998595, -50.00000000000659], 0.2: [-39.362999999975756, -50.00000000000659]}}\n","2 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792]}}\n","2 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064]}}\n","3 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647, 85.2119999999874], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403, 43.55800000000431], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274, 46.48400000000469], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659, 52.73600000000145], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273, 38.842000000020555], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083, 14.28200000001464], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847, 27.206000000016633], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659, 29.893000000016244], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659, 8.093999999994207], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659, -10.887000000001933], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659, -27.201999999981567]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711, 90.57399999998941], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698, 33.85500000001609], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317, -39.804999999968615], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067, -48.82100000000138], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042, -46.23899999999017], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748, -48.776000000001225], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926, -48.6320000000007], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675, -41.048999999967656], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055, -47.35199999999513], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072, -45.92299999999446], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792, -48.60200000000059]}}\n","3 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538, 90.73699999998672], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072, 29.751000000024447], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903, -4.347000000002691], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634, 11.310999999996794], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358, 12.146000000009927], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685, -4.106000000006259], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652, -22.757999999982903], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017, -26.57599999997587], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824, -27.46399999997474], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064, -39.24999999997139], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736, -33.34099999997059]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792, 86.36899999998631], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642, 91.49699999998504], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939, 76.19199999997386], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011, 41.22800000001921], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287, 50.3720000000051], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348, 41.30200000001525], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034, 31.096000000014815], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659, 40.43800000000967], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982, 15.51900000001616], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659, 13.91900000002509], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064, 3.559999999996969]}}\n","4 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647, 85.2119999999874, 96.00599999999245], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403, 43.55800000000431, -48.555000000001556], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274, 46.48400000000469, -38.60599999996974], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659, 52.73600000000145, -28.801999999973567], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273, 38.842000000020555, -31.39199999997399], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083, 14.28200000001464, -13.649999999987426], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847, 27.206000000016633, -27.261999999979285], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659, 29.893000000016244, -41.862999999974626], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659, 8.093999999994207, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659, -10.887000000001933, -42.297999999983915], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659, -27.201999999981567, -42.64599999997437]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711, 90.57399999998941, 94.12899999999007], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698, 33.85500000001609, -12.815999999997665], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317, -39.804999999968615, 10.41499999999501], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067, -48.82100000000138, -25.588999999983137], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042, -46.23899999999017, -40.07699999997017], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748, -48.776000000001225, -42.713999999976664], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926, -48.6320000000007, -48.69100000000205], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675, -41.048999999967656, -48.969000000003064], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055, -47.35199999999513, -50.00000000000659], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072, -45.92299999999446, -44.538999999989684], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792, -48.60200000000059, -48.72300000000103]}}\n","4 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538, 90.73699999998672, 91.53299999999224], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072, 29.751000000024447, 14.381000000009474], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903, -4.347000000002691, -23.112999999983316], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634, 11.310999999996794, -24.307999999978048], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358, 12.146000000009927, -16.32799999999781], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685, -4.106000000006259, 3.5219999999965683], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652, -22.757999999982903, -19.28399999998474], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017, -26.57599999997587, -26.263999999977692], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824, -27.46399999997474, -36.033999999972764], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064, -39.24999999997139, -36.67999999997245], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736, -33.34099999997059, -34.288999999971445]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792, 86.36899999998631, 87.2109999999879], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642, 91.49699999998504, 42.359000000002304], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939, 76.19199999997386, 66.24499999998358], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011, 41.22800000001921, 37.82300000001741], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287, 50.3720000000051, 68.76899999998277], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348, 41.30200000001525, 66.10499999998201], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034, 31.096000000014815, 58.85599999998629], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659, 40.43800000000967, 41.86800000001322], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982, 15.51900000001616, 32.72100000001136], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659, 13.91900000002509, 46.47300000000484], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064, 3.559999999996969, 40.922000000014116]}}\n","5 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647, 85.2119999999874, 96.00599999999245, 86.70499999998108], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403, 43.55800000000431, -48.555000000001556, -50.00000000000659], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274, 46.48400000000469, -38.60599999996974, -50.00000000000659], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659, 52.73600000000145, -28.801999999973567, -47.31299999999499], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273, 38.842000000020555, -31.39199999997399, -50.00000000000659], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083, 14.28200000001464, -13.649999999987426, -48.55100000000041], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847, 27.206000000016633, -27.261999999979285, -50.00000000000659], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659, 29.893000000016244, -41.862999999974626, -50.00000000000659], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659, 8.093999999994207, -50.00000000000659, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659, -10.887000000001933, -42.297999999983915, -44.6509999999902], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659, -27.201999999981567, -42.64599999997437, -21.24199999999246]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711, 90.57399999998941, 94.12899999999007, 82.04499999998299], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698, 33.85500000001609, -12.815999999997665, -24.829999999980508], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317, -39.804999999968615, 10.41499999999501, 12.364999999994257], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067, -48.82100000000138, -25.588999999983137, -23.266999999980253], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042, -46.23899999999017, -40.07699999997017, -3.1480000000064603], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748, -48.776000000001225, -42.713999999976664, -28.79099999997915], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926, -48.6320000000007, -48.69100000000205, -39.54699999996885], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675, -41.048999999967656, -48.969000000003064, -48.562000000000445], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055, -47.35199999999513, -50.00000000000659, -47.26399999999481], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072, -45.92299999999446, -44.538999999989684, -50.00000000000659], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792, -48.60200000000059, -48.72300000000103, -50.00000000000659]}}\n","5 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538, 90.73699999998672, 91.53299999999224, 87.21099999998374], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072, 29.751000000024447, 14.381000000009474, 43.765000000006594], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903, -4.347000000002691, -23.112999999983316, 12.580000000001933], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634, 11.310999999996794, -24.307999999978048, -50.00000000000659], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358, 12.146000000009927, -16.32799999999781, -45.7969999999897], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685, -4.106000000006259, 3.5219999999965683, -17.477999999980963], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652, -22.757999999982903, -19.28399999998474, -15.773999999981077], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017, -26.57599999997587, -26.263999999977692, -0.0430000000036577], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824, -27.46399999997474, -36.033999999972764, 8.140000000003537], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064, -39.24999999997139, -36.67999999997245, 39.342000000011915], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736, -33.34099999997059, -34.288999999971445, 13.819000000003873]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792, 86.36899999998631, 87.2109999999879, 86.03199999998422], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642, 91.49699999998504, 42.359000000002304, 78.88999999997822], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939, 76.19199999997386, 66.24499999998358, 77.31699999998487], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011, 41.22800000001921, 37.82300000001741, 45.54400000000106], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287, 50.3720000000051, 68.76899999998277, 66.44699999998608], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348, 41.30200000001525, 66.10499999998201, 69.5269999999817], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034, 31.096000000014815, 58.85599999998629, -42.95699999997642], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659, 40.43800000000967, 41.86800000001322, -50.00000000000659], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982, 15.51900000001616, 32.72100000001136, -50.00000000000659], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659, 13.91900000002509, 46.47300000000484, -50.00000000000659], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064, 3.559999999996969, 40.922000000014116, -50.00000000000659]}}\n","6 fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647, 85.2119999999874, 96.00599999999245, 86.70499999998108, 91.71199999999139], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403, 43.55800000000431, -48.555000000001556, -50.00000000000659, 6.851999999993934], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274, 46.48400000000469, -38.60599999996974, -50.00000000000659, -7.99100000000307], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659, 52.73600000000145, -28.801999999973567, -47.31299999999499, -26.581999999982095], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273, 38.842000000020555, -31.39199999997399, -50.00000000000659, -39.03399999997135], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083, 14.28200000001464, -13.649999999987426, -48.55100000000041, -40.0909999999704], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847, 27.206000000016633, -27.261999999979285, -50.00000000000659, -50.00000000000659], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659, 29.893000000016244, -41.862999999974626, -50.00000000000659, -50.00000000000659], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659, 8.093999999994207, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659, -10.887000000001933, -42.297999999983915, -44.6509999999902, -50.00000000000659], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659, -27.201999999981567, -42.64599999997437, -21.24199999999246, -50.00000000000659]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711, 90.57399999998941, 94.12899999999007, 82.04499999998299, 97.60899999999542], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698, 33.85500000001609, -12.815999999997665, -24.829999999980508, -3.157000000002503], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317, -39.804999999968615, 10.41499999999501, 12.364999999994257, 6.5809999999974025], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067, -48.82100000000138, -25.588999999983137, -23.266999999980253, 2.9190000000001404], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042, -46.23899999999017, -40.07699999997017, -3.1480000000064603, -25.084999999978375], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748, -48.776000000001225, -42.713999999976664, -28.79099999997915, -31.138999999970807], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926, -48.6320000000007, -48.69100000000205, -39.54699999996885, -38.00299999997179], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675, -41.048999999967656, -48.969000000003064, -48.562000000000445, -41.72499999997789], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055, -47.35199999999513, -50.00000000000659, -47.26399999999481, -46.115999999989725], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072, -45.92299999999446, -44.538999999989684, -50.00000000000659, -50.00000000000659], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792, -48.60200000000059, -48.72300000000103, -50.00000000000659, -48.713000000000996]}}\n","6 fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538, 90.73699999998672, 91.53299999999224, 87.21099999998374, 96.00299999999497], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072, 29.751000000024447, 14.381000000009474, 43.765000000006594, -13.535999999998067], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903, -4.347000000002691, -23.112999999983316, 12.580000000001933, -17.09899999998546], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634, 11.310999999996794, -24.307999999978048, -50.00000000000659, -5.7870000000029265], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358, 12.146000000009927, -16.32799999999781, -45.7969999999897, -42.87199999998732], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685, -4.106000000006259, 3.5219999999965683, -17.477999999980963, -27.22299999998245], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652, -22.757999999982903, -19.28399999998474, -15.773999999981077, -41.645999999972474], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017, -26.57599999997587, -26.263999999977692, -0.0430000000036577, -38.813999999970655], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824, -27.46399999997474, -36.033999999972764, 8.140000000003537, -37.33699999996995], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064, -39.24999999997139, -36.67999999997245, 39.342000000011915, -37.03399999997112], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736, -33.34099999997059, -34.288999999971445, 13.819000000003873, -47.186000000003624]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792, 86.36899999998631, 87.2109999999879, 86.03199999998422, 96.23399999999401], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642, 91.49699999998504, 42.359000000002304, 78.88999999997822, 43.67300000000163], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939, 76.19199999997386, 66.24499999998358, 77.31699999998487, 31.12700000001571], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011, 41.22800000001921, 37.82300000001741, 45.54400000000106, 11.292000000005379], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287, 50.3720000000051, 68.76899999998277, 66.44699999998608, -26.34899999997258], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348, 41.30200000001525, 66.10499999998201, 69.5269999999817, -16.478999999992116], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034, 31.096000000014815, 58.85599999998629, -42.95699999997642, -15.446999999988742], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659, 40.43800000000967, 41.86800000001322, -50.00000000000659, -19.63799999998932], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982, 15.51900000001616, 32.72100000001136, -50.00000000000659, -24.18899999998184], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659, 13.91900000002509, 46.47300000000484, -50.00000000000659, -21.455999999983643], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064, 3.559999999996969, 40.922000000014116, -50.00000000000659, -10.705999999999273]}}\n","----\n","fgsm (ut): {'goal': {0.0: [95.39999999999215, 49.75699999999632, 88.23099999998647, 85.2119999999874, 96.00599999999245, 86.70499999998108, 91.71199999999139], 0.02: [-11.078000000003392, -14.685999999990273, 84.75399999998403, 43.55800000000431, -48.555000000001556, -50.00000000000659, 6.851999999993934], 0.04: [-2.73000000000287, 0.22299999999739767, 35.44800000001274, 46.48400000000469, -38.60599999996974, -50.00000000000659, -7.99100000000307], 0.06: [12.871000000017554, -37.28999999997227, -50.00000000000659, 52.73600000000145, -28.801999999973567, -47.31299999999499, -26.581999999982095], 0.08: [-14.700999999999826, 41.519000000014394, -45.69199999999273, 38.842000000020555, -31.39199999997399, -50.00000000000659, -39.03399999997135], 0.1: [-30.389999999976354, -50.00000000000659, -10.030000000007083, 14.28200000001464, -13.649999999987426, -48.55100000000041, -40.0909999999704], 0.12: [-28.899999999975577, -50.00000000000659, -42.80499999997847, 27.206000000016633, -27.261999999979285, -50.00000000000659, -50.00000000000659], 0.14: [-29.323999999974134, -50.00000000000659, -50.00000000000659, 29.893000000016244, -41.862999999974626, -50.00000000000659, -50.00000000000659], 0.16: [-45.611999999987894, -50.00000000000659, -50.00000000000659, 8.093999999994207, -50.00000000000659, -50.00000000000659, -50.00000000000659], 0.18: [-45.80399999998859, -50.00000000000659, -50.00000000000659, -10.887000000001933, -42.297999999983915, -44.6509999999902, -50.00000000000659], 0.2: [-44.564999999994235, -50.00000000000659, -50.00000000000659, -27.201999999981567, -42.64599999997437, -21.24199999999246, -50.00000000000659]}, 'action': {0.0: [95.48599999999173, 49.14599999998998, 91.79199999998711, 90.57399999998941, 94.12899999999007, 82.04499999998299, 97.60899999999542], 0.02: [-8.00900000000679, -47.48899999999803, 98.35599999999698, 33.85500000001609, -12.815999999997665, -24.829999999980508, -3.157000000002503], 0.04: [56.44799999998324, -48.69600000000093, 92.98999999999317, -39.804999999968615, 10.41499999999501, 12.364999999994257, 6.5809999999974025], 0.06: [17.94300000000507, -46.119999999989744, 64.64599999996067, -48.82100000000138, -25.588999999983137, -23.266999999980253, 2.9190000000001404], 0.08: [49.61500000000261, -47.42199999999993, 0.2379999999988042, -46.23899999999017, -40.07699999997017, -3.1480000000064603, -25.084999999978375], 0.1: [21.900000000014888, -50.00000000000659, -8.011000000005748, -48.776000000001225, -42.713999999976664, -28.79099999997915, -31.138999999970807], 0.12: [-36.290999999973046, -50.00000000000659, 32.866000000013926, -48.6320000000007, -48.69100000000205, -39.54699999996885, -38.00299999997179], 0.14: [-15.446999999991215, -50.00000000000659, -15.981999999983675, -41.048999999967656, -48.969000000003064, -48.562000000000445, -41.72499999997789], 0.16: [-34.925999999970955, -35.07399999997133, -21.171999999978055, -47.35199999999513, -50.00000000000659, -47.26399999999481, -46.115999999989725], 0.18: [-29.57099999997383, -50.00000000000659, -37.42499999997072, -45.92299999999446, -44.538999999989684, -50.00000000000659, -50.00000000000659], 0.2: [-32.2359999999757, -50.00000000000659, -42.89099999997792, -48.60200000000059, -48.72300000000103, -50.00000000000659, -48.713000000000996]}}\n","fgsm (t): {'goal': {0.0: [95.75099999999209, 47.737999999993654, 90.04299999998538, 90.73699999998672, 91.53299999999224, 87.21099999998374, 96.00299999999497], 0.02: [24.244000000014683, -39.39899999996785, 90.58299999999072, 29.751000000024447, 14.381000000009474, 43.765000000006594, -13.535999999998067], 0.04: [-50.00000000000659, 71.34899999998744, 71.12099999997903, -4.347000000002691, -23.112999999983316, 12.580000000001933, -17.09899999998546], 0.06: [-50.00000000000659, -29.674999999973224, 76.46399999998634, 11.310999999996794, -24.307999999978048, -50.00000000000659, -5.7870000000029265], 0.08: [-47.13399999999907, -50.00000000000659, 40.69900000001358, 12.146000000009927, -16.32799999999781, -45.7969999999897, -42.87199999998732], 0.1: [-33.19799999997098, -50.00000000000659, 7.327999999996685, -4.106000000006259, 3.5219999999965683, -17.477999999980963, -27.22299999998245], 0.12: [-50.00000000000659, -50.00000000000659, -12.701000000000652, -22.757999999982903, -19.28399999998474, -15.773999999981077, -41.645999999972474], 0.14: [-40.10799999997379, -50.00000000000659, -15.627999999992017, -26.57599999997587, -26.263999999977692, -0.0430000000036577, -38.813999999970655], 0.16: [-50.00000000000659, -50.00000000000659, -11.439000000002824, -27.46399999997474, -36.033999999972764, 8.140000000003537, -37.33699999996995], 0.18: [-48.70000000000095, -50.00000000000659, -21.122999999990064, -39.24999999997139, -36.67999999997245, 39.342000000011915, -37.03399999997112], 0.2: [-50.00000000000659, -50.00000000000659, -25.238999999979736, -33.34099999997059, -34.288999999971445, 13.819000000003873, -47.186000000003624]}, 'action': {0.0: [95.94199999999272, 46.70799999999976, 90.08999999998792, 86.36899999998631, 87.2109999999879, 86.03199999998422, 96.23399999999401], 0.02: [40.92300000001343, 37.46300000001904, 92.59599999999642, 91.49699999998504, 42.359000000002304, 78.88999999997822, 43.67300000000163], 0.04: [-30.467999999974932, 10.896000000010563, 81.35799999998939, 76.19199999997386, 66.24499999998358, 77.31699999998487, 31.12700000001571], 0.06: [-27.434999999977137, 12.977999999997724, 51.19199999999011, 41.22800000001921, 37.82300000001741, 45.54400000000106, 11.292000000005379], 0.08: [-8.421000000002646, 45.44300000000417, 49.66600000000287, 50.3720000000051, 68.76899999998277, 66.44699999998608, -26.34899999997258], 0.1: [-48.60700000000061, -18.034999999978375, 20.342000000010348, 41.30200000001525, 66.10499999998201, 69.5269999999817, -16.478999999992116], 0.12: [-41.833999999972555, -5.1590000000053635, -48.53200000000034, 31.096000000014815, 58.85599999998629, -42.95699999997642, -15.446999999988742], 0.14: [-27.479999999979068, -50.00000000000659, -50.00000000000659, 40.43800000000967, 41.86800000001322, -50.00000000000659, -19.63799999998932], 0.16: [-36.21299999997097, -50.00000000000659, -47.07799999999982, 15.51900000001616, 32.72100000001136, -50.00000000000659, -24.18899999998184], 0.18: [-25.05999999998595, -50.00000000000659, -50.00000000000659, 13.91900000002509, 46.47300000000484, -50.00000000000659, -21.455999999983643], 0.2: [-39.362999999975756, -50.00000000000659, -40.33199999997064, 3.559999999996969, 40.922000000014116, -50.00000000000659, -10.705999999999273]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mit6CCnJob4o"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointPushEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxjTe0z5csNW","executionInfo":{"status":"ok","timestamp":1611480130893,"user_tz":-60,"elapsed":3448069,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"4ddc6db6-8dfb-44c3-faf3-3399d7f6e7af"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 scale: {1.0: [-15.173999999984815], 1.5: [-14.111999999984583], 2.0: [-0.7060000000002684], 2.5: [1.0359999999962617], 3.0: [72.12399999998179], 3.5: [94.7109999999904], 4.0: [95.5639999999921], 4.5: [96.20799999999312], 5.0: [96.6849999999942], 5.5: [95.9619999999935], 6.0: [93.12199999998768], 6.5: [80.0129999999775], 7.0: [61.16699999999068]}\n","1 scale: {1.0: [-15.173999999984815, -50.00000000000659], 1.5: [-14.111999999984583, -50.00000000000659], 2.0: [-0.7060000000002684, -33.766999999977756], 2.5: [1.0359999999962617, -14.570999999991823], 3.0: [72.12399999998179, 33.217000000021834], 3.5: [94.7109999999904, 63.02499999998151], 4.0: [95.5639999999921, 48.65999999999338], 4.5: [96.20799999999312, 53.878999999995386], 5.0: [96.6849999999942, 16.916000000001176], 5.5: [95.9619999999935, -17.411999999989774], 6.0: [93.12199999998768, -42.84899999998142], 6.5: [80.0129999999775, -50.00000000000659], 7.0: [61.16699999999068, -48.751000000001135]}\n","2 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789]}\n","3 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935]}\n","4 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677]}\n","5 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795]}\n","6 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317]}\n","7 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774]}\n","8 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738]}\n","9 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147, -45.77299999998848], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024, 42.54500000000593], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431, 73.29299999998776], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874, 81.91399999998738], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953, 94.2909999999901], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487, 95.79799999999264], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994, 96.65399999999417], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217, 96.61799999999384], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978, 95.0879999999927], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155, 92.06499999998564], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679, 93.36099999999027], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705, 93.89099999999259], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738, 82.04799999998407]}\n","10 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147, -45.77299999998848, 4.419999999994251], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024, 42.54500000000593, 18.066000000003577], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431, 73.29299999998776, 69.77299999997898], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874, 81.91399999998738, 78.89699999997987], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953, 94.2909999999901, 71.75899999998407], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487, 95.79799999999264, 66.11699999999159], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994, 96.65399999999417, 89.8709999999865], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217, 96.61799999999384, 87.68299999998915], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978, 95.0879999999927, 65.68199999998532], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155, 92.06499999998564, 45.744999999999656], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679, 93.36099999999027, 17.025000000002834], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705, 93.89099999999259, -27.967999999978087], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738, 82.04799999998407, -27.394999999974598]}\n","11 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147, -45.77299999998848, 4.419999999994251, 25.083000000005583], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024, 42.54500000000593, 18.066000000003577, 55.92299999999223], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431, 73.29299999998776, 69.77299999997898, 97.21199999999497], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874, 81.91399999998738, 78.89699999997987, 59.82100000000076], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953, 94.2909999999901, 71.75899999998407, 96.80399999999418], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487, 95.79799999999264, 66.11699999999159, 92.68799999999446], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994, 96.65399999999417, 89.8709999999865, 88.2219999999887], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217, 96.61799999999384, 87.68299999998915, 95.41999999999285], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978, 95.0879999999927, 65.68199999998532, 93.9429999999943], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155, 92.06499999998564, 45.744999999999656, 97.1869999999949], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679, 93.36099999999027, 17.025000000002834, 97.67799999999578], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705, 93.89099999999259, -27.967999999978087, 74.00799999998239], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738, 82.04799999998407, -27.394999999974598, 47.27000000000732]}\n","12 scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147, -45.77299999998848, 4.419999999994251, 25.083000000005583, 24.325000000014125], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024, 42.54500000000593, 18.066000000003577, 55.92299999999223, 1.1430000000009009], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431, 73.29299999998776, 69.77299999997898, 97.21199999999497, 95.00599999999278], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874, 81.91399999998738, 78.89699999997987, 59.82100000000076, 97.92499999999617], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953, 94.2909999999901, 71.75899999998407, 96.80399999999418, 97.79499999999597], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487, 95.79799999999264, 66.11699999999159, 92.68799999999446, 97.82099999999589], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994, 96.65399999999417, 89.8709999999865, 88.2219999999887, 90.46899999998737], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217, 96.61799999999384, 87.68299999998915, 95.41999999999285, 55.126999999996926], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978, 95.0879999999927, 65.68199999998532, 93.9429999999943, 11.932000000006799], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155, 92.06499999998564, 45.744999999999656, 97.1869999999949, 22.343000000016982], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679, 93.36099999999027, 17.025000000002834, 97.67799999999578, -9.013999999997445], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705, 93.89099999999259, -27.967999999978087, 74.00799999998239, -21.897999999981945], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738, 82.04799999998407, -27.394999999974598, 47.27000000000732, -41.33899999997588]}\n","----\n","scale: {1.0: [-15.173999999984815, -50.00000000000659, -16.431999999994243, 27.784000000016963, -43.90199999998099, 15.54800000000497, -43.3309999999799, -13.499999999989875, -32.16199999997147, -45.77299999998848, 4.419999999994251, 25.083000000005583, 24.325000000014125], 1.5: [-14.111999999984583, -50.00000000000659, 20.894000000005043, 40.64800000001717, -50.00000000000659, 0.6509999999995578, -20.64699999997912, -12.7180000000014, -21.63999999998024, 42.54500000000593, 18.066000000003577, 55.92299999999223, 1.1430000000009009], 2.0: [-0.7060000000002684, -33.766999999977756, 61.154999999982856, 68.5799999999894, -50.00000000000659, 53.388999999995455, -10.151999999989412, 30.79000000001582, 37.94500000001431, 73.29299999998776, 69.77299999997898, 97.21199999999497, 95.00599999999278], 2.5: [1.0359999999962617, -14.570999999991823, 80.45499999998037, 91.53799999999077, 59.61299999998501, 68.25799999997709, -25.447999999984855, 39.337000000015365, 89.3209999999874, 81.91399999998738, 78.89699999997987, 59.82100000000076, 97.92499999999617], 3.0: [72.12399999998179, 33.217000000021834, 92.4909999999856, 89.30599999998677, 96.45199999999281, 84.11499999998632, 77.63799999999218, 46.80599999999836, 93.2229999999953, 94.2909999999901, 71.75899999998407, 96.80399999999418, 97.79499999999597], 3.5: [94.7109999999904, 63.02499999998151, 88.83899999998434, 89.01699999999067, 91.4539999999918, 87.90299999998541, 97.6839999999957, 87.74699999998997, 96.80099999999487, 95.79799999999264, 66.11699999999159, 92.68799999999446, 97.82099999999589], 4.0: [95.5639999999921, 48.65999999999338, 89.18999999998589, 88.27899999998748, 93.07499999999143, 87.7229999999785, 97.65499999999554, 94.81299999999061, 97.048999999994, 96.65399999999417, 89.8709999999865, 88.2219999999887, 90.46899999998737], 4.5: [96.20799999999312, 53.878999999995386, 80.07599999997838, 75.6699999999797, 92.50699999999186, 68.63299999998195, 77.62899999998831, 93.16199999998891, 95.50899999999217, 96.61799999999384, 87.68299999998915, 95.41999999999285, 55.126999999996926], 5.0: [96.6849999999942, 16.916000000001176, 73.42999999998092, 32.903000000020825, 83.08299999997703, 70.06699999998462, 75.49299999997822, 94.33899999999063, 93.47599999998978, 95.0879999999927, 65.68199999998532, 93.9429999999943, 11.932000000006799], 5.5: [95.9619999999935, -17.411999999989774, 32.46400000001562, 22.843000000020865, 74.6219999999791, 79.35299999997686, 28.41500000001461, 94.45199999999092, 95.36499999999155, 92.06499999998564, 45.744999999999656, 97.1869999999949, 22.343000000016982], 6.0: [93.12199999998768, -42.84899999998142, 43.70000000001502, 33.59000000001492, 44.25300000000534, 89.76499999998305, 8.971999999995127, 95.87499999999261, 92.54199999998679, 93.36099999999027, 17.025000000002834, 97.67799999999578, -9.013999999997445], 6.5: [80.0129999999775, -50.00000000000659, 76.63299999998294, 13.060000000002477, 49.97100000000579, 84.99599999998944, -31.46599999997311, 89.54499999999027, 79.06799999998705, 93.89099999999259, -27.967999999978087, 74.00799999998239, -21.897999999981945], 7.0: [61.16699999999068, -48.751000000001135, 84.95299999997789, 31.312000000019935, 41.45700000000677, 87.78599999998795, -27.183999999975317, 86.41199999998774, 57.36199999998738, 82.04799999998407, -27.394999999974598, 47.27000000000732, -41.33899999997588]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6g9omuyeT51"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz-OF0Zl6zp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612798920385,"user_tz":-60,"elapsed":83452,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"4aea5aff-c52d-4252-c444-7dbcb3043205"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        save_trajectories(agent, episodes, True)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","episodes.pop(5 - 1)\n","episodes.pop(11 - 2)\n","\n","torch.save(episodes, \"PointPush_dirty_eps.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTjUVh0r8nx9"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] += math.pi / 2. # start facing up\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = agent.select_goal(g_state, False, False)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZYgmaK7PhX1","executionInfo":{"status":"ok","timestamp":1612554737036,"user_tz":-60,"elapsed":1375616,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"663bf7db-38e5-41bc-b06e-7cd32a92db68"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 13:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_push_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 range: {0.0: [95.44499999999128], 0.05: [95.3999999999917], 0.1: [94.03699999999236], 0.15: [94.04799999999324], 0.2: [89.60499999999023], 0.25: [75.1459999999871], 0.3: [65.60199999998177], 0.35: [52.56599999998234], 0.4: [45.32799999999969]}\n","1 range: {0.0: [95.44499999999128, 36.05900000001852], 0.05: [95.3999999999917, 35.320000000018034], 0.1: [94.03699999999236, 22.28400000001501], 0.15: [94.04799999999324, 25.4340000000167], 0.2: [89.60499999999023, 24.524000000004122], 0.25: [75.1459999999871, 7.6449999999989355], 0.3: [65.60199999998177, 6.787999999996071], 0.35: [52.56599999998234, -14.179999999999676], 0.4: [45.32799999999969, -15.628999999992866]}\n","2 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174]}\n","3 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655]}\n","4 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266]}\n","5 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336]}\n","6 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255]}\n","7 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778]}\n","8 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622]}\n","9 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947, 96.41599999999272], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495, 94.78899999999399], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822, 70.96699999998725], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872, 62.56499999999385], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723, 64.15099999998169], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415, 50.433000000002814], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816, 45.21900000001185], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517, 38.647000000016895], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622, 28.589000000020828]}\n","10 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947, 96.41599999999272, 93.42199999998874], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495, 94.78899999999399, 81.36599999998685], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822, 70.96699999998725, 66.75299999999257], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872, 62.56499999999385, 46.08400000000275], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723, 64.15099999998169, 43.32500000000374], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415, 50.433000000002814, 29.05400000002128], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816, 45.21900000001185, 40.33300000001278], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517, 38.647000000016895, 24.03200000001668], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622, 28.589000000020828, 25.19200000001165]}\n","11 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947, 96.41599999999272, 93.42199999998874, 95.62299999999314], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495, 94.78899999999399, 81.36599999998685, 91.05599999999224], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822, 70.96699999998725, 66.75299999999257, 86.19099999999162], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872, 62.56499999999385, 46.08400000000275, 77.4169999999892], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723, 64.15099999998169, 43.32500000000374, 68.8189999999857], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415, 50.433000000002814, 29.05400000002128, 48.184999999992606], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816, 45.21900000001185, 40.33300000001278, 49.8069999999921], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517, 38.647000000016895, 24.03200000001668, 24.381000000015316], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622, 28.589000000020828, 25.19200000001165, 25.57800000001338]}\n","12 range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947, 96.41599999999272, 93.42199999998874, 95.62299999999314, 94.9579999999925], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495, 94.78899999999399, 81.36599999998685, 91.05599999999224, 72.58499999999282], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822, 70.96699999998725, 66.75299999999257, 86.19099999999162, 66.68499999998218], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872, 62.56499999999385, 46.08400000000275, 77.4169999999892, 41.606000000003945], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723, 64.15099999998169, 43.32500000000374, 68.8189999999857, 36.968000000014975], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415, 50.433000000002814, 29.05400000002128, 48.184999999992606, 44.392000000012985], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816, 45.21900000001185, 40.33300000001278, 49.8069999999921, 23.896000000011924], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517, 38.647000000016895, 24.03200000001668, 24.381000000015316, 25.43100000000503], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622, 28.589000000020828, 25.19200000001165, 25.57800000001338, 16.632000000002314]}\n","----\n","range: {0.0: [95.44499999999128, 36.05900000001852, 90.06899999998274, 88.00699999999104, 94.55599999999153, 87.56699999998202, 96.14499999999386, 90.99999999998664, 97.0999999999947, 96.41599999999272, 93.42199999998874, 95.62299999999314, 94.9579999999925], 0.05: [95.3999999999917, 35.320000000018034, 89.11599999998919, 81.50699999999077, 87.9319999999874, 83.49799999998048, 93.28299999999295, 81.83599999998732, 93.60599999999495, 94.78899999999399, 81.36599999998685, 91.05599999999224, 72.58499999999282], 0.1: [94.03699999999236, 22.28400000001501, 78.08399999998227, 76.73499999999493, 84.82399999998871, 86.19299999997973, 91.31599999998963, 78.76499999998512, 71.13199999997822, 70.96699999998725, 66.75299999999257, 86.19099999999162, 66.68499999998218], 0.15: [94.04799999999324, 25.4340000000167, 69.13299999998137, 72.58599999998597, 86.29499999998814, 84.24699999998089, 91.01399999999003, 60.248999999987156, 72.9879999999872, 62.56499999999385, 46.08400000000275, 77.4169999999892, 41.606000000003945], 0.2: [89.60499999999023, 24.524000000004122, 60.61899999999437, 59.81399999999276, 74.47399999998618, 70.86999999998338, 82.34899999998439, 48.48499999997889, 42.16800000001723, 64.15099999998169, 43.32500000000374, 68.8189999999857, 36.968000000014975], 0.25: [75.1459999999871, 7.6449999999989355, 54.73599999998436, 62.098999999981686, 61.1379999999888, 63.07599999997908, 74.7129999999733, 50.146999999997135, 32.917000000012415, 50.433000000002814, 29.05400000002128, 48.184999999992606, 44.392000000012985], 0.3: [65.60199999998177, 6.787999999996071, 41.67400000001556, 48.145999999996576, 53.73400000000786, 66.38799999998058, 75.83799999997808, 57.14499999999433, 25.761000000013816, 45.21900000001185, 40.33300000001278, 49.8069999999921, 23.896000000011924], 0.35: [52.56599999998234, -14.179999999999676, 47.028999999992095, 50.50799999999881, 37.97700000001683, 41.50200000001119, 53.847999999997185, 40.031000000013336, 10.826000000012517, 38.647000000016895, 24.03200000001668, 24.381000000015316, 25.43100000000503], 0.4: [45.32799999999969, -15.628999999992866, 41.25500000001174, 43.607000000001655, 35.44100000001266, 43.115000000004336, 37.741000000011255, 37.10800000001778, 8.501999999995622, 28.589000000020828, 25.19200000001165, 25.57800000001338, 16.632000000002314]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVujCiQrP3hh"},"source":[""],"execution_count":null,"outputs":[]}]}