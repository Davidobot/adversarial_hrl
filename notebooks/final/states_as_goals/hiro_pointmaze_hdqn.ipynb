{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"hiro_pointmaze_hdqn.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-Y3vLeDo4pG","executionInfo":{"status":"ok","timestamp":1617372447198,"user_tz":-60,"elapsed":42426,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"2fa921f3-bbf2-4ea5-d9a7-5dbd9b40e5d1"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","!cp \"/content/drive/My Drive/Dissertation/envs/point_maze.py\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaaz1IRfpF1l","executionInfo":{"status":"ok","timestamp":1617372447201,"user_tz":-60,"elapsed":6936,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# for inference, not continued training\n","def save_model(model, name):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/point_maze_hdqn/{name}\" \n","\n","    torch.save({\n","      'meta_controller': model.meta_controller.state_dict(),\n","      'controller': {\n","          'critic': model.controller.critic.state_dict(),\n","          'actor': model.controller.actor.state_dict(),\n","      }\n","    }, path)\n","\n","import copy\n","def load_model(model, name, dir=\"point_maze_hdqn\"):\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{dir}/{name}\" \n","    checkpoint = torch.load(path)\n","\n","    #model.meta_controller.critic.load_state_dict(checkpoint['meta_controller']['critic'])\n","    #model.meta_controller.critic_target = copy.deepcopy(model.meta_controller.critic)\n","    #model.meta_controller.actor.load_state_dict(checkpoint['meta_controller']['actor'])\n","    #model.meta_controller.actor_target = copy.deepcopy(model.meta_controller.actor)\n","\n","    model.meta_controller.load_state_dict(checkpoint['meta_controller'])\n","    model.meta_controller_target = copy.deepcopy(model.meta_controller)\n","\n","    model.controller.critic.load_state_dict(checkpoint['controller']['critic'])\n","    model.controller.critic_target = copy.deepcopy(model.controller.critic)\n","    model.controller.actor.load_state_dict(checkpoint['controller']['actor'])\n","    model.controller.actor_target = copy.deepcopy(model.controller.actor)\n","\n","    # model.eval() for evaluation instead\n","    model.eval()\n","    model.meta_controller.eval()\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1617372448761,"user_tz":-60,"elapsed":7861,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1617372448763,"user_tz":-60,"elapsed":6095,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1617372448765,"user_tz":-60,"elapsed":5168,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["from point_maze import PointMazeEnv \n","env = NormalizedEnv(PointMazeEnv(4))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1617372448767,"user_tz":-60,"elapsed":3544,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1617372448769,"user_tz":-60,"elapsed":3370,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1617372448771,"user_tz":-60,"elapsed":2950,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","  \n","# (state, action) -> (next_state, reward, done)\n","transition_meta = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done', 'state_seq', 'action_seq'))\n","\n","# replay memory D with capacity N\n","class ReplayMemoryMeta(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition_meta(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1617372449250,"user_tz":-60,"elapsed":2318,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["DEPTH = 128\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, nb_actions)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return torch.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l2 = nn.Linear(DEPTH, DEPTH)\n","        self.l3 = nn.Linear(DEPTH, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, DEPTH)\n","        self.l5 = nn.Linear(DEPTH, DEPTH)\n","        self.l6 = nn.Linear(DEPTH, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1617372449651,"user_tz":-60,"elapsed":1815,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions, is_meta=False):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        self.is_meta = is_meta\n","\n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000) if not self.is_meta else ReplayMemoryMeta(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 50000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.is_training = True\n","\n","    def update_policy(self, off_policy_correction=None):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        if not self.is_meta:\n","            batch = transition(*zip(*transitions))\n","            action_batch = torch.cat(batch.action)\n","        else:\n","            batch = transition_meta(*zip(*transitions))\n","\n","            action_batch = torch.cat(batch.action)\n","            state_seq_batch = torch.stack(batch.state_seq)\n","            action_seq_batch = torch.stack(batch.action_seq)\n","\n","            action_batch = off_policy_correction(action_batch.cpu().numpy(), state_seq_batch.cpu().numpy(), action_seq_batch.cpu().numpy())\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","      \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau / 5)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, s_t, a_t, s_t1, r_t, done):\n","        self.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def random_action(self):\n","        return torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","\n","    def select_action(self, s_t, warmup, decay_epsilon):\n","        if warmup:\n","            return self.random_action()\n","\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            #action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * np.random.uniform(-1.,1.,1)).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            return action\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 100000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, DEPTH)\n","        self.fc2 = nn.Linear(DEPTH, DEPTH)\n","        self.head = nn.Linear(DEPTH, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.0\n","        self.EPS_DECAY = 10000 # in number of steps\n","        self.TAU = 0.001\n","\n","        self.eps_printed = False\n","\n","        self.policy_update = 2\n","        self.tot_updates = 0\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, warmup, is_training):\n","        if warmup: \n","            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            if eps_threshold <= 0.2 and not self.eps_printed:\n","                self.eps_printed = True\n","                print(\"EPS_THRESHOLD below 0.2\")\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.tot_updates += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        loss_val = loss.item()\n","\n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","\n","        if self.tot_updates % self.policy_update == 0:\n","            soft_update(target, self, self.TAU)\n","\n","        return loss_val"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u23kJqHhvw8","executionInfo":{"status":"ok","timestamp":1617372450350,"user_tz":-60,"elapsed":1343,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["class HIRO(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(HIRO, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions = nb_actions\n","        self.goal_dim = [0, 1]\n","        self.goal_dimen = 2\n","      \n","        learning_rate = 2.5e-4\n","        self.meta_controller = DQN(nb_states, 11 * 11).to(device)\n","        self.meta_controller_optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n","        self.meta_controller_target = DQN(nb_states, 11 * 11, mem_len = 0).to(device)\n","        self.meta_controller_target.eval()\n","\n","        self.max_goal_dist = torch.from_numpy(np.array([2.5, 2.5])).to(device)\n","        self.goal_offset = torch.from_numpy(np.array([1., 1.])).to(device)\n","\n","        self.controller = TD3(nb_states + len(self.goal_dim), nb_actions).to(device)\n","        #self.controller.depsilon = 1.0 / 500000\n","\n","    def teach_controller(self):\n","        self.controller.update_policy()\n","    def teach_meta_controller(self):\n","        self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n","\n","    def h(self, state, goal, next_state):\n","        #return goal\n","        return state[:,self.goal_dim] + goal - next_state[:,self.goal_dim]\n","    #def intrinsic_reward(self, action, goal):\n","    #    return torch.tensor(1.0 if self.goal_reached(action, goal) else 0.0, device=device) \n","    #def goal_reached(self, action, goal, threshold = 0.1):\n","    #    return torch.abs(action - goal) <= threshold\n","    def intrinsic_reward(self, reward, state, goal, next_state):\n","        #return torch.tensor(2 * reward if self.goal_reached(state, goal, next_state) else reward / 10, device=device) #reward / 2\n","        # just L2 norm\n","        return -torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5)\n","    def goal_reached(self, state, goal, next_state, threshold = 0.1):\n","        return torch.pow(sum(torch.pow(state.squeeze(0)[self.goal_dim] + goal.squeeze(0) - next_state.squeeze(0)[self.goal_dim], 2)), 0.5) <= threshold\n","        #return torch.pow(sum(goal.squeeze(0), 2), 0.5) <= threshold\n","\n","    def observe_controller(self, s_t, a_t, s_t1, r_t, done):\n","        self.controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","    def observe_meta_controller(self, s_t, a_t, s_t1, r_t, done, state_seq, action_seq):\n","        self.meta_controller.memory.store(s_t, a_t, s_t1, r_t, done)\n","\n","    def action_to_2D(self, a):\n","        x = (a % 11)\n","        y = (a // 11)\n","        return -1.0 + 0.2 * torch.cat([x, y], axis=1).float()\n","\n","    def convert_goal(self, a):\n","        return self.action_to_2D(a) * self.max_goal_dist + self.goal_offset\n","\n","    def select_goal(self, s_t, warmup, is_training):\n","        return self.meta_controller.act(s_t, warmup, is_training)\n","    \n","    def select_action(self, s_t, g_t, warmup, decay_epsilon):\n","        sg_t = torch.cat([s_t, g_t], 1).float()\n","        return self.controller.select_action(sg_t, warmup, decay_epsilon)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["import time\n","SAVE_OFFSET = 6\n","def train_model():\n","    global SAVE_OFFSET\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = HIRO(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    observation = None\n","    \n","    warmup = 100\n","    num_episodes = 4000 # M\n","    episode_durations = []\n","    goal_durations = []\n","\n","    steps = 0\n","    c = 10\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        \n","        overall_reward = 0\n","        overall_intrinsic = 0\n","        episode_steps = 0\n","        done = False\n","        goals_done = 0\n","\n","        while not done:\n","            goal_raw = agent.select_goal(state, i_episode <= warmup, True)\n","            goal = agent.convert_goal(goal_raw)\n","            #goal_durations.append((steps, goal[:,0]))\n","\n","            state_seq, action_seq = None, None\n","            first_goal = goal\n","            goal_done = False\n","            total_extrinsic = 0\n","\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([state, goal], axis=1).float()\n","\n","                # agent pick action ...\n","                action = agent.select_action(state, goal, i_episode <= warmup, True)\n","                \n","                # env response with next_observation, reward, terminate_info\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                steps += 1\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                next_goal = agent.h(state, goal, next_state)\n","                joint_next_state = torch.cat([next_state, next_goal], axis=1).float()\n","                \n","                if max_episode_length and episode_steps >= max_episode_length -1:\n","                    done = True\n","                    \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","                intrinsic_reward = agent.intrinsic_reward(reward, state, goal, next_state).unsqueeze(0)\n","                #intrinsic_reward = agent.intrinsic_reward(action, goal).unsqueeze(0)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","                overall_intrinsic += intrinsic_reward\n","\n","                goal_reached = agent.goal_reached(state, goal, next_state)\n","                #goal_done = agent.goal_reached(action, goal)\n","\n","                # agent observe and update policy\n","                agent.observe_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done) #goal_done.item())\n","\n","                if state_seq is None:\n","                    state_seq = state\n","                else:\n","                    state_seq = torch.cat([state_seq, state])\n","                if action_seq is None:\n","                    action_seq = action\n","                else:\n","                    action_seq = torch.cat([action_seq, action])\n","\n","                episode_steps += 1\n","\n","                if goal_reached:\n","                    goals_done += 1\n","                \n","                if (episode_steps % c) == 0:\n","                    agent.observe_meta_controller(state_seq[0].unsqueeze(0), goal_raw, next_state, torch.tensor([total_extrinsic], device=device), done,\\\n","                                                  state_seq, action_seq)\n","                    goal_done = True\n","\n","                    if i_episode > warmup:\n","                        agent.teach_meta_controller()\n","\n","                state = next_state\n","                goal = next_goal\n","                \n","                if i_episode > warmup:\n","                    agent.teach_controller()\n","\n","        goal_durations.append((i_episode, overall_intrinsic / episode_steps))\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, goal_durations)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0:\n","                print(f\"{i_episode}: {np.mean(dur[-100:])}\")\n","            if i_episode >= 300 and i_episode % 100 == 0 and np.mean(dur[-100:]) <= -49.0:\n","                print(f\"Unlucky after {i_episode} eps! Terminating...\")\n","                return None\n","            if np.mean(dur[-100:]) >= 90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(agent, f\"hiro_freeze_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return agent\n","\n","    return None # did not train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def eval_model(agent, episode_durations, goal_attack, action_attack, same_noise):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for l2norm in np.arange(0.0,0.51,0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","            if goal_attack:\n","                g_state = g_state + state_range * noise\n","                g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","            if action_attack:\n","                if same_noise:\n","                    state = state + state_range * noise\n","                else:\n","                    state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    if goal_attack:\n","                        g_next_state = g_next_state + state_range * noise\n","                        g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                    if action_attack:\n","                        if same_noise:\n","                            next_state = next_state + state_range * noise\n","                        else:\n","                            next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                        next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(l2norm, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-GH33rpv6-Z","executionInfo":{"status":"ok","timestamp":1617374031747,"user_tz":-60,"elapsed":519,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def fgsm_attack(data, eps, data_grad):\n","    sign_data_grad = data_grad.sign()\n","\n","    perturbed_data = data + eps * sign_data_grad * state_range\n","\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\n","\n","    return clipped_perturbed_data\n","\n","def fgsm_goal(g_state, agent, eps, target, targeted):\n","    g_state = g_state.clone().detach().requires_grad_(True)\n","\n","    # initial forward pass\n","    goal = agent.meta_controller(g_state)\n","\n","    if targeted:\n","        loss = F.mse_loss(goal, target)\n","    else:\n","        loss = F.mse_loss(goal, -goal) # doing -goal inverts the argmax \n","\n","    agent.meta_controller.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = g_state.grad.data\n","\n","    # perturb state\n","    g_state_p = fgsm_attack(g_state, eps, data_grad).float()\n","    return g_state_p\n","\n","def fgsm_action(state, goal, agent, eps, target, targeted):\n","    #state = torch.tensor(state, requires_grad=True)\n","    state = state.clone().detach().requires_grad_(True)\n","    goal = goal.clone().detach()\n","\n","    sg_t = torch.cat([state, goal], 1).float()\n","\n","    if targeted:\n","        # initial forward pass\n","        action = agent.controller.actor(sg_t)\n","        action = torch.clamp(action, -1., 1.)\n","\n","        loss = F.mse_loss(action, target)\n","    else:\n","        loss = agent.controller.critic.Q1(sg_t, agent.controller.actor(sg_t)).mean()\n","\n","    agent.controller.actor.zero_grad()\n","\n","    # calc loss\n","    loss.backward()\n","    data_grad = state.grad.data\n","    # perturb state\n","    state_p = fgsm_attack(state, eps, data_grad).float()\n","    return state_p\n","\n","def apply_fgsm(agent, episode_durations, goal_attack, targeted):\n","    TARGET_GOAL = torch.tensor([[0.0] * (11 * 11)], device=device, dtype=torch.float)\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\n","\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for eps in np.arange(0.0, 0.201, 0.02):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            if goal_attack: # target meta controller\n","                state = fgsm_goal(og_state, agent, eps, TARGET_GOAL, targeted)\n","            else: # target controller\n","                goal_raw = agent.select_goal(og_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","                state = fgsm_action(og_state, goal, agent, eps, TARGET_ACTION, targeted)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                goal_raw = agent.select_goal(state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    \n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","\n","                    next_og_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    if goal_attack: # target meta controller\n","                        next_state = fgsm_goal(next_og_state, agent, eps, TARGET_GOAL, targeted)\n","                    else: # target controller\n","                        goal_temp = agent.h(state, goal, next_og_state)\n","                        next_state = fgsm_action(next_og_state, goal_temp, agent, eps, TARGET_ACTION, targeted)\n","\n","                    next_goal = agent.h(state, goal, next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(state, goal, next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    goal = next_goal\n","\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrR0kvDhFwRa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617186614776,"user_tz":-60,"elapsed":9270840,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"835667d7-01b9-4529-b502-5f8bf8f87072"},"source":["noise_hrl = {'both': {}, 'action_only': {}, 'goal_only': {}, 'both_same': {}}\n","for l2norm in np.arange(0,0.51,0.05):\n","    for i in [noise_hrl['both'], noise_hrl['action_only'], noise_hrl['goal_only'], noise_hrl['both_same']]:\n","        i[np.round(l2norm, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 6\n","while i < 8:\n","    agent = train_model()\n","    #agent = HIRO(n_observations, n_actions).to(device)\n","    #load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, noise_hrl['both_same'], True, True, True)\n","        eval_model(agent, noise_hrl['both'], True, True, False)\n","        eval_model(agent, noise_hrl['action_only'], False, True, False)\n","        eval_model(agent, noise_hrl['goal_only'], True, False, False)\n","        print(f\"{i} noise_hrl: {noise_hrl}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"noise_hrl: {noise_hrl}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100: -47.91100000000043\n","200: -30.8890000000004\n","EPS_THRESHOLD below 0.2\n","300: -50.000000000000426\n","Unlucky after 300 eps! Terminating...\n","100: -46.60600000000042\n","200: -37.45600000000042\n","EPS_THRESHOLD below 0.2\n","300: -24.86000000000039\n","400: -37.27500000000042\n","500: -32.494000000000405\n","600: -34.78100000000041\n","700: -29.876000000000403\n","800: -13.18800000000036\n","900: -2.121000000000334\n","1000: -2.967000000000334\n","1100: 4.234999999999701\n","1200: 3.037999999999697\n","1300: 30.70399999999976\n","1400: 7.1029999999996996\n","1500: 30.432999999999765\n","1600: 27.254999999999747\n","1700: 43.51499999999983\n","1800: 32.20399999999979\n","1900: 52.54299999999987\n","2000: 53.54299999999986\n","2100: 62.243999999999886\n","2200: 70.85999999999991\n","2300: 78.64299999999994\n","2400: 82.24099999999994\n","2500: 80.39399999999996\n","2600: 88.75999999999995\n","Solved after 2609 episodes!\n","6 noise_hrl: {'both': {0.0: [97.82899999999603], 0.05: [96.44699999999304], 0.1: [93.81199999999184], 0.15: [91.70999999998986], 0.2: [89.22599999997797], 0.25: [86.09199999997871], 0.3: [82.31999999997177], 0.35: [77.74999999997164], 0.4: [62.13699999997598], 0.45: [37.91500000001881], 0.5: [19.007000000007235]}, 'action_only': {0.0: [97.69399999999581], 0.05: [96.4959999999941], 0.1: [95.61999999999207], 0.15: [93.5629999999892], 0.2: [89.37699999998011], 0.25: [86.77699999997911], 0.3: [81.65199999997623], 0.35: [79.45099999997542], 0.4: [75.07399999997591], 0.45: [67.41199999996898], 0.5: [62.98799999997861]}, 'goal_only': {0.0: [97.77699999999602], 0.05: [97.82899999999569], 0.1: [96.53099999999408], 0.15: [96.22499999999586], 0.2: [93.33499999999061], 0.25: [95.44799999999387], 0.3: [94.00299999999257], 0.35: [95.68799999999061], 0.4: [95.33099999999092], 0.45: [93.52499999998685], 0.5: [90.64399999998761]}, 'both_same': {0.0: [97.71999999999589], 0.05: [96.8359999999948], 0.1: [94.21999999999001], 0.15: [92.40799999998937], 0.2: [90.28599999998467], 0.25: [86.67799999997892], 0.3: [85.3019999999742], 0.35: [84.74399999997381], 0.4: [79.87599999997215], 0.45: [68.55099999996884], 0.5: [62.87899999997895]}}\n","100: -46.516000000000425\n","200: -30.980000000000405\n","EPS_THRESHOLD below 0.2\n","300: -41.88000000000042\n","400: -36.861000000000416\n","500: -28.30900000000038\n","600: -19.61000000000037\n","700: -1.7350000000003296\n","800: 18.712999999999727\n","900: 29.721999999999774\n","1000: 26.66999999999976\n","1100: 42.838999999999835\n","1200: 41.966999999999814\n","1300: 26.47399999999977\n","1400: 38.37399999999982\n","1500: 26.82199999999976\n","1600: 44.599999999999824\n","1700: 69.68899999999991\n","1800: 70.31399999999992\n","1900: 63.1109999999999\n","2000: 73.6719999999999\n","2100: 75.50899999999993\n","2200: 73.85899999999994\n","2300: 73.01899999999996\n","2400: 82.08199999999997\n","2500: 81.19099999999996\n","Solved after 2599 episodes!\n","7 noise_hrl: {'both': {0.0: [97.82899999999603, 65.29299999998354], 0.05: [96.44699999999304, 92.15399999998591], 0.1: [93.81199999999184, 92.5369999999855], 0.15: [91.70999999998986, 91.31299999998562], 0.2: [89.22599999997797, 87.76699999997496], 0.25: [86.09199999997871, 84.58399999997476], 0.3: [82.31999999997177, 64.9439999999748], 0.35: [77.74999999997164, 49.18899999999287], 0.4: [62.13699999997598, 19.622000000009365], 0.45: [37.91500000001881, 15.316000000010893], 0.5: [19.007000000007235, 2.3459999999994205]}, 'action_only': {0.0: [97.69399999999581, 59.11199999999989], 0.05: [96.4959999999941, 77.41099999997395], 0.1: [95.61999999999207, 86.57099999997834], 0.15: [93.5629999999892, 86.219999999977], 0.2: [89.37699999998011, 84.12599999997842], 0.25: [86.77699999997911, 80.37199999997414], 0.3: [81.65199999997623, 65.06299999998367], 0.35: [79.45099999997542, 39.876000000016596], 0.4: [75.07399999997591, 12.745999999998315], 0.45: [67.41199999996898, 2.257999999995346], 0.5: [62.98799999997861, -9.262999999999911]}, 'goal_only': {0.0: [97.77699999999602, 71.5839999999821], 0.05: [97.82899999999569, 85.31499999998645], 0.1: [96.53099999999408, 89.38899999998706], 0.15: [96.22499999999586, 92.94299999998557], 0.2: [93.33499999999061, 92.71299999998712], 0.25: [95.44799999999387, 93.25299999998701], 0.3: [94.00299999999257, 93.10099999998891], 0.35: [95.68799999999061, 92.98599999998928], 0.4: [95.33099999999092, 92.67699999998686], 0.45: [93.52499999998685, 91.43899999998601], 0.5: [90.64399999998761, 90.28399999998756]}, 'both_same': {0.0: [97.71999999999589, 59.01999999997413], 0.05: [96.8359999999948, 89.28599999998396], 0.1: [94.21999999999001, 92.29999999998745], 0.15: [92.40799999998937, 90.35499999998409], 0.2: [90.28599999998467, 82.84899999997594], 0.25: [86.67799999997892, 78.0859999999681], 0.3: [85.3019999999742, 58.71599999998339], 0.35: [84.74399999997381, 56.87999999998598], 0.4: [79.87599999997215, 35.62000000002147], 0.45: [68.55099999996884, 18.95700000002237], 0.5: [62.87899999997895, 1.8829999999964457]}}\n","----\n","noise_hrl: {'both': {0.0: [97.82899999999603, 65.29299999998354], 0.05: [96.44699999999304, 92.15399999998591], 0.1: [93.81199999999184, 92.5369999999855], 0.15: [91.70999999998986, 91.31299999998562], 0.2: [89.22599999997797, 87.76699999997496], 0.25: [86.09199999997871, 84.58399999997476], 0.3: [82.31999999997177, 64.9439999999748], 0.35: [77.74999999997164, 49.18899999999287], 0.4: [62.13699999997598, 19.622000000009365], 0.45: [37.91500000001881, 15.316000000010893], 0.5: [19.007000000007235, 2.3459999999994205]}, 'action_only': {0.0: [97.69399999999581, 59.11199999999989], 0.05: [96.4959999999941, 77.41099999997395], 0.1: [95.61999999999207, 86.57099999997834], 0.15: [93.5629999999892, 86.219999999977], 0.2: [89.37699999998011, 84.12599999997842], 0.25: [86.77699999997911, 80.37199999997414], 0.3: [81.65199999997623, 65.06299999998367], 0.35: [79.45099999997542, 39.876000000016596], 0.4: [75.07399999997591, 12.745999999998315], 0.45: [67.41199999996898, 2.257999999995346], 0.5: [62.98799999997861, -9.262999999999911]}, 'goal_only': {0.0: [97.77699999999602, 71.5839999999821], 0.05: [97.82899999999569, 85.31499999998645], 0.1: [96.53099999999408, 89.38899999998706], 0.15: [96.22499999999586, 92.94299999998557], 0.2: [93.33499999999061, 92.71299999998712], 0.25: [95.44799999999387, 93.25299999998701], 0.3: [94.00299999999257, 93.10099999998891], 0.35: [95.68799999999061, 92.98599999998928], 0.4: [95.33099999999092, 92.67699999998686], 0.45: [93.52499999998685, 91.43899999998601], 0.5: [90.64399999998761, 90.28399999998756]}, 'both_same': {0.0: [97.71999999999589, 59.01999999997413], 0.05: [96.8359999999948, 89.28599999998396], 0.1: [94.21999999999001, 92.29999999998745], 0.15: [92.40799999998937, 90.35499999998409], 0.2: [90.28599999998467, 82.84899999997594], 0.25: [86.67799999997892, 78.0859999999681], 0.3: [85.3019999999742, 58.71599999998339], 0.35: [84.74399999997381, 56.87999999998598], 0.4: [79.87599999997215, 35.62000000002147], 0.45: [68.55099999996884, 18.95700000002237], 0.5: [62.87899999997895, 1.8829999999964457]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QY8Hlh4y2gYn"},"source":["Solved after 2291 episodes!\n","0 noise_hrl: {'both': {0.0: [-30.829999999972802], 0.05: [44.355000000004544], 0.1: [81.24899999998394], 0.15: [90.456999999985], 0.2: [90.84699999998669], 0.25: [88.66699999998532], 0.3: [76.20899999997536], 0.35: [50.00099999999796], 0.4: [34.30700000001833], 0.45: [21.420000000003917], 0.5: [12.15899999999758]}, 'action_only': {0.0: [-20.799999999982944], 0.05: [58.010999999977805], 0.1: [74.64899999999506], 0.15: [83.07699999998587], 0.2: [91.40699999998455], 0.25: [90.08799999998205], 0.3: [78.69899999998805], 0.35: [63.095999999981686], 0.4: [19.254000000008674], 0.45: [25.772000000010276], 0.5: [10.354999999998443]}, 'goal_only': {0.0: [-26.453999999979782], 0.05: [-37.191999999971216], 0.1: [-32.58999999997594], 0.15: [-31.337999999975867], 0.2: [60.63999999999929], 0.25: [83.5179999999866], 0.3: [93.42299999998794], 0.35: [81.613999999985], 0.4: [89.31799999998591], 0.45: [87.89599999998948], 0.5: [84.92599999998062]}, 'both_same': {0.0: [-22.420999999980612], 0.05: [53.55499999999253], 0.1: [87.31299999998333], 0.15: [92.64499999998989], 0.2: [88.94999999998097], 0.25: [79.96199999998352], 0.3: [71.88799999998301], 0.35: [46.09799999999483], 0.4: [22.516000000017613], 0.45: [15.250999999999108], 0.5: [6.852999999997402]}}\n","5 noise_hrl: {'both': {0.0: [98.01599999999632, 36.639000000023046, 97.55199999999559, 92.34099999999056, 84.88899999997722], 0.05: [58.28899999999032, 66.57299999997998, 95.64599999999312, 96.16199999999206, 77.0429999999785], 0.1: [78.18799999997394, 87.93399999997868, 96.6599999999937, 95.80499999999235, 88.77799999998224], 0.15: [76.80599999998189, 87.15399999998104, 86.58699999998959, 94.81699999999012, 87.25099999998193], 0.2: [68.3009999999912, 85.61099999997661, 90.16199999998754, 92.79199999998713, 88.36799999997946], 0.25: [53.39499999999981, 82.75299999997458, 90.55599999998424, 87.54399999998245, 78.27599999997085], 0.3: [27.53400000001844, 74.16799999997197, 88.57999999998182, 65.69999999998221, 29.63900000001708], 0.35: [13.664000000014068, 51.14800000000055, 78.48099999997677, 29.8750000000182, -7.209000000003411], 0.4: [-8.589000000006196, 23.072000000019, 59.733999999987454, 14.714999999999682, 0.6390000000008723], 0.45: [-6.692000000006735, 2.467999999998482, 34.94700000001449, -5.663000000004641, -14.066999999979803], 0.5: [-13.730999999998703, -4.9600000000013, 24.084000000021142, -10.959000000001637, -20.08099999998329]}, 'action_only': {0.0: [98.01699999999633, 32.129000000018884, 97.55199999999546, 89.10099999998911, 88.10699999997438], 0.05: [74.02599999997364, 46.40499999998697, 93.54099999999224, 96.2049999999927, 64.03999999997953], 0.1: [67.16999999997108, 74.37399999996306, 96.51299999999328, 95.18899999999452, 73.99199999996809], 0.15: [82.27099999998256, 81.74499999997238, 94.9499999999901, 96.05499999999302, 80.92999999996903], 0.2: [83.47899999998603, 82.45099999997753, 94.34599999999062, 94.88199999999085, 81.04899999997383], 0.25: [60.79599999998488, 77.78699999997103, 92.51799999998633, 88.31099999998685, 71.88699999996089], 0.3: [47.50500000000852, 71.33799999997565, 90.590999999986, 78.43899999997718, 33.247000000019604], 0.35: [22.09100000000553, 37.819000000022115, 83.17599999998407, 57.877999999981746, -9.5860000000061], 0.4: [3.4589999999961596, 18.058000000003354, 72.93699999997924, 26.971000000016083, -24.664999999976885], 0.45: [-5.250000000005879, -4.13200000000586, 55.618999999999616, 15.976000000009288, -27.21099999997503], 0.5: [-12.2729999999968, -2.3810000000042426, 40.084000000011486, -2.909999999999247, -33.94699999997683]}, 'goal_only': {0.0: [98.01699999999633, 34.55700000001995, 97.58799999999547, 89.50399999998916, 79.19999999997552], 0.05: [78.43999999998027, 40.63700000002469, 96.12099999999387, 90.99199999998764, 79.41999999997216], 0.1: [83.73399999998095, 81.49699999997507, 97.59199999999556, 95.98799999999308, 90.91199999998345], 0.15: [81.77799999998993, 84.96199999997626, 95.82199999999366, 94.21899999998959, 88.46999999998617], 0.2: [87.01199999998926, 86.63499999997504, 97.18199999999466, 87.81999999998796, 95.50799999999055], 0.25: [80.02499999998363, 88.63999999998363, 96.63099999999433, 93.33999999998753, 95.58199999999141], 0.3: [83.34599999998369, 89.26099999998195, 93.45999999999361, 94.55799999999071, 95.49199999999175], 0.35: [89.33799999998722, 89.97099999998177, 92.61999999998736, 93.9869999999914, 95.07499999999149], 0.4: [93.33299999998941, 91.13399999998414, 95.50499999999346, 92.83999999998976, 95.2619999999916], 0.45: [87.7649999999844, 90.94799999998453, 91.63899999999246, 93.27699999998941, 94.80399999999072], 0.5: [88.32699999998579, 91.38499999998476, 91.85899999998588, 93.31899999998716, 94.5309999999899]}, 'both_same': {0.0: [98.00599999999633, 29.817000000021878, 97.56499999999559, 93.37699999998918, 86.89099999998079], 0.05: [69.10599999997555, 72.61899999997904, 94.01399999999033, 93.14399999999196, 81.53999999998506], 0.1: [72.37499999997979, 87.25999999997853, 94.8119999999918, 95.72699999999212, 88.40499999998242], 0.15: [83.74699999997942, 87.76199999998082, 94.0959999999921, 95.39499999999042, 87.93599999998237], 0.2: [79.39899999998362, 87.53799999997895, 91.656999999986, 93.13099999998981, 89.6909999999831], 0.25: [65.31199999998294, 83.14599999997334, 89.97699999998297, 78.16999999997944, 81.30699999997334], 0.3: [53.73399999999106, 75.02899999998276, 85.36999999997568, 54.61499999998938, 37.177000000019426], 0.35: [34.627000000013915, 50.75999999999898, 67.92899999997651, 23.79400000001535, 11.203999999996539], 0.4: [22.919000000008726, 28.07900000002257, 55.890999999980195, 8.285999999993898, -7.459000000003863], 0.45: [4.899999999995564, 10.023999999997603, 27.164000000018945, -8.26000000000785, -8.860000000006037], 0.5: [0.038999999998428334, 2.1289999999983915, 30.762000000016602, -8.867000000007742, -7.663999999999401]}}\n","7 noise_hrl: {'both': {0.0: [97.82899999999603, 65.29299999998354], 0.05: [96.44699999999304, 92.15399999998591], 0.1: [93.81199999999184, 92.5369999999855], 0.15: [91.70999999998986, 91.31299999998562], 0.2: [89.22599999997797, 87.76699999997496], 0.25: [86.09199999997871, 84.58399999997476], 0.3: [82.31999999997177, 64.9439999999748], 0.35: [77.74999999997164, 49.18899999999287], 0.4: [62.13699999997598, 19.622000000009365], 0.45: [37.91500000001881, 15.316000000010893], 0.5: [19.007000000007235, 2.3459999999994205]}, 'action_only': {0.0: [97.69399999999581, 59.11199999999989], 0.05: [96.4959999999941, 77.41099999997395], 0.1: [95.61999999999207, 86.57099999997834], 0.15: [93.5629999999892, 86.219999999977], 0.2: [89.37699999998011, 84.12599999997842], 0.25: [86.77699999997911, 80.37199999997414], 0.3: [81.65199999997623, 65.06299999998367], 0.35: [79.45099999997542, 39.876000000016596], 0.4: [75.07399999997591, 12.745999999998315], 0.45: [67.41199999996898, 2.257999999995346], 0.5: [62.98799999997861, -9.262999999999911]}, 'goal_only': {0.0: [97.77699999999602, 71.5839999999821], 0.05: [97.82899999999569, 85.31499999998645], 0.1: [96.53099999999408, 89.38899999998706], 0.15: [96.22499999999586, 92.94299999998557], 0.2: [93.33499999999061, 92.71299999998712], 0.25: [95.44799999999387, 93.25299999998701], 0.3: [94.00299999999257, 93.10099999998891], 0.35: [95.68799999999061, 92.98599999998928], 0.4: [95.33099999999092, 92.67699999998686], 0.45: [93.52499999998685, 91.43899999998601], 0.5: [90.64399999998761, 90.28399999998756]}, 'both_same': {0.0: [97.71999999999589, 59.01999999997413], 0.05: [96.8359999999948, 89.28599999998396], 0.1: [94.21999999999001, 92.29999999998745], 0.15: [92.40799999998937, 90.35499999998409], 0.2: [90.28599999998467, 82.84899999997594], 0.25: [86.67799999997892, 78.0859999999681], 0.3: [85.3019999999742, 58.71599999998339], 0.35: [84.74399999997381, 56.87999999998598], 0.4: [79.87599999997215, 35.62000000002147], 0.45: [68.55099999996884, 18.95700000002237], 0.5: [62.87899999997895, 1.8829999999964457]}}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhC6f7N6sJoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617386020376,"user_tz":-60,"elapsed":11970719,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"5ac0cc51-c1f4-40e2-832d-7d43f3566fb4"},"source":["targeted = {'goal': {}, 'action': {}}\n","untargeted = {'goal': {}, 'action': {}}\n","for eps in np.arange(0.0, 0.201, 0.02):\n","    for x in ['goal', 'action']:\n","        targeted[x][eps] = []\n","        untargeted[x][eps] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 1\n","while i < 7:\n","    if i == 2:\n","        i +=1 \n","        continue\n","\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        apply_fgsm(agent, untargeted['goal'], True, False)  \n","        apply_fgsm(agent, untargeted['action'], False, False)   \n","        print(f\"{i} fgsm (ut): {untargeted}\")\n","\n","        apply_fgsm(agent, targeted['goal'], True, True)\n","        apply_fgsm(agent, targeted['action'], False, True)   \n","        print(f\"{i} fgsm (t): {targeted}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"fgsm (ut): {untargeted}\")\n","print(f\"fgsm (t): {targeted}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["1 fgsm (ut): {'goal': {0.0: [98.01999999999634], 0.02: [25.084000000020552], 0.04: [59.63099999998164], 0.06: [37.924000000016655], 0.08: [69.38399999998408], 0.1: [58.85599999998814], 0.12: [47.533999999996425], 0.14: [50.84199999999169], 0.16: [35.63100000001336], 0.18: [15.292000000015904], 0.2: [3.68599999999391]}, 'action': {0.0: [98.01599999999635], 0.02: [5.719999999994806], 0.04: [9.759999999998149], 0.06: [18.841000000016358], 0.08: [23.346000000009607], 0.1: [22.929000000011147], 0.12: [28.72400000001588], 0.14: [27.320000000013756], 0.16: [17.27100000000984], 0.18: [2.579999999999727], 0.2: [-22.13099999998295]}}\n","1 fgsm (t): {'goal': {0.0: [98.01499999999632], 0.02: [28.845000000011268], 0.04: [44.61800000001236], 0.06: [45.538000000002036], 0.08: [57.06999999997237], 0.1: [44.58400000001001], 0.12: [46.358000000010335], 0.14: [38.69200000001914], 0.16: [32.884000000020436], 0.18: [2.1829999999999807], 0.2: [-4.071000000004246]}, 'action': {0.0: [98.01599999999635], 0.02: [93.97799999999357], 0.04: [65.67799999998644], 0.06: [80.36099999998686], 0.08: [67.33899999998445], 0.1: [67.40299999997808], 0.12: [56.91199999998366], 0.14: [37.283000000013516], 0.16: [29.69400000001788], 0.18: [6.8509999999971605], 0.2: [-12.825000000003277]}}\n","3 fgsm (ut): {'goal': {0.0: [98.01999999999634, 97.57599999999556], 0.02: [25.084000000020552, 97.4389999999953], 0.04: [59.63099999998164, 94.55399999999045], 0.06: [37.924000000016655, 87.53499999997948], 0.08: [69.38399999998408, 76.37199999998988], 0.1: [58.85599999998814, 48.356999999996525], 0.12: [47.533999999996425, 66.0909999999815], 0.14: [50.84199999999169, 58.84199999998029], 0.16: [35.63100000001336, 65.0669999999846], 0.18: [15.292000000015904, 52.894999999983035], 0.2: [3.68599999999391, 36.16600000002009]}, 'action': {0.0: [98.01599999999635, 97.57399999999548], 0.02: [5.719999999994806, 80.25399999998783], 0.04: [9.759999999998149, 66.63199999997997], 0.06: [18.841000000016358, 23.058000000010814], 0.08: [23.346000000009607, 2.164999999999567], 0.1: [22.929000000011147, 22.336000000014355], 0.12: [28.72400000001588, 13.622000000005098], 0.14: [27.320000000013756, -12.092999999978723], 0.16: [17.27100000000984, -17.729999999986145], 0.18: [2.579999999999727, -29.635999999979443], 0.2: [-22.13099999998295, -31.332999999973104]}}\n","3 fgsm (t): {'goal': {0.0: [98.01499999999632, 97.60199999999558], 0.02: [28.845000000011268, 97.45499999999552], 0.04: [44.61800000001236, 94.15599999999462], 0.06: [45.538000000002036, 89.3019999999811], 0.08: [57.06999999997237, 78.16799999999067], 0.1: [44.58400000001001, 43.408000000012336], 0.12: [46.358000000010335, 66.67299999998231], 0.14: [38.69200000001914, 65.131999999988], 0.16: [32.884000000020436, 64.49999999997614], 0.18: [2.1829999999999807, 56.58099999998245], 0.2: [-4.071000000004246, 35.10400000002046]}, 'action': {0.0: [98.01599999999635, 97.58799999999557], 0.02: [93.97799999999357, 90.37699999999403], 0.04: [65.67799999998644, 91.98899999998982], 0.06: [80.36099999998686, 84.69699999998707], 0.08: [67.33899999998445, 70.74999999997172], 0.1: [67.40299999997808, 74.51999999997682], 0.12: [56.91199999998366, 71.37999999997524], 0.14: [37.283000000013516, 66.08699999998078], 0.16: [29.69400000001788, 41.61700000001738], 0.18: [6.8509999999971605, 25.562000000014606], 0.2: [-12.825000000003277, 16.15300000000458]}}\n","4 fgsm (ut): {'goal': {0.0: [98.01999999999634, 97.57599999999556, 90.75399999998822], 0.02: [25.084000000020552, 97.4389999999953, 91.69899999998533], 0.04: [59.63099999998164, 94.55399999999045, 83.8279999999853], 0.06: [37.924000000016655, 87.53499999997948, 89.63499999998919], 0.08: [69.38399999998408, 76.37199999998988, 86.89799999998492], 0.1: [58.85599999998814, 48.356999999996525, 88.35399999998184], 0.12: [47.533999999996425, 66.0909999999815, 60.30199999998754], 0.14: [50.84199999999169, 58.84199999998029, 35.796000000017486], 0.16: [35.63100000001336, 65.0669999999846, 43.75300000000993], 0.18: [15.292000000015904, 52.894999999983035, 10.705999999994024], 0.2: [3.68599999999391, 36.16600000002009, 2.5309999999984534]}, 'action': {0.0: [98.01599999999635, 97.57399999999548, 92.26799999998974], 0.02: [5.719999999994806, 80.25399999998783, 60.66799999998297], 0.04: [9.759999999998149, 66.63199999997997, 63.596999999978294], 0.06: [18.841000000016358, 23.058000000010814, -13.584999999997917], 0.08: [23.346000000009607, 2.164999999999567, -8.843000000000231], 0.1: [22.929000000011147, 22.336000000014355, -30.01899999997263], 0.12: [28.72400000001588, 13.622000000005098, -27.802999999974755], 0.14: [27.320000000013756, -12.092999999978723, -30.4159999999724], 0.16: [17.27100000000984, -17.729999999986145, -27.148999999979736], 0.18: [2.579999999999727, -29.635999999979443, -35.82199999997346], 0.2: [-22.13099999998295, -31.332999999973104, -35.719999999971876]}}\n","4 fgsm (t): {'goal': {0.0: [98.01499999999632, 97.60199999999558, 93.7089999999893], 0.02: [28.845000000011268, 97.45499999999552, 91.64799999998633], 0.04: [44.61800000001236, 94.15599999999462, 93.40799999999048], 0.06: [45.538000000002036, 89.3019999999811, 87.16999999998424], 0.08: [57.06999999997237, 78.16799999999067, 87.12499999998445], 0.1: [44.58400000001001, 43.408000000012336, 88.79999999997834], 0.12: [46.358000000010335, 66.67299999998231, 67.17199999997128], 0.14: [38.69200000001914, 65.131999999988, 45.06300000000963], 0.16: [32.884000000020436, 64.49999999997614, 31.53800000001534], 0.18: [2.1829999999999807, 56.58099999998245, 4.117999999995458], 0.2: [-4.071000000004246, 35.10400000002046, 11.479999999994448]}, 'action': {0.0: [98.01599999999635, 97.58799999999557, 91.48599999999183], 0.02: [93.97799999999357, 90.37699999999403, 74.37099999997676], 0.04: [65.67799999998644, 91.98899999998982, 67.85799999997614], 0.06: [80.36099999998686, 84.69699999998707, 79.88699999998288], 0.08: [67.33899999998445, 70.74999999997172, 76.5129999999776], 0.1: [67.40299999997808, 74.51999999997682, 51.01899999999085], 0.12: [56.91199999998366, 71.37999999997524, 54.83199999998621], 0.14: [37.283000000013516, 66.08699999998078, 18.198000000009557], 0.16: [29.69400000001788, 41.61700000001738, 6.955999999996449], 0.18: [6.8509999999971605, 25.562000000014606, -31.096999999974624], 0.2: [-12.825000000003277, 16.15300000000458, -23.881999999975818]}}\n","5 fgsm (ut): {'goal': {0.0: [98.01999999999634, 97.57599999999556, 90.75399999998822, 83.65899999998014], 0.02: [25.084000000020552, 97.4389999999953, 91.69899999998533, 78.11299999998722], 0.04: [59.63099999998164, 94.55399999999045, 83.8279999999853, 27.310000000021727], 0.06: [37.924000000016655, 87.53499999997948, 89.63499999998919, 48.718999999997166], 0.08: [69.38399999998408, 76.37199999998988, 86.89799999998492, 19.475000000014678], 0.1: [58.85599999998814, 48.356999999996525, 88.35399999998184, 14.55199999999891], 0.12: [47.533999999996425, 66.0909999999815, 60.30199999998754, 7.63199999999674], 0.14: [50.84199999999169, 58.84199999998029, 35.796000000017486, -12.135999999997436], 0.16: [35.63100000001336, 65.0669999999846, 43.75300000000993, -29.29699999997112], 0.18: [15.292000000015904, 52.894999999983035, 10.705999999994024, -47.44099999999773], 0.2: [3.68599999999391, 36.16600000002009, 2.5309999999984534, -43.37899999998]}, 'action': {0.0: [98.01599999999635, 97.57399999999548, 92.26799999998974, 85.37999999998333], 0.02: [5.719999999994806, 80.25399999998783, 60.66799999998297, 84.1069999999875], 0.04: [9.759999999998149, 66.63199999997997, 63.596999999978294, 70.45599999998788], 0.06: [18.841000000016358, 23.058000000010814, -13.584999999997917, 18.10300000001049], 0.08: [23.346000000009607, 2.164999999999567, -8.843000000000231, -23.309999999975627], 0.1: [22.929000000011147, 22.336000000014355, -30.01899999997263, -20.04199999998272], 0.12: [28.72400000001588, 13.622000000005098, -27.802999999974755, -27.39799999997581], 0.14: [27.320000000013756, -12.092999999978723, -30.4159999999724, -39.776999999971], 0.16: [17.27100000000984, -17.729999999986145, -27.148999999979736, -44.55499999998678], 0.18: [2.579999999999727, -29.635999999979443, -35.82199999997346, -46.445999999990924], 0.2: [-22.13099999998295, -31.332999999973104, -35.719999999971876, -48.904000000002824]}}\n","5 fgsm (t): {'goal': {0.0: [98.01499999999632, 97.60199999999558, 93.7089999999893, 85.07499999998089], 0.02: [28.845000000011268, 97.45499999999552, 91.64799999998633, 75.18599999997636], 0.04: [44.61800000001236, 94.15599999999462, 93.40799999999048, 26.55500000001166], 0.06: [45.538000000002036, 89.3019999999811, 87.16999999998424, 39.73200000001918], 0.08: [57.06999999997237, 78.16799999999067, 87.12499999998445, 20.923000000017105], 0.1: [44.58400000001001, 43.408000000012336, 88.79999999997834, 14.396000000004628], 0.12: [46.358000000010335, 66.67299999998231, 67.17199999997128, -6.037000000005223], 0.14: [38.69200000001914, 65.131999999988, 45.06300000000963, -4.732000000006178], 0.16: [32.884000000020436, 64.49999999997614, 31.53800000001534, -33.1749999999701], 0.18: [2.1829999999999807, 56.58099999998245, 4.117999999995458, -39.990999999970505], 0.2: [-4.071000000004246, 35.10400000002046, 11.479999999994448, -44.6899999999859]}, 'action': {0.0: [98.01599999999635, 97.58799999999557, 91.48599999999183, 82.9609999999756], 0.02: [93.97799999999357, 90.37699999999403, 74.37099999997676, 62.96699999996557], 0.04: [65.67799999998644, 91.98899999998982, 67.85799999997614, 57.21399999998684], 0.06: [80.36099999998686, 84.69699999998707, 79.88699999998288, 63.536999999968494], 0.08: [67.33899999998445, 70.74999999997172, 76.5129999999776, 71.2589999999796], 0.1: [67.40299999997808, 74.51999999997682, 51.01899999999085, 54.80099999998672], 0.12: [56.91199999998366, 71.37999999997524, 54.83199999998621, 47.369999999989005], 0.14: [37.283000000013516, 66.08699999998078, 18.198000000009557, 19.50600000001585], 0.16: [29.69400000001788, 41.61700000001738, 6.955999999996449, -14.968000000002236], 0.18: [6.8509999999971605, 25.562000000014606, -31.096999999974624, -43.2159999999773], 0.2: [-12.825000000003277, 16.15300000000458, -23.881999999975818, -48.70200000000095]}}\n","6 fgsm (ut): {'goal': {0.0: [98.01999999999634, 97.57599999999556, 90.75399999998822, 83.65899999998014, 97.71399999999578], 0.02: [25.084000000020552, 97.4389999999953, 91.69899999998533, 78.11299999998722, 84.56499999998421], 0.04: [59.63099999998164, 94.55399999999045, 83.8279999999853, 27.310000000021727, 85.57299999998507], 0.06: [37.924000000016655, 87.53499999997948, 89.63499999998919, 48.718999999997166, 69.67599999998657], 0.08: [69.38399999998408, 76.37199999998988, 86.89799999998492, 19.475000000014678, 54.11199999998313], 0.1: [58.85599999998814, 48.356999999996525, 88.35399999998184, 14.55199999999891, 27.226000000015173], 0.12: [47.533999999996425, 66.0909999999815, 60.30199999998754, 7.63199999999674, -9.998000000005254], 0.14: [50.84199999999169, 58.84199999998029, 35.796000000017486, -12.135999999997436, -8.063000000006829], 0.16: [35.63100000001336, 65.0669999999846, 43.75300000000993, -29.29699999997112, -5.148000000002853], 0.18: [15.292000000015904, 52.894999999983035, 10.705999999994024, -47.44099999999773, -4.813000000000504], 0.2: [3.68599999999391, 36.16600000002009, 2.5309999999984534, -43.37899999998, -29.931999999974643]}, 'action': {0.0: [98.01599999999635, 97.57399999999548, 92.26799999998974, 85.37999999998333, 97.81499999999596], 0.02: [5.719999999994806, 80.25399999998783, 60.66799999998297, 84.1069999999875, 59.099999999980184], 0.04: [9.759999999998149, 66.63199999997997, 63.596999999978294, 70.45599999998788, 38.4690000000225], 0.06: [18.841000000016358, 23.058000000010814, -13.584999999997917, 18.10300000001049, 1.9609999999933294], 0.08: [23.346000000009607, 2.164999999999567, -8.843000000000231, -23.309999999975627, -14.47299999998967], 0.1: [22.929000000011147, 22.336000000014355, -30.01899999997263, -20.04199999998272, -27.21599999997842], 0.12: [28.72400000001588, 13.622000000005098, -27.802999999974755, -27.39799999997581, -16.681999999979272], 0.14: [27.320000000013756, -12.092999999978723, -30.4159999999724, -39.776999999971, -18.239999999982214], 0.16: [17.27100000000984, -17.729999999986145, -27.148999999979736, -44.55499999998678, -7.79900000000576], 0.18: [2.579999999999727, -29.635999999979443, -35.82199999997346, -46.445999999990924, -17.15699999998847], 0.2: [-22.13099999998295, -31.332999999973104, -35.719999999971876, -48.904000000002824, -29.402999999975233]}}\n","6 fgsm (t): {'goal': {0.0: [98.01499999999632, 97.60199999999558, 93.7089999999893, 85.07499999998089, 97.70899999999578], 0.02: [28.845000000011268, 97.45499999999552, 91.64799999998633, 75.18599999997636, 83.10899999998095], 0.04: [44.61800000001236, 94.15599999999462, 93.40799999999048, 26.55500000001166, 81.89699999998774], 0.06: [45.538000000002036, 89.3019999999811, 87.16999999998424, 39.73200000001918, 63.044999999990594], 0.08: [57.06999999997237, 78.16799999999067, 87.12499999998445, 20.923000000017105, 46.95299999999331], 0.1: [44.58400000001001, 43.408000000012336, 88.79999999997834, 14.396000000004628, 15.866000000001861], 0.12: [46.358000000010335, 66.67299999998231, 67.17199999997128, -6.037000000005223, -4.635000000004249], 0.14: [38.69200000001914, 65.131999999988, 45.06300000000963, -4.732000000006178, 5.429999999998825], 0.16: [32.884000000020436, 64.49999999997614, 31.53800000001534, -33.1749999999701, -10.387000000000558], 0.18: [2.1829999999999807, 56.58099999998245, 4.117999999995458, -39.990999999970505, -12.190999999997933], 0.2: [-4.071000000004246, 35.10400000002046, 11.479999999994448, -44.6899999999859, -19.509999999979453]}, 'action': {0.0: [98.01599999999635, 97.58799999999557, 91.48599999999183, 82.9609999999756, 97.7579999999957], 0.02: [93.97799999999357, 90.37699999999403, 74.37099999997676, 62.96699999996557, 90.0899999999946], 0.04: [65.67799999998644, 91.98899999998982, 67.85799999997614, 57.21399999998684, 65.42099999998986], 0.06: [80.36099999998686, 84.69699999998707, 79.88699999998288, 63.536999999968494, 73.57099999998134], 0.08: [67.33899999998445, 70.74999999997172, 76.5129999999776, 71.2589999999796, 67.69399999998569], 0.1: [67.40299999997808, 74.51999999997682, 51.01899999999085, 54.80099999998672, 58.715999999984824], 0.12: [56.91199999998366, 71.37999999997524, 54.83199999998621, 47.369999999989005, 49.608999999998304], 0.14: [37.283000000013516, 66.08699999998078, 18.198000000009557, 19.50600000001585, 31.395000000015457], 0.16: [29.69400000001788, 41.61700000001738, 6.955999999996449, -14.968000000002236, 3.035999999997957], 0.18: [6.8509999999971605, 25.562000000014606, -31.096999999974624, -43.2159999999773, -14.493999999984423], 0.2: [-12.825000000003277, 16.15300000000458, -23.881999999975818, -48.70200000000095, -8.887000000007552]}}\n","----\n","fgsm (ut): {'goal': {0.0: [98.01999999999634, 97.57599999999556, 90.75399999998822, 83.65899999998014, 97.71399999999578], 0.02: [25.084000000020552, 97.4389999999953, 91.69899999998533, 78.11299999998722, 84.56499999998421], 0.04: [59.63099999998164, 94.55399999999045, 83.8279999999853, 27.310000000021727, 85.57299999998507], 0.06: [37.924000000016655, 87.53499999997948, 89.63499999998919, 48.718999999997166, 69.67599999998657], 0.08: [69.38399999998408, 76.37199999998988, 86.89799999998492, 19.475000000014678, 54.11199999998313], 0.1: [58.85599999998814, 48.356999999996525, 88.35399999998184, 14.55199999999891, 27.226000000015173], 0.12: [47.533999999996425, 66.0909999999815, 60.30199999998754, 7.63199999999674, -9.998000000005254], 0.14: [50.84199999999169, 58.84199999998029, 35.796000000017486, -12.135999999997436, -8.063000000006829], 0.16: [35.63100000001336, 65.0669999999846, 43.75300000000993, -29.29699999997112, -5.148000000002853], 0.18: [15.292000000015904, 52.894999999983035, 10.705999999994024, -47.44099999999773, -4.813000000000504], 0.2: [3.68599999999391, 36.16600000002009, 2.5309999999984534, -43.37899999998, -29.931999999974643]}, 'action': {0.0: [98.01599999999635, 97.57399999999548, 92.26799999998974, 85.37999999998333, 97.81499999999596], 0.02: [5.719999999994806, 80.25399999998783, 60.66799999998297, 84.1069999999875, 59.099999999980184], 0.04: [9.759999999998149, 66.63199999997997, 63.596999999978294, 70.45599999998788, 38.4690000000225], 0.06: [18.841000000016358, 23.058000000010814, -13.584999999997917, 18.10300000001049, 1.9609999999933294], 0.08: [23.346000000009607, 2.164999999999567, -8.843000000000231, -23.309999999975627, -14.47299999998967], 0.1: [22.929000000011147, 22.336000000014355, -30.01899999997263, -20.04199999998272, -27.21599999997842], 0.12: [28.72400000001588, 13.622000000005098, -27.802999999974755, -27.39799999997581, -16.681999999979272], 0.14: [27.320000000013756, -12.092999999978723, -30.4159999999724, -39.776999999971, -18.239999999982214], 0.16: [17.27100000000984, -17.729999999986145, -27.148999999979736, -44.55499999998678, -7.79900000000576], 0.18: [2.579999999999727, -29.635999999979443, -35.82199999997346, -46.445999999990924, -17.15699999998847], 0.2: [-22.13099999998295, -31.332999999973104, -35.719999999971876, -48.904000000002824, -29.402999999975233]}}\n","fgsm (t): {'goal': {0.0: [98.01499999999632, 97.60199999999558, 93.7089999999893, 85.07499999998089, 97.70899999999578], 0.02: [28.845000000011268, 97.45499999999552, 91.64799999998633, 75.18599999997636, 83.10899999998095], 0.04: [44.61800000001236, 94.15599999999462, 93.40799999999048, 26.55500000001166, 81.89699999998774], 0.06: [45.538000000002036, 89.3019999999811, 87.16999999998424, 39.73200000001918, 63.044999999990594], 0.08: [57.06999999997237, 78.16799999999067, 87.12499999998445, 20.923000000017105, 46.95299999999331], 0.1: [44.58400000001001, 43.408000000012336, 88.79999999997834, 14.396000000004628, 15.866000000001861], 0.12: [46.358000000010335, 66.67299999998231, 67.17199999997128, -6.037000000005223, -4.635000000004249], 0.14: [38.69200000001914, 65.131999999988, 45.06300000000963, -4.732000000006178, 5.429999999998825], 0.16: [32.884000000020436, 64.49999999997614, 31.53800000001534, -33.1749999999701, -10.387000000000558], 0.18: [2.1829999999999807, 56.58099999998245, 4.117999999995458, -39.990999999970505, -12.190999999997933], 0.2: [-4.071000000004246, 35.10400000002046, 11.479999999994448, -44.6899999999859, -19.509999999979453]}, 'action': {0.0: [98.01599999999635, 97.58799999999557, 91.48599999999183, 82.9609999999756, 97.7579999999957], 0.02: [93.97799999999357, 90.37699999999403, 74.37099999997676, 62.96699999996557, 90.0899999999946], 0.04: [65.67799999998644, 91.98899999998982, 67.85799999997614, 57.21399999998684, 65.42099999998986], 0.06: [80.36099999998686, 84.69699999998707, 79.88699999998288, 63.536999999968494, 73.57099999998134], 0.08: [67.33899999998445, 70.74999999997172, 76.5129999999776, 71.2589999999796, 67.69399999998569], 0.1: [67.40299999997808, 74.51999999997682, 51.01899999999085, 54.80099999998672, 58.715999999984824], 0.12: [56.91199999998366, 71.37999999997524, 54.83199999998621, 47.369999999989005, 49.608999999998304], 0.14: [37.283000000013516, 66.08699999998078, 18.198000000009557, 19.50600000001585, 31.395000000015457], 0.16: [29.69400000001788, 41.61700000001738, 6.955999999996449, -14.968000000002236, 3.035999999997957], 0.18: [6.8509999999971605, 25.562000000014606, -31.096999999974624, -43.2159999999773, -14.493999999984423], 0.2: [-12.825000000003277, 16.15300000000458, -23.881999999975818, -48.70200000000095, -8.887000000007552]}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8pnki1eCngGQ"},"source":["def eval_scale(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for scale in np.arange(1.0,7.01,0.5):\n","        env = NormalizedEnv(PointMazeEnv(scale))\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(scale, 2)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmUPiWlPqUts","executionInfo":{"status":"ok","timestamp":1617194608542,"user_tz":-60,"elapsed":1282946,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"6e19e5ed-ff89-42c0-d11f-5d1c72f3d118"},"source":["episodes = {}\n","for scale in np.arange(1.0,7.01,0.5):\n","    episodes[np.round(scale, 2)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 1\n","while i < 7:\n","    if i == 2:\n","        i+=1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_scale(agent, episodes)\n","        print(f\"{i} scale: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"scale: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 scale: {1.0: [7.606999999992642], 1.5: [80.12899999998449], 2.0: [51.220999999988656], 2.5: [43.32700000001315], 3.0: [98.23599999999675], 3.5: [98.19699999999669], 4.0: [98.01899999999634], 4.5: [97.79099999999595], 5.0: [96.04999999999609], 5.5: [89.92099999999118], 6.0: [65.26399999998607], 6.5: [69.11999999997697], 7.0: [96.57799999999385]}\n","3 scale: {1.0: [7.606999999992642, -13.699999999994269], 1.5: [80.12899999998449, 50.88700000000336], 2.0: [51.220999999988656, 67.5399999999821], 2.5: [43.32700000001315, 90.5179999999891], 3.0: [98.23599999999675, 97.89199999999626], 3.5: [98.19699999999669, 97.65699999999562], 4.0: [98.01899999999634, 97.57799999999561], 4.5: [97.79099999999595, 97.47999999999546], 5.0: [96.04999999999609, 97.09099999999476], 5.5: [89.92099999999118, 96.92399999999446], 6.0: [65.26399999998607, 93.88399999999083], 6.5: [69.11999999997697, 89.22199999998774], 7.0: [96.57799999999385, 93.41499999999003]}\n","4 scale: {1.0: [7.606999999992642, -13.699999999994269, -36.18399999997008], 1.5: [80.12899999998449, 50.88700000000336, 70.62099999998789], 2.0: [51.220999999988656, 67.5399999999821, 81.4279999999882], 2.5: [43.32700000001315, 90.5179999999891, 80.50499999998765], 3.0: [98.23599999999675, 97.89199999999626, 83.8429999999888], 3.5: [98.19699999999669, 97.65699999999562, 82.80599999998793], 4.0: [98.01899999999634, 97.57799999999561, 88.51899999999102], 4.5: [97.79099999999595, 97.47999999999546, 88.30399999998664], 5.0: [96.04999999999609, 97.09099999999476, 92.42199999998782], 5.5: [89.92099999999118, 96.92399999999446, 93.91799999999097], 6.0: [65.26399999998607, 93.88399999999083, 88.56499999998732], 6.5: [69.11999999997697, 89.22199999998774, 88.71899999999141], 7.0: [96.57799999999385, 93.41499999999003, 88.71799999998414]}\n","5 scale: {1.0: [7.606999999992642, -13.699999999994269, -36.18399999997008, -27.967999999974413], 1.5: [80.12899999998449, 50.88700000000336, 70.62099999998789, 89.42699999998293], 2.0: [51.220999999988656, 67.5399999999821, 81.4279999999882, 89.66999999998885], 2.5: [43.32700000001315, 90.5179999999891, 80.50499999998765, 69.51799999998558], 3.0: [98.23599999999675, 97.89199999999626, 83.8429999999888, 93.20799999999045], 3.5: [98.19699999999669, 97.65699999999562, 82.80599999998793, 92.8689999999898], 4.0: [98.01899999999634, 97.57799999999561, 88.51899999999102, 84.66399999998008], 4.5: [97.79099999999595, 97.47999999999546, 88.30399999998664, 57.95899999997904], 5.0: [96.04999999999609, 97.09099999999476, 92.42199999998782, 17.826000000021264], 5.5: [89.92099999999118, 96.92399999999446, 93.91799999999097, 13.423000000003674], 6.0: [65.26399999998607, 93.88399999999083, 88.56499999998732, 37.504000000028405], 6.5: [69.11999999997697, 89.22199999998774, 88.71899999999141, 36.897000000022125], 7.0: [96.57799999999385, 93.41499999999003, 88.71799999998414, 44.57100000001501]}\n","6 scale: {1.0: [7.606999999992642, -13.699999999994269, -36.18399999997008, -27.967999999974413, -32.1569999999725], 1.5: [80.12899999998449, 50.88700000000336, 70.62099999998789, 89.42699999998293, 67.12199999999216], 2.0: [51.220999999988656, 67.5399999999821, 81.4279999999882, 89.66999999998885, 71.43999999998256], 2.5: [43.32700000001315, 90.5179999999891, 80.50499999998765, 69.51799999998558, 75.31199999999748], 3.0: [98.23599999999675, 97.89199999999626, 83.8429999999888, 93.20799999999045, 98.2459999999966], 3.5: [98.19699999999669, 97.65699999999562, 82.80599999998793, 92.8689999999898, 98.2139999999967], 4.0: [98.01899999999634, 97.57799999999561, 88.51899999999102, 84.66399999998008, 97.71799999999567], 4.5: [97.79099999999595, 97.47999999999546, 88.30399999998664, 57.95899999997904, 97.65899999999544], 5.0: [96.04999999999609, 97.09099999999476, 92.42199999998782, 17.826000000021264, 95.38699999999022], 5.5: [89.92099999999118, 96.92399999999446, 93.91799999999097, 13.423000000003674, 61.83299999996922], 6.0: [65.26399999998607, 93.88399999999083, 88.56499999998732, 37.504000000028405, 62.63899999996806], 6.5: [69.11999999997697, 89.22199999998774, 88.71899999999141, 36.897000000022125, 67.0879999999688], 7.0: [96.57799999999385, 93.41499999999003, 88.71799999998414, 44.57100000001501, 69.53799999996559]}\n","----\n","scale: {1.0: [7.606999999992642, -13.699999999994269, -36.18399999997008, -27.967999999974413, -32.1569999999725], 1.5: [80.12899999998449, 50.88700000000336, 70.62099999998789, 89.42699999998293, 67.12199999999216], 2.0: [51.220999999988656, 67.5399999999821, 81.4279999999882, 89.66999999998885, 71.43999999998256], 2.5: [43.32700000001315, 90.5179999999891, 80.50499999998765, 69.51799999998558, 75.31199999999748], 3.0: [98.23599999999675, 97.89199999999626, 83.8429999999888, 93.20799999999045, 98.2459999999966], 3.5: [98.19699999999669, 97.65699999999562, 82.80599999998793, 92.8689999999898, 98.2139999999967], 4.0: [98.01899999999634, 97.57799999999561, 88.51899999999102, 84.66399999998008, 97.71799999999567], 4.5: [97.79099999999595, 97.47999999999546, 88.30399999998664, 57.95899999997904, 97.65899999999544], 5.0: [96.04999999999609, 97.09099999999476, 92.42199999998782, 17.826000000021264, 95.38699999999022], 5.5: [89.92099999999118, 96.92399999999446, 93.91799999999097, 13.423000000003674, 61.83299999996922], 6.0: [65.26399999998607, 93.88399999999083, 88.56499999998732, 37.504000000028405, 62.63899999996806], 6.5: [69.11999999997697, 89.22199999998774, 88.71899999999141, 36.897000000022125, 67.0879999999688], 7.0: [96.57799999999385, 93.41499999999003, 88.71799999998414, 44.57100000001501, 69.53799999996559]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gd2_86AIqOt4"},"source":["def eval_starting_position(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 100\n","\n","    c = 10\n","\n","    for extra_range in np.arange(0.0, 0.401, 0.05):\n","        \n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            extra = np.random.uniform(-0.1 - extra_range, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = np.random.uniform(0.1, 0.1 + extra_range, env.starting_point.shape)\n","            #extra = extra * (2*np.random.randint(0,2,size=env.starting_point.shape)-1)\n","            env.unwrapped.state = np.array(env.starting_point + extra, dtype=np.float32)\n","            env.unwrapped.state[2] = env.state[2] % (2 * math.pi)\n","            observation = env.normalised_state()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal_raw = agent.select_goal(g_state, False, False)\n","                goal = agent.convert_goal(goal_raw)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    action = agent.select_action(state, goal, False, False)\n","                    observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                    \n","                    next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    next_goal = agent.h(g_state, goal, g_next_state)\n","                                      \n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    #goal_done = agent.goal_reached(action, goal)\n","                    goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                    if (episode_steps % c) == 0:\n","                        goal_done = True\n","\n","                    state = next_state\n","                    g_state = g_next_state\n","                    goal = next_goal\n","\n","        episode_durations[np.round(extra_range, 3)].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJjpZcCLqjua","executionInfo":{"status":"ok","timestamp":1617195111777,"user_tz":-60,"elapsed":1782506,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"00c98f82-6629-49e5-998b-d78cea79a4fa"},"source":["episodes = {}\n","for extra_range in np.arange(0.0, 0.401, 0.05):\n","    episodes[np.round(extra_range, 3)] = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","env = NormalizedEnv(PointMazeEnv(4))\n","i = 1\n","while i < 7:\n","    if i == 2:\n","        i+=1\n","        continue\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_starting_position(agent, episodes)\n","        print(f\"{i} range: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","print(f\"range: {episodes}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 range: {0.0: [98.0059999999963], 0.05: [97.98499999999629], 0.1: [97.94499999999618], 0.15: [97.93399999999617], 0.2: [97.90699999999615], 0.25: [97.90499999999611], 0.3: [96.3489999999942], 0.35: [93.37399999999735], 0.4: [94.38999999999322]}\n","3 range: {0.0: [98.0059999999963, 97.61499999999562], 0.05: [97.98499999999629, 95.60899999999602], 0.1: [97.94499999999618, 93.15399999999745], 0.15: [97.93399999999617, 82.79599999998416], 0.2: [97.90699999999615, 84.40099999998303], 0.25: [97.90499999999611, 72.65699999998233], 0.3: [96.3489999999942, 74.09499999998931], 0.35: [93.37399999999735, 72.22199999998814], 0.4: [94.38999999999322, 64.57699999999238]}\n","4 range: {0.0: [98.0059999999963, 97.61499999999562, 91.41899999998778], 0.05: [97.98499999999629, 95.60899999999602, 92.63699999998882], 0.1: [97.94499999999618, 93.15399999999745, 86.76299999998704], 0.15: [97.93399999999617, 82.79599999998416, 87.06199999998033], 0.2: [97.90699999999615, 84.40099999998303, 87.1499999999927], 0.25: [97.90499999999611, 72.65699999998233, 85.20699999998251], 0.3: [96.3489999999942, 74.09499999998931, 80.01099999997989], 0.35: [93.37399999999735, 72.22199999998814, 70.85299999997633], 0.4: [94.38999999999322, 64.57699999999238, 77.39399999998867]}\n","5 range: {0.0: [98.0059999999963, 97.61499999999562, 91.41899999998778, 82.72999999996995], 0.05: [97.98499999999629, 95.60899999999602, 92.63699999998882, 81.54799999997238], 0.1: [97.94499999999618, 93.15399999999745, 86.76299999998704, 86.82799999997803], 0.15: [97.93399999999617, 82.79599999998416, 87.06199999998033, 83.31099999998291], 0.2: [97.90699999999615, 84.40099999998303, 87.1499999999927, 83.70599999997653], 0.25: [97.90499999999611, 72.65699999998233, 85.20699999998251, 83.5659999999786], 0.3: [96.3489999999942, 74.09499999998931, 80.01099999997989, 82.08799999998415], 0.35: [93.37399999999735, 72.22199999998814, 70.85299999997633, 81.89199999997619], 0.4: [94.38999999999322, 64.57699999999238, 77.39399999998867, 81.28499999997715]}\n","6 range: {0.0: [98.0059999999963, 97.61499999999562, 91.41899999998778, 82.72999999996995, 97.72399999999593], 0.05: [97.98499999999629, 95.60899999999602, 92.63699999998882, 81.54799999997238, 96.26699999999396], 0.1: [97.94499999999618, 93.15399999999745, 86.76299999998704, 86.82799999997803, 94.37799999999224], 0.15: [97.93399999999617, 82.79599999998416, 87.06199999998033, 83.31099999998291, 88.45599999998797], 0.2: [97.90699999999615, 84.40099999998303, 87.1499999999927, 83.70599999997653, 80.8919999999944], 0.25: [97.90499999999611, 72.65699999998233, 85.20699999998251, 83.5659999999786, 83.7829999999825], 0.3: [96.3489999999942, 74.09499999998931, 80.01099999997989, 82.08799999998415, 76.72399999998296], 0.35: [93.37399999999735, 72.22199999998814, 70.85299999997633, 81.89199999997619, 74.39899999998474], 0.4: [94.38999999999322, 64.57699999999238, 77.39399999998867, 81.28499999997715, 66.95299999998362]}\n","----\n","range: {0.0: [98.0059999999963, 97.61499999999562, 91.41899999998778, 82.72999999996995, 97.72399999999593], 0.05: [97.98499999999629, 95.60899999999602, 92.63699999998882, 81.54799999997238, 96.26699999999396], 0.1: [97.94499999999618, 93.15399999999745, 86.76299999998704, 86.82799999997803, 94.37799999999224], 0.15: [97.93399999999617, 82.79599999998416, 87.06199999998033, 83.31099999998291, 88.45599999998797], 0.2: [97.90699999999615, 84.40099999998303, 87.1499999999927, 83.70599999997653, 80.8919999999944], 0.25: [97.90499999999611, 72.65699999998233, 85.20699999998251, 83.5659999999786, 83.7829999999825], 0.3: [96.3489999999942, 74.09499999998931, 80.01099999997989, 82.08799999998415, 76.72399999998296], 0.35: [93.37399999999735, 72.22199999998814, 70.85299999997633, 81.89199999997619, 74.39899999998474], 0.4: [94.38999999999322, 64.57699999999238, 77.39399999998867, 81.28499999997715, 66.95299999998362]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BhvsIWF-qrHj"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device).float()\n","state_min = torch.from_numpy(env.observation_space.low).to(device).float()\n","state_mid = (state_max + state_min) / 2.\n","state_range = (state_max - state_min)\n","def save_trajectories(agent, episode_durations, dirty):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    for i_episode in range(num_episodes):\n","        path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","\n","        if dirty:\n","            g_state = g_state + state_range * noise\n","            g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","        if dirty:\n","            state = state + state_range * torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","            state = torch.max(torch.min(state, state_max), state_min).float()\n","\n","        episode_steps = 0\n","        overall_reward = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","            path[\"manager\"].append((episode_steps, g_state_.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                path[\"worker\"].append((episode_steps, torch.cat([state_, goal], 1).detach().cpu().squeeze(0).numpy(), action.detach().cpu().squeeze(0).numpy()))\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_state_ = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                noise = torch.FloatTensor(state.shape).uniform_(-l2norm, l2norm).to(device)\n","                if dirty:\n","                    g_next_state = g_next_state + state_range * noise\n","                    g_next_state = torch.max(torch.min(g_next_state, state_max), state_min).float()\n","                if dirty:\n","                    next_state = next_state + state_range * torch.FloatTensor(next_state.shape).uniform_(-l2norm, l2norm).to(device)\n","                    next_state = torch.max(torch.min(next_state, state_max), state_min).float()\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","\n","        path[\"overall_reward\"] = overall_reward\n","        episode_durations[-1].append(path)\n","\n","def save_random_manager(agent, episode_durations):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_points = 10000\n","\n","    c = 10\n","\n","    l2norm = 0.3\n","    episode_durations.append([])\n","    \n","    path = {\"overall_reward\": 0, \"manager\": [], \"worker\": []}\n","\n","    for _ in range(num_points):\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        noise = torch.FloatTensor(state.shape).uniform_(0.0, 1.0).to(device)\n","\n","        g_state = state_min + state_range * noise\n","        g_state = torch.max(torch.min(g_state, state_max), state_min).float()\n","\n","        goal = agent.select_goal(g_state, False, False)\n","        path[\"manager\"].append((0, g_state.detach().cpu().squeeze(0).numpy(), goal.detach().cpu().squeeze(0).numpy()))\n","\n","    episode_durations[-1].append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWeLBDKTP3Ao","executionInfo":{"status":"ok","timestamp":1616493270550,"user_tz":0,"elapsed":38321,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"0107b42c-f0dc-4328-af79-819acc958d8a"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 0\n","while i < 7:\n","    #agent = train_model()\n","    agent = HIRO(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hiro_freeze_{i}\")\n","\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        #save_trajectories(agent, episodes, False)\n","        save_random_manager(agent, episodes)\n","        #print(f\"{i} paths: {episodes}\")\n","        i += 1\n","\n","print(\"----\")\n","#print(f\"paths: {episodes}\")\n","\n","episodes.pop(1)\n","torch.save(episodes, \"PointMaze_Freeze_manager.pt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y6VhJjD8QHJl"},"source":["def get_intrinsic_reward(agent):\n","    agent.eval()\n","    agent.meta_controller.eval()\n","    agent.controller.eval()\n","\n","    max_episode_length = 500\n","    agent.meta_controller.is_training = False\n","    agent.controller.is_training = False\n","\n","    num_episodes = 10\n","\n","    c = 10\n","\n","    overall_reward = 0\n","    intr_rews = []\n","\n","    for i_episode in range(num_episodes):\n","        cur_intr = []\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        episode_steps = 0\n","        done = False\n","        while not done:\n","            # select a goal\n","            goal = agent.select_goal(g_state, False, False)\n","\n","            goal_done = False\n","            while not done and not goal_done:\n","                action = agent.select_action(state, goal, False, False)\n","                observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","                \n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                g_next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                next_goal = agent.h(g_state, goal, g_next_state)\n","                                  \n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                #goal_done = agent.goal_reached(action, goal)\n","                cur_intr.append(agent.intrinsic_reward(reward, state, goal, next_state).detach().item())\n","                goal_reached = agent.goal_reached(g_state, goal, g_next_state)\n","\n","                if (episode_steps % c) == 0:\n","                    goal_done = True\n","\n","                state = next_state\n","                g_state = g_next_state\n","                goal = next_goal\n","        intr_rews.append(cur_intr)\n","    print(overall_reward / num_episodes)\n","    return intr_rews"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"SBjnn7HWZTyF","executionInfo":{"status":"ok","timestamp":1616492202635,"user_tz":0,"elapsed":868,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"8fdc8c67-b902-442f-b4f1-c33f9b6077ad"},"source":["import matplotlib.pyplot as plt\n","\n","episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 6\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_freeze_{i}\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Freeze\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[3])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["97.31999999999961\n","Freeze\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"11oJHE1hZknp","executionInfo":{"status":"ok","timestamp":1616492002184,"user_tz":0,"elapsed":1263,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FzBpJhrUOXgzWW4qa9Nf-REhQJ53qmzZVKDvGOA=s64","userId":"08578009112219899929"}},"outputId":"b31e2135-12e2-4224-8ae0-c3f0be8a00d7"},"source":["episodes = []\n","\n","n_observations = env.observation_space.shape[0]\n","n_actions = env.action_space.shape[0]\n","\n","i = 3\n","agent = HIRO(n_observations, n_actions).to(device)\n","load_model(agent, f\"hiro_{i}\", \"point_maze_time\")\n","episodes = get_intrinsic_reward(agent)\n","\n","print(\"Standard\")\n","\n","eps = np.array([np.array(l) for l in episodes])\n","#eps = np.mean(eps, 0)\n","\n","plt.plot(eps[2])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["95.64999999999947\n","Standard\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"cpUcYkH1afDr"},"source":[""],"execution_count":null,"outputs":[]}]}