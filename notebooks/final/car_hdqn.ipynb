{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"HDQN_car.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BOa4bDnuU8w7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608453801465,"user_tz":-60,"elapsed":30253,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}},"outputId":"cbc0df4a-f2e3-45c2-f4d0-9e84c4facfe1"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#!cp \"/content/drive/My Drive/Dissertation/preprocessing.py\" .\n","#!cp -r \"/content/drive/My Drive/Dissertation/gym_maze\" .\n","#!cp -r \"/content/drive/My Drive/Dissertation/envs\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UjRnm66x6Sga","executionInfo":{"status":"ok","timestamp":1608453804282,"user_tz":-60,"elapsed":1616,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# for inference, not continued training\r\n","def save_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{name}\" \r\n","\r\n","    torch.save({\r\n","      'meta_controller': model.meta_controller.state_dict(),\r\n","      'controller': model.controller.state_dict()\r\n","    }, path)\r\n","\r\n","import copy\r\n","def load_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{name}\" \r\n","    checkpoint = torch.load(path)\r\n","\r\n","    model.meta_controller.load_state_dict(checkpoint['meta_controller'])\r\n","    model.meta_controller_target = copy.deepcopy(model.meta_controller)\r\n","    model.controller.load_state_dict(checkpoint['controller'])\r\n","    model.controller_target = copy.deepcopy(model.controller)\r\n","\r\n","    model.eval()\r\n","    model.meta_controller.eval()\r\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_nd1HgGU8w7","executionInfo":{"status":"ok","timestamp":1608453810029,"user_tz":-60,"elapsed":5931,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","import cv2\n","from PIL import Image\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nwe9sNmHU8w7","executionInfo":{"status":"ok","timestamp":1608453810036,"user_tz":-60,"elapsed":1535,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["#from cartpole import CartPoleEnv \n","#env = CartPoleEnv()\n","env = gym.make(\"MountainCar-v0\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vG98TmEU8w8"},"source":["---\n","### Helper functions"]},{"cell_type":"code","metadata":{"id":"KdafwykgU8w8","executionInfo":{"status":"ok","timestamp":1608453812739,"user_tz":-60,"elapsed":2195,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def plot_durations(episode_durations, actions_taken):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    axs[1].set_xlabel('Episode')\n","    axs[1].set_ylabel('Actions Taken')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    durations_t, durations = list(map(list, zip(*actions_taken)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","\n","    axs[1].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qx6CukwVU8w8"},"source":["---\n","### Code"]},{"cell_type":"code","metadata":{"id":"fisoWfOnU8w8","executionInfo":{"status":"ok","timestamp":1608453813358,"user_tz":-60,"elapsed":844,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZ9BRCoIU8w8","executionInfo":{"status":"ok","timestamp":1608453815772,"user_tz":-60,"elapsed":2569,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["BATCH_SIZE = 16\n","GAMMA = 0.95\n","\n","def one_hot(n, v):\n","    a = np.zeros(n)\n","    a[v] = 1.0\n","    return np.expand_dims(a, axis=0)\n","\n","def rev_one_hot(a):\n","    return np.where(a[0] > 0)[0][0]\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 2000000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, 256)\n","        self.fc2 = nn.Linear(256, 256)\n","        self.head = nn.Linear(256, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.0\n","        self.EPS_DECAY = 100000 # in number of steps\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, is_training=True):\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        \n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","        \n","class HDQN(nn.Module):\n","    def __init__(self, inputs, outputs):\n","        super(HDQN, self).__init__()\n","        # Optimizer\n","        learning_rate = 2.5e-4\n","        \n","        # goal is left/right\n","        self.meta_controller = DQN(inputs, outputs, mem_len = 2000000).to(device)\n","        self.meta_controller_optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n","        self.meta_controller_target = DQN(inputs, outputs, mem_len = 0).to(device)\n","        self.meta_controller_target.eval()\n","        \n","        # takes goal+state jointly\n","        self.controller = DQN(inputs + outputs, outputs, mem_len = 2000000).to(device)\n","        self.controller_optimizer = optim.RMSprop(self.controller.parameters(), lr=learning_rate)\n","        self.controller_target = DQN(inputs + outputs, outputs, mem_len = 0).to(device)\n","        self.controller_target.eval()\n","    \n","    def store_controller(self, *args):\n","        self.controller.memory.store(*args)\n","    \n","    def store_meta_controller(self, *args):\n","        self.meta_controller.memory.store(*args)\n","    \n","    def select_goal(self, external_observation, is_training=True):\n","        return self.meta_controller.act(external_observation, is_training)\n","        \n","    def select_action(self, joint_goal_obs, is_training=True):\n","        return self.controller.act(joint_goal_obs, is_training)\n","    \n","    def experience_replay(self):\n","        self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n","        self.controller.experience_replay(self.controller_optimizer, self.controller_target)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgHmamR6U8w8","executionInfo":{"status":"ok","timestamp":1608453816266,"user_tz":-60,"elapsed":1808,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def plot_norms(episode_durations):\n","    plt.figure(2, figsize=(10,10))\n","    \n","    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n","    \n","    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n","    plt.xlabel('L2 Norm')\n","    plt.ylabel('Average Reward')\n","    \n","    mu = np.mean(ys, axis=1)\n","    plt.plot(x / 10, mu)\n","    stds = np.std(ys, axis = 1)\n","    plt.fill_between(x / 10, mu + stds , mu - stds, alpha=0.2)\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nr9-j_IbU8w8","executionInfo":{"status":"ok","timestamp":1608453816269,"user_tz":-60,"elapsed":967,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["import time\n","SAVE_OFFSET = 1\n","\n","def train_model():\n","    global SAVE_OFFSET\n","    # Get number of actions and observations from gym action space\n","    n_actions = env.action_space.n\n","    n_observations = env.observation_space.shape[0]\n","\n","    # Initialize action-value function Q with random weights\n","    hdqnAgent = HDQN(n_observations, n_actions).to(device)\n","\n","    max_episode_length = 200\n","\n","    num_episodes = 5000 # M\n","    episode_durations = []\n","    actions_taken = [(0, 0)]\n","\n","    steps = 0\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        overall_reward = 0\n","        ep_state_visits = {0: 0, 1: 0, 2: 0, 3: 0}\n","        done = False\n","        episode_steps = 0\n","        while not done:\n","            # select a goal\n","            goal = hdqnAgent.select_goal(state)\n","            goal_i = goal.item()\n","            encoded_goal = torch.from_numpy(one_hot(n_actions, goal)).float().to(device)\n","\n","            goal_done = False\n","            total_extrinsic = 0\n","            s_0 = state\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([encoded_goal, state], axis=1)\n","\n","                # Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n","                action = hdqnAgent.select_action(joint_goal_state)\n","                action_i = action.item()\n","\n","                #actions_taken.append((steps, action_i))\n","\n","                observation, reward, done, _ = env.step(action_i)\n","                steps += 1\n","                extrinsic_reward = torch.tensor([reward], device=device)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","\n","                # preprocess φ_{t+1} = φ(s_{t+1})\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                joint_next_state = torch.cat([encoded_goal, next_state], axis=1)\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                goal_done = (goal_i == action_i)\n","\n","                intrinsic_reward = torch.tensor([1.0 if goal_done else 0.0], device=device)\n","\n","                # Store transition (φt, at, rt, φt+1) in D\n","                hdqnAgent.store_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done)\n","\n","                state = next_state\n","\n","                hdqnAgent.experience_replay()\n","\n","            # Store transition for meta controller\n","            hdqnAgent.store_meta_controller(s_0, goal, next_state, torch.tensor([total_extrinsic], device=device), done)\n","            hdqnAgent.experience_replay()\n","\n","        if i_episode % 20 == 0:\n","            hdqnAgent.meta_controller_target.load_state_dict(hdqnAgent.meta_controller.state_dict())\n","            hdqnAgent.controller_target.load_state_dict(hdqnAgent.controller.state_dict())\n","\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations, actions_taken)\n","\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if np.mean(dur[-100:]) >= -110:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(hdqnAgent, f\"hdqn_car_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return hdqnAgent\n","\n","    return None # did not train"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6BqeP-7w7MN","executionInfo":{"status":"ok","timestamp":1608453822450,"user_tz":-60,"elapsed":1564,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["#agent = train_model()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"55noGZWJ7rFU","executionInfo":{"status":"ok","timestamp":1608453836648,"user_tz":-60,"elapsed":11541,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\r\n","def fgsm_attack(data, eps, data_grad):\r\n","    sign_data_grad = data_grad.sign()\r\n","\r\n","    perturbed_data = data + eps * sign_data_grad * state_max\r\n","\r\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), -state_max)\r\n","\r\n","    return clipped_perturbed_data\r\n","\r\n","def fgsm_goal(g_state, agent, eps, target, targetted):\r\n","    #g_state = torch.tensor(g_state, requires_grad=True)\r\n","\r\n","    g_state_var = g_state.clone().detach().requires_grad_(True)\r\n","\r\n","    # initial forward pass\r\n","    goal = agent.meta_controller(g_state_var)\r\n","    #goal = temp.max(1)[1].view(1, 1)\r\n","\r\n","    if targetted:\r\n","        loss = F.smooth_l1_loss(goal, target)\r\n","    else:\r\n","        pass\r\n","        #loss = F.smooth_l1_loss(goal, temp.min(1)[1].view(1, 1).float())\r\n","\r\n","    agent.meta_controller.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = g_state_var.grad.data\r\n","\r\n","    # perturb state\r\n","    g_state_p = fgsm_attack(g_state, eps, data_grad)\r\n","    return agent.select_goal(g_state_p, False)\r\n","\r\n","def fgsm_action(state, goal, agent, eps, target, targetted):\r\n","    #state = torch.tensor(state, requires_grad=True)\r\n","    state_var = state.clone().detach().requires_grad_(True)\r\n","\r\n","    joint_goal_state = torch.cat([goal, state_var], 1).float()\r\n","    \r\n","    # initial forward pass\r\n","    action = agent.controller(joint_goal_state)\r\n","    #action = temp.max(1)[1].view(1, 1).float()\r\n","\r\n","    if targetted:\r\n","        loss = F.smooth_l1_loss(action, target)\r\n","    else:\r\n","        pass\r\n","        #loss = F.smooth_l1_loss(action, temp.min(1)[1].view(1, 1).float())\r\n","\r\n","    agent.controller.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = state_var.grad.data\r\n","    # perturb state\r\n","    state_p = fgsm_attack(state, eps, data_grad)\r\n","\r\n","    joint_goal_state = torch.cat([goal, state_p], 1).float()\r\n","    return agent.select_action(joint_goal_state, False)\r\n","\r\n","def apply_fgsm(agent, episode_durations, goal_attack, action_attack, targetted):\r\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\r\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0]], device=device, dtype=torch.float)\r\n","\r\n","    agent.eval()\r\n","    agent.meta_controller.eval()\r\n","    agent.controller.eval()\r\n","\r\n","    max_episode_length = 200\r\n","    agent.meta_controller.is_training = False\r\n","    agent.controller.is_training = False\r\n","\r\n","    num_episodes = 100\r\n","\r\n","    for eps in np.arange(0.0, 0.031, 0.0025):\r\n","\r\n","        overall_reward = 0\r\n","        for i_episode in range(num_episodes):\r\n","            observation = env.reset()\r\n","\r\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","                \r\n","            episode_steps = 0\r\n","            done = False\r\n","            while not done:\r\n","                # select a goal\r\n","                if goal_attack:\r\n","                    goal = fgsm_goal(state, agent, eps, TARGET_GOAL, targetted)\r\n","                else:\r\n","                    goal = agent.select_goal(state, False)\r\n","                goal_i = goal.item()\r\n","                encoded_goal = torch.from_numpy(one_hot(n_actions, goal)).float().to(device)\r\n","\r\n","                goal_done = False\r\n","                while not done and not goal_done:\r\n","                    joint_goal_state = torch.cat([encoded_goal, state], axis=1)\r\n","\r\n","                    if action_attack:\r\n","                        action = fgsm_action(state, encoded_goal, agent, eps, TARGET_ACTION, targetted)\r\n","                    else:\r\n","                        action = agent.select_action(joint_goal_state, False)\r\n","\r\n","                    action_i = action.item()\r\n","                    observation, reward, done, _ = env.step(action_i)\r\n","\r\n","                    overall_reward += reward\r\n","\r\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\r\n","                        done = True\r\n","                    episode_steps += 1\r\n","\r\n","                    goal_done = (goal_i == action_i)\r\n","\r\n","                    state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktebJKBtU8w8","executionInfo":{"status":"ok","timestamp":1608453836659,"user_tz":-60,"elapsed":10010,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\n","def eval_model(hdqnAgent, episode_durations, goal_noise, action_noise, same_noise):\n","    hdqnAgent.eval()\n","    hdqnAgent.meta_controller.eval()\n","    hdqnAgent.controller.eval()\n","\n","    max_episode_length = 200\n","    num_episodes = 100\n","\n","    for l2norm in np.arange(0,0.31,0.03):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","            if goal_noise:\n","                g_state = state + state_max * noise\n","                g_state = g_state.float()\n","            if action_noise:\n","                if same_noise:\n","                    state = state + state_max * noise\n","                else:\n","                    state = state + state_max * torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                state = state.float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = hdqnAgent.select_goal(g_state, False)\n","                goal_i = goal.item()\n","                encoded_goal = torch.from_numpy(one_hot(n_actions, goal)).float().to(device)\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    joint_goal_state = torch.cat([encoded_goal, state], axis=1)\n","\n","                    action = hdqnAgent.select_action(joint_goal_state, False)\n","                    action_i = action.item()\n","                    observation, reward, done, _ = env.step(action_i)\n","\n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    goal_done = (goal_i == action_i)\n","\n","                    state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                    if goal_noise:\n","                        g_state = state + state_max * noise\n","                        g_state = g_state.float()\n","                    if action_noise:\n","                        if same_noise:\n","                            state = state + state_max * noise\n","                        else:\n","                            state = state + state_max * torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                        state = state.float()\n","\n","        episode_durations[l2norm].append(overall_reward / num_episodes)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJKWKLPjAipR","executionInfo":{"status":"ok","timestamp":1608453836662,"user_tz":-60,"elapsed":8692,"user":{"displayName":"David Khachaturov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjv6TQULJqrRmGx4orsjnj2Qw8l6izQ6vq8Xhw1=s64","userId":"00615802394785083853"}}},"source":["def plot_fgsm(episode_durations):\r\n","    plt.figure(2, figsize=(10,10))\r\n","    \r\n","    for kk in ['both', 'goal_only', 'action_only']:\r\n","        x, ys = np.array(list(episode_durations[kk].keys())), np.array(list(episode_durations[kk].values()))\r\n","        #plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\r\n","        plt.xlabel('$\\epsilon$')\r\n","        plt.ylabel('Average Reward')\r\n","        \r\n","        mu = np.mean(ys, axis=1)\r\n","        plt.plot(x, mu, label=kk)\r\n","        stds = np.std(ys, axis = 1)\r\n","        plt.fill_between(x, mu + stds , mu - stds, alpha=0.2)\r\n","    \r\n","    plt.legend()\r\n","    plt.pause(0.001)  # pause a bit so that plots are updated\r\n","    display.clear_output(wait=True)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpWwUiRkAjnM"},"source":["targeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\r\n","untargeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\r\n","for eps in np.arange(0.0, 0.031, 0.0025):\r\n","    for x in ['both', 'goal_only', 'action_only']:\r\n","        targeted[x][eps] = []\r\n","        untargeted[x][eps] = []\r\n","\r\n","n_actions = env.action_space.n\r\n","n_observations = env.observation_space.shape[0]\r\n","\r\n","i = 1\r\n","while i < 25:\r\n","    #agent = train_model()\r\n","    agent = HDQN(n_observations, n_actions).to(device)\r\n","    load_model(agent, f\"hdqn_car_{i}\")\r\n","    if agent is not None:\r\n","        apply_fgsm(agent, targeted['both'], True, True, True)\r\n","        apply_fgsm(agent, targeted['goal_only'], True, False, True)\r\n","        apply_fgsm(agent, targeted['action_only'], False, True, True)\r\n","        #apply_fgsm(agent, untargeted['both'], True, True, False)\r\n","        #apply_fgsm(agent, untargeted['goal_only'], True, False, False)\r\n","        #apply_fgsm(agent, untargeted['action_only'], False, True, False)\r\n","        print(i)\r\n","        print(f\"Targeted: {targeted}\")\r\n","        print(f\"Untargeted: {untargeted}\")\r\n","        #plot_fgsm(episode_durations)\r\n","        i += 1\r\n","\r\n","#plot_fgsm(episode_durations)\r\n","print(f\"Targeted: {targeted}\")\r\n","print(f\"Untargeted: {untargeted}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBfZ2tc_U8w8"},"source":["same_noise = {}\n","diff_noise = {}\n","goal_only = {}\n","action_only = {}\n","for l2norm in np.arange(0,0.31,0.03):\n","    for i in [same_noise, diff_noise, goal_only, action_only]:\n","        i[l2norm] = []\n","\n","# train 20 models, then eval them\n","i = 1\n","while i < 20:\n","    agent = train_model()\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, same_noise, True, True, True)\n","        eval_model(agent, diff_noise, True, True, False)\n","        eval_model(agent, goal_only, True, False, False)\n","        eval_model(agent, action_only, False, True, False)\n","        print(i)\n","        print(f\"same noise: {same_noise}\")\n","        print(f\"diff noise: {diff_noise}\")\n","        print(f\"goal only: {goal_only}\")\n","        print(f\"action only: {action_only}\")\n","        i += 1\n","\n","print(f\"same noise: {same_noise}\")\n","print(f\"diff noise: {diff_noise}\")\n","print(f\"goal only: {goal_only}\")\n","print(f\"action only: {action_only}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUQHmv4-U8w9"},"source":[""],"execution_count":null,"outputs":[]}]}