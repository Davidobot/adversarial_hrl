{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"td3_pendulum.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sbszcg0jEtQ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605712873240,"user_tz":0,"elapsed":5640,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"422e9a6a-1760-426e-8b2d-fc170b553047"},"source":["#!pip install torch==1.4.0 torchvision==0.5.0\n","!pip install gym\n","!pip install gym[atari]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n","Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n","Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n","Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CJMjXntuErvs","executionInfo":{"status":"ok","timestamp":1605895960594,"user_tz":0,"elapsed":5074,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from continuous_cartpole import ContinuousCartPoleEnv \n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0","executionInfo":{"status":"ok","timestamp":1605895968947,"user_tz":0,"elapsed":383,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["from continuous_cartpole import ContinuousCartPoleEnv \n","env = NormalizedEnv(ContinuousCartPoleEnv(False))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs","executionInfo":{"status":"ok","timestamp":1605895968438,"user_tz":0,"elapsed":431,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4","executionInfo":{"status":"ok","timestamp":1605895970237,"user_tz":0,"elapsed":488,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def plot_durations(episode_durations):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6","executionInfo":{"status":"ok","timestamp":1605895971639,"user_tz":0,"elapsed":485,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9","executionInfo":{"status":"ok","timestamp":1605895972697,"user_tz":0,"elapsed":1009,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA","executionInfo":{"status":"ok","timestamp":1605895972697,"user_tz":0,"elapsed":532,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD","executionInfo":{"status":"ok","timestamp":1605895973955,"user_tz":0,"elapsed":445,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def fanin_init(size, fanin=None):\n","    fanin = fanin or size[0]\n","    v = 1. / np.sqrt(fanin)\n","    return torch.Tensor(size).uniform_(-v, v)\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.head = nn.Linear(128, nb_actions)\n","        \n","        self.tanh = nn.Tanh()\n","        self.init_weights(3e-3)\n","    \n","    def init_weights(self, init_w):\n","        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n","        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n","        self.head.weight.data.uniform_(-init_w, init_w)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l2 = nn.Linear(128, 128)\n","        self.l3 = nn.Linear(128, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l5 = nn.Linear(128, 128)\n","        self.l6 = nn.Linear(128, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG","executionInfo":{"status":"ok","timestamp":1605895977030,"user_tz":0,"elapsed":914,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 50000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.s_t = None # Most recent state\n","        self.a_t = None # Most recent action\n","        self.is_training = True\n","\n","    def update_policy(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","        \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, r_t, s_t1, done):\n","        if self.is_training:\n","            self.memory.store(self.s_t, self.a_t, s_t1, r_t, done)\n","            self.s_t = s_t1\n","\n","    def random_action(self):\n","        action = torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","        self.a_t = action\n","        \n","        return action\n","\n","    def select_action(self, s_t, decay_epsilon=True):\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            self.a_t = action\n","            return action\n","\n","    def reset(self, obs):\n","        self.s_t = obs\n","        self.random_process.reset_states()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI","executionInfo":{"status":"ok","timestamp":1605896020616,"user_tz":0,"elapsed":412,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def train_model():\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = TD3(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 50\n","    num_episodes = 2000 # M\n","    episode_durations = []\n","\n","    steps = 0\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        agent.reset(state)\n","        \n","        overall_reward = 0\n","        episode_steps = 0\n","        done = False\n","        while not done:\n","            # agent pick action ...\n","            if i_episode <= warmup:\n","                action = agent.random_action()\n","            else:\n","                action = agent.select_action(state)\n","\n","            # env response with next_observation, reward, terminate_info\n","            observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","            steps += 1\n","            \n","            if max_episode_length and episode_steps >= max_episode_length -1:\n","                done = True\n","                \n","            extrinsic_reward = torch.tensor([reward], device=device)\n","            \n","            overall_reward += reward\n","            \n","            next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            # agent observe and update policy\n","            agent.observe(extrinsic_reward, next_state, done)\n","\n","            episode_steps += 1            \n","\n","            state = next_state\n","            \n","            if i_episode > warmup:\n","                agent.update_policy()\n","\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations)\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if np.mean(dur[-100:]) >= 195:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                return agent\n","    return agent"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxbm4KEzAsq_","executionInfo":{"status":"ok","timestamp":1605896388638,"user_tz":0,"elapsed":811,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\n","def eval_model(agent, episode_durations):\n","    agent.eval()\n","\n","    max_episode_length = 200\n","    num_episodes = 100\n","\n","    for noise in np.arange(0,0.31,0.03):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","            # unsqueeze adds batch dimension\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                state = state + state_max * torch.FloatTensor(state.shape).uniform_(-noise/2, noise/2).to(device)\n","                state = state.float()\n","\n","                action = agent.select_action(state)\n","                observation, reward, done, _ = env.step(action.detach().cpu().squeeze(0).numpy())\n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        episode_durations[noise].append(overall_reward / num_episodes)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"34H37Md4A-PZ","executionInfo":{"status":"ok","timestamp":1605896084706,"user_tz":0,"elapsed":435,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["def plot_norms(episode_durations):\n","    plt.figure(2, figsize=(10,10))\n","    \n","    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n","    \n","    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n","    plt.xlabel('L2 Norm')\n","    plt.ylabel('Average Reward')\n","    \n","    mu = np.mean(ys, axis=1)\n","    plt.plot(x / 10, mu)\n","    stds = np.std(ys, axis = 1)\n","    plt.fill_between(x / 10, mu + stds , mu - stds, alpha=0.2)\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"jazaVnJNErwK","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1605901742928,"user_tz":0,"elapsed":5353367,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"81ea70c0-1d41-4758-9621-0b181e74807f"},"source":["episodes = {}\n","for l2norm in np.arange(0,0.31,0.03):\n","    episodes[l2norm] = []\n","\n","# train 20 models for 200 steps, then eval them\n","for i in range(20):\n","    agent = train_model()\n","    eval_model(agent, episodes)\n","    print(i, episodes)\n","    plot_norms(episodes)\n","\n","plot_norms(episodes)\n","print(episodes)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{0.0: [199.96, 195.21, 200.0, 199.13, 200.0, 200.0, 200.0, 189.13, 197.08, 199.88, 194.55, 190.67, 199.92, 158.1, 200.0, 197.94, 199.48, 191.84, 200.0, 169.8], 0.03: [114.3, 36.53, 109.54, 181.7, 103.88, 32.98, 125.95, 49.81, 156.45, 39.77, 154.84, 189.48, 75.67, 88.33, 76.32, 61.9, 199.52, 47.24, 83.42, 182.34], 0.06: [44.66, 35.35, 46.66, 92.69, 40.73, 29.54, 60.17, 34.51, 64.16, 33.38, 54.87, 132.21, 37.21, 44.4, 37.46, 31.08, 188.41, 30.33, 43.91, 137.44], 0.09: [32.73, 29.24, 41.25, 51.72, 32.98, 23.44, 47.9, 25.82, 41.79, 30.17, 39.23, 79.74, 31.09, 34.84, 37.19, 29.9, 139.93, 29.84, 36.15, 81.43], 0.12: [31.22, 24.25, 30.37, 35.86, 28.04, 26.34, 43.63, 29.31, 34.03, 26.86, 35.54, 47.17, 27.37, 31.69, 31.06, 27.27, 90.83, 24.77, 31.58, 55.22], 0.15: [26.47, 26.01, 26.39, 30.36, 27.17, 23.71, 35.43, 23.95, 27.73, 26.45, 31.1, 42.11, 24.85, 27.0, 27.69, 24.1, 71.58, 24.07, 24.62, 47.36], 0.18: [26.0, 26.29, 26.05, 34.3, 27.74, 23.32, 34.1, 26.82, 29.81, 25.18, 28.53, 33.61, 24.07, 26.47, 28.43, 27.07, 52.77, 23.74, 26.94, 44.37], 0.21: [27.36, 23.77, 24.93, 28.95, 26.71, 24.65, 33.64, 28.84, 30.56, 26.3, 28.86, 30.63, 26.49, 25.7, 27.46, 25.41, 43.2, 24.67, 28.7, 40.61], 0.24: [23.73, 23.08, 27.99, 27.3, 28.05, 21.1, 29.75, 24.36, 28.08, 25.18, 27.69, 31.15, 23.18, 24.26, 26.09, 21.87, 40.13, 26.03, 24.58, 34.86], 0.27: [24.16, 22.83, 25.06, 27.53, 25.95, 23.67, 27.68, 23.45, 28.17, 25.66, 25.87, 28.14, 24.64, 28.1, 24.02, 24.38, 31.01, 22.3, 27.71, 31.01], 0.3: [22.96, 23.65, 26.95, 25.28, 27.42, 26.27, 28.25, 21.02, 25.08, 23.58, 23.48, 29.72, 23.69, 27.73, 25.07, 23.4, 28.42, 23.03, 25.4, 33.27]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y5kgVRwJErwO","executionInfo":{"status":"ok","timestamp":1605901757240,"user_tz":0,"elapsed":401,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["import json\n","json = json.dumps(episodes)\n","f = open(\"td3_episodes.json\",\"w\")\n","f.write(json)\n","f.close()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"6T1_obc7eegO"},"source":[""],"execution_count":null,"outputs":[]}]}