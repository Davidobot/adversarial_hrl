{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"HDQN_acrobot.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BOa4bDnuU8w7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609333441538,"user_tz":-60,"elapsed":22225,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"de95607a-edc1-4b3b-eef4-18b908786a68"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#!cp \"/content/drive/My Drive/Dissertation/preprocessing.py\" .\n","#!cp -r \"/content/drive/My Drive/Dissertation/gym_maze\" .\n","#!cp -r \"/content/drive/My Drive/Dissertation/envs\" ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UjRnm66x6Sga","executionInfo":{"status":"ok","timestamp":1609333442962,"user_tz":-60,"elapsed":606,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# for inference, not continued training\r\n","def save_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{name}\" \r\n","\r\n","    torch.save({\r\n","      'meta_controller': model.meta_controller.state_dict(),\r\n","      'controller': model.controller.state_dict()\r\n","    }, path)\r\n","\r\n","import copy\r\n","def load_model(model, name):\r\n","    path = f\"/content/drive/My Drive/Dissertation/saved_models/{name}\" \r\n","    checkpoint = torch.load(path)\r\n","\r\n","    model.meta_controller.load_state_dict(checkpoint['meta_controller'])\r\n","    model.meta_controller_target = copy.deepcopy(model.meta_controller)\r\n","    model.controller.load_state_dict(checkpoint['controller'])\r\n","    model.controller_target = copy.deepcopy(model.controller)\r\n","\r\n","    model.eval()\r\n","    model.meta_controller.eval()\r\n","    model.controller.eval()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_nd1HgGU8w7","executionInfo":{"status":"ok","timestamp":1609333449606,"user_tz":-60,"elapsed":4826,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","import cv2\n","from PIL import Image\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nwe9sNmHU8w7","executionInfo":{"status":"ok","timestamp":1609333449614,"user_tz":-60,"elapsed":4220,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["env = gym.make(\"Acrobot-v1\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vG98TmEU8w8"},"source":["---\n","### Helper functions"]},{"cell_type":"code","metadata":{"id":"KdafwykgU8w8","executionInfo":{"status":"ok","timestamp":1609333450479,"user_tz":-60,"elapsed":854,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["loss_plot = []\n","loss_plot_meta = []\n","def plot_durations(episode_durations):\n","    global loss_plot\n","    fig, axs = plt.subplots(3, figsize=(10,15))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    axs[1].set_xlabel('Steps')\n","    axs[1].set_ylabel('Loss')\n","    axs[2].set_xlabel('Steps')\n","    axs[2].set_ylabel('Loss (meta)')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","\n","    if len(loss_plot) > 0:\n","        durations_t, durations = list(map(list, zip(*loss_plot)))\n","        durations = torch.tensor(durations, dtype=torch.float)\n","\n","        axs[1].plot(durations_t, durations.numpy())\n","    if len(loss_plot_meta) > 0:\n","        durations_t, durations = list(map(list, zip(*loss_plot_meta)))\n","        durations = torch.tensor(durations, dtype=torch.float)\n","\n","        axs[2].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qx6CukwVU8w8"},"source":["---\n","### Code"]},{"cell_type":"code","metadata":{"id":"fisoWfOnU8w8","executionInfo":{"status":"ok","timestamp":1609333450488,"user_tz":-60,"elapsed":604,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZ9BRCoIU8w8","executionInfo":{"status":"ok","timestamp":1609333452176,"user_tz":-60,"elapsed":622,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, mem_len = 100000):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(inputs, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.head = nn.Linear(128, outputs)\n","        \n","        self.memory = ReplayMemory(mem_len)\n","\n","        self.n_actions = outputs\n","        self.steps_done = 0\n","        \n","        self.EPS_START = 1.0\n","        self.EPS_END = 0.01\n","        self.EPS_DECAY = 10000 # in number of steps\n","        self.TAU = 0.001\n","\n","        self.policy_update = 2\n","        self.tot_updates = 0\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.head(x)\n","    \n","    def act(self, state, is_training):\n","        if is_training:\n","            eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n","            self.steps_done += 1\n","\n","            # With probability eps select a random action\n","            if random.random() < eps_threshold:\n","                return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n","\n","        # otherwise select action = maxa Q∗(φ(st), a; θ)\n","        with torch.no_grad():\n","            return self(state).max(1)[1].view(1, 1)\n","    \n","    def experience_replay(self, optimizer, target):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.tot_updates += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","        \n","        current_Q_values = self(state_batch).gather(1, action_batch)\n","        # Compute next Q value based on which goal gives max Q values\n","        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n","        next_max_q = target(next_state_batch).detach().max(1)[0]\n","        next_Q_values = not_done_mask * next_max_q\n","        # Compute the target of the current Q values\n","        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n","        # Compute Bellman error (using Huber loss)\n","        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n","        loss_val = loss.item()\n","\n","        # Optimize the model\n","        optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        optimizer.step()\n","\n","        if self.tot_updates % self.policy_update == 0:\n","            soft_update(target, self, self.TAU)\n","\n","        return loss_val\n","        \n","class HDQN(nn.Module):\n","    def __init__(self, inputs, outputs):\n","        super(HDQN, self).__init__()\n","        # Optimizer\n","        learning_rate = 2.5e-4\n","        \n","        # goal is left/right\n","        self.meta_controller = DQN(inputs, outputs).to(device)\n","        self.meta_controller_optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n","        self.meta_controller_target = DQN(inputs, outputs, mem_len = 0).to(device)\n","        self.meta_controller_target.eval()\n","        \n","        # takes goal+state jointly\n","        self.controller = DQN(inputs + 1, outputs).to(device)\n","        self.controller_optimizer = optim.RMSprop(self.controller.parameters(), lr=learning_rate)\n","        self.controller_target = DQN(inputs + 1, outputs, mem_len = 0).to(device)\n","        self.controller_target.eval()\n","\n","        self.controller.EPS_END = 0.0\n","        self.controller.EPS_DECAY = 1000\n","    \n","    def store_controller(self, *args):\n","        self.controller.memory.store(*args)\n","    \n","    def store_meta_controller(self, *args):\n","        self.meta_controller.memory.store(*args)\n","    \n","    def select_goal(self, external_observation, is_training):\n","        return self.meta_controller.act(external_observation, is_training)\n","        \n","    def select_action(self, joint_goal_obs, is_training):\n","        return self.controller.act(joint_goal_obs, is_training)\n","    \n","    def teach_meta_controller(self):\n","        return self.meta_controller.experience_replay(self.meta_controller_optimizer, self.meta_controller_target)\n","\n","    def teach_controller(self):\n","        return self.controller.experience_replay(self.controller_optimizer, self.controller_target)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgHmamR6U8w8"},"source":["def plot_norms(episode_durations):\n","    plt.figure(2, figsize=(10,10))\n","    \n","    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n","    \n","    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n","    plt.xlabel('L2 Norm')\n","    plt.ylabel('Average Reward')\n","    \n","    mu = np.mean(ys, axis=1)\n","    plt.plot(x, mu)\n","    stds = np.std(ys, axis = 1)\n","    plt.fill_between(x, mu + stds , mu - stds, alpha=0.2)\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nr9-j_IbU8w8","executionInfo":{"status":"ok","timestamp":1609333530680,"user_tz":-60,"elapsed":563,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["SAVE_OFFSET = 10\n","\n","def train_model():\n","    global SAVE_OFFSET\n","    # Get number of actions and observations from gym action space\n","    n_actions = env.action_space.n\n","    n_observations = env.observation_space.shape[0]\n","\n","    # Initialize action-value function Q with random weights\n","    hdqnAgent = HDQN(n_observations, n_actions).to(device)\n","\n","    max_episode_length = 500\n","\n","    num_episodes = 5000 # M\n","    episode_durations = []\n","\n","    steps = 0\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        overall_reward = 0\n","        done = False\n","        episode_steps = 0\n","        while not done:\n","            # select a goal\n","            goal = hdqnAgent.select_goal(state, True)\n","            goal_i = goal.item()\n","\n","            goal_done = False\n","            total_extrinsic = 0\n","            s_0 = state\n","            steps_until_goal = 0\n","            while not done and not goal_done:\n","                joint_goal_state = torch.cat([goal, state], axis=1)\n","\n","                # Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n","                action = hdqnAgent.select_action(joint_goal_state, True)\n","                action_i = action.item()\n","\n","                observation, reward, done, _ = env.step(action_i)\n","                steps += 1\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","                \n","                extrinsic_reward = torch.tensor([reward], device=device)\n","\n","                overall_reward += reward\n","                total_extrinsic += reward\n","\n","                # preprocess φ_{t+1} = φ(s_{t+1})\n","                next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                joint_next_state = torch.cat([goal, next_state], axis=1)\n","\n","                goal_done = (goal_i == action_i)\n","                if not goal_done:\n","                    steps_until_goal += 1\n","                else:\n","                    pass\n","                    #loss_plot.append((steps, steps_until_goal))\n","\n","                intrinsic_reward = torch.tensor([1.0 if goal_done else 0], device=device)\n","\n","                # Store transition (φt, at, rt, φt+1) in D\n","                hdqnAgent.store_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done)\n","\n","                state = next_state\n","\n","                loss = hdqnAgent.teach_controller()\n","                if loss is not None and i_episode % 50 == 0:\n","                    loss_plot.append((steps, loss))\n","\n","            # Store transition for meta controller\n","            hdqnAgent.store_meta_controller(s_0, goal, next_state, torch.tensor([total_extrinsic], device=device), done)\n","            loss = hdqnAgent.teach_meta_controller()\n","            if loss is not None and i_episode % 50 == 0:\n","                loss_plot_meta.append((steps, loss))\n","        \n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations)\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if i_episode % 100 == 0 and i_episode >= 1000 and np.mean(dur[-100:]) < -200.0:\n","                print(f\"Failed to get lucky after 1000 eps, terminating... Avg: {np.mean(dur[-100:])}\")\n","                return None # unlucky\n","            if i_episode % 300 == 0:\n","                print(f\"Episode {i_episode}: {np.mean(dur[-100:])}\")\n","            if np.mean(dur[-100:]) >= -90:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                save_model(hdqnAgent, f\"hdqn_acrobot_{SAVE_OFFSET}\")\n","                SAVE_OFFSET += 1\n","                return hdqnAgent\n","\n","    return None # did not train"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6BqeP-7w7MN"},"source":["loss_plot = []\r\n","loss_plot_meta = []\r\n","#agent = train_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhhY2VPXxmvZ","executionInfo":{"status":"ok","timestamp":1609334882053,"user_tz":-60,"elapsed":1349278,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"c40ed559-8661-45d8-8e43-96d4886f3d08"},"source":["i = 10\r\n","while i < 11:\r\n","    loss_plot = []\r\n","    loss_plot_meta = []\r\n","    agent = train_model()\r\n","    if agent is not None:\r\n","        print(f\"Num. {i} done!\")\r\n","        i += 1"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Episode 300: -145.29\n","Episode 600: -144.33\n","Episode 900: -126.0\n","Episode 1200: -97.58\n","Solved after 1361 episodes!\n","Num. 10 done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"55noGZWJ7rFU","executionInfo":{"status":"ok","timestamp":1609335779252,"user_tz":-60,"elapsed":571,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\r\n","state_min = torch.from_numpy(env.observation_space.low).to(device)\r\n","def fgsm_attack(data, eps, data_grad):\r\n","    sign_data_grad = data_grad.sign()\r\n","\r\n","    perturbed_data = data + eps * sign_data_grad * state_max\r\n","\r\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\r\n","\r\n","    return clipped_perturbed_data\r\n","\r\n","def fgsm_goal(g_state, agent, eps, target, targetted):\r\n","    #g_state = torch.tensor(g_state, requires_grad=True)\r\n","\r\n","    g_state_var = g_state.clone().detach().requires_grad_(True)\r\n","\r\n","    # initial forward pass\r\n","    goal = agent.meta_controller(g_state_var)\r\n","    #goal = temp.max(1)[1].view(1, 1)\r\n","\r\n","    if targetted:\r\n","        loss = F.smooth_l1_loss(goal, target)\r\n","    else:\r\n","        pass\r\n","        #loss = F.smooth_l1_loss(goal, temp.min(1)[1].view(1, 1).float())\r\n","\r\n","    agent.meta_controller.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = g_state_var.grad.data\r\n","\r\n","    # perturb state\r\n","    g_state_p = fgsm_attack(g_state, eps, data_grad)\r\n","    return agent.select_goal(g_state_p, False)\r\n","\r\n","def fgsm_action(state, goal, agent, eps, target, targetted):\r\n","    #state = torch.tensor(state, requires_grad=True)\r\n","    state_var = state.clone().detach().requires_grad_(True)\r\n","\r\n","    joint_goal_state = torch.cat([goal, state_var], 1).float()\r\n","    \r\n","    # initial forward pass\r\n","    action = agent.controller(joint_goal_state)\r\n","    #action = temp.max(1)[1].view(1, 1).float()\r\n","\r\n","    if targetted:\r\n","        loss = F.smooth_l1_loss(action, target)\r\n","    else:\r\n","        pass\r\n","        #loss = F.smooth_l1_loss(action, temp.min(1)[1].view(1, 1).float())\r\n","\r\n","    agent.controller.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = state_var.grad.data\r\n","    # perturb state\r\n","    state_p = fgsm_attack(state, eps, data_grad)\r\n","\r\n","    joint_goal_state = torch.cat([goal, state_p], 1).float()\r\n","    return agent.select_action(joint_goal_state, False)\r\n","\r\n","def apply_fgsm(agent, episode_durations, goal_attack, action_attack, targetted):\r\n","    TARGET_GOAL = torch.tensor([[0.0, 0.0, 0.0]], device=device, dtype=torch.float)\r\n","    TARGET_ACTION = torch.tensor([[0.0, 0.0, 0.0]], device=device, dtype=torch.float)\r\n","\r\n","    agent.eval()\r\n","    agent.meta_controller.eval()\r\n","    agent.controller.eval()\r\n","\r\n","    max_episode_length = 500\r\n","\r\n","    num_episodes = 100\r\n","\r\n","    for eps in np.arange(0.0, 0.031, 0.0025):\r\n","\r\n","        overall_reward = 0\r\n","        for i_episode in range(num_episodes):\r\n","            observation = env.reset()\r\n","\r\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","                \r\n","            episode_steps = 0\r\n","            done = False\r\n","            while not done:\r\n","                # select a goal\r\n","                if goal_attack:\r\n","                    goal = fgsm_goal(state, agent, eps, TARGET_GOAL, targetted)\r\n","                else:\r\n","                    goal = agent.select_goal(state, False)\r\n","                goal_i = goal.item()\r\n","\r\n","                goal_done = False\r\n","                while not done and not goal_done:\r\n","                    joint_goal_state = torch.cat([goal, state], axis=1)\r\n","\r\n","                    if action_attack:\r\n","                        action = fgsm_action(state, goal, agent, eps, TARGET_ACTION, targetted)\r\n","                    else:\r\n","                        action = agent.select_action(joint_goal_state, False)\r\n","\r\n","                    action_i = action.item()\r\n","                    observation, reward, done, _ = env.step(action_i)\r\n","\r\n","                    overall_reward += reward\r\n","\r\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\r\n","                        done = True\r\n","                    episode_steps += 1\r\n","\r\n","                    goal_done = (goal_i == action_i)\r\n","\r\n","                    state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktebJKBtU8w8","executionInfo":{"status":"ok","timestamp":1609334882061,"user_tz":-60,"elapsed":1327010,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}}},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\n","def eval_model(hdqnAgent, episode_durations, goal_noise, action_noise, same_noise):\n","    hdqnAgent.eval()\n","    hdqnAgent.meta_controller.eval()\n","    hdqnAgent.controller.eval()\n","\n","    max_episode_length = 500\n","    num_episodes = 100\n","\n","    for l2norm in np.arange(0,0.31,0.03):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","            g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            noise = torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","            if goal_noise:\n","                g_state = state + state_max * noise\n","                g_state = g_state.float()\n","            if action_noise:\n","                if same_noise:\n","                    state = state + state_max * noise\n","                else:\n","                    state = state + state_max * torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                state = state.float()\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                # select a goal\n","                goal = hdqnAgent.select_goal(g_state, False)\n","                goal_i = goal.item()\n","\n","                goal_done = False\n","                while not done and not goal_done:\n","                    joint_goal_state = torch.cat([goal, state], axis=1)\n","\n","                    action = hdqnAgent.select_action(joint_goal_state, False)\n","                    action_i = action.item()\n","                    observation, reward, done, _ = env.step(action_i)\n","\n","                    overall_reward += reward\n","\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\n","                        done = True\n","                    episode_steps += 1\n","\n","                    goal_done = (goal_i == action_i)\n","\n","                    state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","                    g_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","                    noise = torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                    if goal_noise:\n","                        g_state = state + state_max * noise\n","                        g_state = g_state.float()\n","                    if action_noise:\n","                        if same_noise:\n","                            state = state + state_max * noise\n","                        else:\n","                            state = state + state_max * torch.FloatTensor(state.shape).uniform_(-l2norm/2, l2norm/2).to(device)\n","                        state = state.float()\n","\n","        episode_durations[l2norm].append(overall_reward / num_episodes)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJKWKLPjAipR"},"source":["def plot_fgsm(episode_durations):\r\n","    plt.figure(2, figsize=(10,10))\r\n","    \r\n","    for kk in ['both', 'goal_only', 'action_only']:\r\n","        x, ys = np.array(list(episode_durations[kk].keys())), np.array(list(episode_durations[kk].values()))\r\n","        #plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\r\n","        plt.xlabel('$\\epsilon$')\r\n","        plt.ylabel('Average Reward')\r\n","        \r\n","        mu = np.mean(ys, axis=1)\r\n","        plt.plot(x, mu, label=kk)\r\n","        stds = np.std(ys, axis = 1)\r\n","        plt.fill_between(x, mu + stds , mu - stds, alpha=0.2)\r\n","    \r\n","    plt.legend()\r\n","    plt.pause(0.001)  # pause a bit so that plots are updated\r\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpWwUiRkAjnM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609347006979,"user_tz":-60,"elapsed":11216494,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"0e74bd29-7fa4-4e39-c40d-c630437d8f40"},"source":["targeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\r\n","untargeted = {'both': {}, 'goal_only': {}, 'action_only': {}}\r\n","for eps in np.arange(0.0, 0.031, 0.0025):\r\n","    for x in ['both', 'goal_only', 'action_only']:\r\n","        targeted[x][eps] = []\r\n","        untargeted[x][eps] = []\r\n","\r\n","n_actions = env.action_space.n\r\n","n_observations = env.observation_space.shape[0]\r\n","\r\n","i = 0\r\n","while i < 11:\r\n","    #agent = train_model()\r\n","    agent = HDQN(n_observations, n_actions).to(device)\r\n","    load_model(agent, f\"hdqn_acrobot_{i}\")\r\n","    if agent is not None:\r\n","        apply_fgsm(agent, targeted['both'], True, True, True)\r\n","        apply_fgsm(agent, targeted['goal_only'], True, False, True)\r\n","        apply_fgsm(agent, targeted['action_only'], False, True, True)\r\n","        #apply_fgsm(agent, untargeted['both'], True, True, False)\r\n","        #apply_fgsm(agent, untargeted['goal_only'], True, False, False)\r\n","        #apply_fgsm(agent, untargeted['action_only'], False, True, False)\r\n","        print(i)\r\n","        print(f\"Targeted: {targeted}\")\r\n","        print(f\"Untargeted: {untargeted}\")\r\n","        #plot_fgsm(episode_durations)\r\n","        i += 1\r\n","\r\n","#plot_fgsm(episode_durations)\r\n","print(f\"Targeted: {targeted}\")\r\n","print(f\"Untargeted: {untargeted}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["0\n","Targeted: {'both': {0.0: [-95.42], 0.0025: [-97.86], 0.005: [-99.11], 0.0075: [-100.9], 0.01: [-96.07], 0.0125: [-114.32], 0.015: [-119.39], 0.0175: [-114.82], 0.02: [-117.03], 0.0225: [-127.63], 0.025: [-122.41], 0.0275: [-124.3], 0.03: [-146.24]}, 'goal_only': {0.0: [-92.32], 0.0025: [-96.31], 0.005: [-94.1], 0.0075: [-101.48], 0.01: [-91.48], 0.0125: [-96.54], 0.015: [-102.71], 0.0175: [-104.31], 0.02: [-115.98], 0.0225: [-105.39], 0.025: [-102.84], 0.0275: [-123.53], 0.03: [-123.79]}, 'action_only': {0.0: [-99.28], 0.0025: [-100.16], 0.005: [-95.38], 0.0075: [-102.32], 0.01: [-100.72], 0.0125: [-102.34], 0.015: [-97.2], 0.0175: [-96.13], 0.02: [-96.72], 0.0225: [-94.4], 0.025: [-96.36], 0.0275: [-96.48], 0.03: [-95.89]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","1\n","Targeted: {'both': {0.0: [-95.42, -86.81], 0.0025: [-97.86, -87.41], 0.005: [-99.11, -91.32], 0.0075: [-100.9, -89.73], 0.01: [-96.07, -87.38], 0.0125: [-114.32, -89.41], 0.015: [-119.39, -93.05], 0.0175: [-114.82, -95.79], 0.02: [-117.03, -94.04], 0.0225: [-127.63, -96.43], 0.025: [-122.41, -100.07], 0.0275: [-124.3, -97.06], 0.03: [-146.24, -112.42]}, 'goal_only': {0.0: [-92.32, -93.01], 0.0025: [-96.31, -88.03], 0.005: [-94.1, -86.8], 0.0075: [-101.48, -91.83], 0.01: [-91.48, -89.53], 0.0125: [-96.54, -95.19], 0.015: [-102.71, -94.57], 0.0175: [-104.31, -98.65], 0.02: [-115.98, -106.44], 0.0225: [-105.39, -95.93], 0.025: [-102.84, -99.07], 0.0275: [-123.53, -103.61], 0.03: [-123.79, -99.48]}, 'action_only': {0.0: [-99.28, -92.73], 0.0025: [-100.16, -94.33], 0.005: [-95.38, -97.7], 0.0075: [-102.32, -91.2], 0.01: [-100.72, -95.2], 0.0125: [-102.34, -89.08], 0.015: [-97.2, -97.11], 0.0175: [-96.13, -97.45], 0.02: [-96.72, -93.76], 0.0225: [-94.4, -104.02], 0.025: [-96.36, -97.6], 0.0275: [-96.48, -90.59], 0.03: [-95.89, -106.31]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","2\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19], 0.0025: [-97.86, -87.41, -137.33], 0.005: [-99.11, -91.32, -115.66], 0.0075: [-100.9, -89.73, -92.43], 0.01: [-96.07, -87.38, -98.21], 0.0125: [-114.32, -89.41, -94.78], 0.015: [-119.39, -93.05, -95.29], 0.0175: [-114.82, -95.79, -101.12], 0.02: [-117.03, -94.04, -104.66], 0.0225: [-127.63, -96.43, -108.53], 0.025: [-122.41, -100.07, -116.84], 0.0275: [-124.3, -97.06, -129.0], 0.03: [-146.24, -112.42, -128.89]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67], 0.0025: [-96.31, -88.03, -143.17], 0.005: [-94.1, -86.8, -105.78], 0.0075: [-101.48, -91.83, -107.64], 0.01: [-91.48, -89.53, -91.67], 0.0125: [-96.54, -95.19, -91.76], 0.015: [-102.71, -94.57, -93.55], 0.0175: [-104.31, -98.65, -94.47], 0.02: [-115.98, -106.44, -93.03], 0.0225: [-105.39, -95.93, -90.67], 0.025: [-102.84, -99.07, -96.23], 0.0275: [-123.53, -103.61, -94.32], 0.03: [-123.79, -99.48, -93.5]}, 'action_only': {0.0: [-99.28, -92.73, -180.74], 0.0025: [-100.16, -94.33, -144.34], 0.005: [-95.38, -97.7, -143.47], 0.0075: [-102.32, -91.2, -181.0], 0.01: [-100.72, -95.2, -181.27], 0.0125: [-102.34, -89.08, -161.5], 0.015: [-97.2, -97.11, -198.74], 0.0175: [-96.13, -97.45, -176.85], 0.02: [-96.72, -93.76, -154.36], 0.0225: [-94.4, -104.02, -163.13], 0.025: [-96.36, -97.6, -160.66], 0.0275: [-96.48, -90.59, -186.5], 0.03: [-95.89, -106.31, -193.1]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","3\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18], 0.0025: [-97.86, -87.41, -137.33, -90.22], 0.005: [-99.11, -91.32, -115.66, -103.11], 0.0075: [-100.9, -89.73, -92.43, -101.49], 0.01: [-96.07, -87.38, -98.21, -127.77], 0.0125: [-114.32, -89.41, -94.78, -134.6], 0.015: [-119.39, -93.05, -95.29, -136.2], 0.0175: [-114.82, -95.79, -101.12, -132.13], 0.02: [-117.03, -94.04, -104.66, -148.44], 0.0225: [-127.63, -96.43, -108.53, -142.31], 0.025: [-122.41, -100.07, -116.84, -134.85], 0.0275: [-124.3, -97.06, -129.0, -126.81], 0.03: [-146.24, -112.42, -128.89, -128.34]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75], 0.0025: [-96.31, -88.03, -143.17, -106.52], 0.005: [-94.1, -86.8, -105.78, -114.24], 0.0075: [-101.48, -91.83, -107.64, -117.17], 0.01: [-91.48, -89.53, -91.67, -128.87], 0.0125: [-96.54, -95.19, -91.76, -127.64], 0.015: [-102.71, -94.57, -93.55, -137.14], 0.0175: [-104.31, -98.65, -94.47, -142.21], 0.02: [-115.98, -106.44, -93.03, -156.34], 0.0225: [-105.39, -95.93, -90.67, -115.6], 0.025: [-102.84, -99.07, -96.23, -114.24], 0.0275: [-123.53, -103.61, -94.32, -122.7], 0.03: [-123.79, -99.48, -93.5, -109.61]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72], 0.0025: [-100.16, -94.33, -144.34, -90.44], 0.005: [-95.38, -97.7, -143.47, -92.13], 0.0075: [-102.32, -91.2, -181.0, -90.75], 0.01: [-100.72, -95.2, -181.27, -97.18], 0.0125: [-102.34, -89.08, -161.5, -92.39], 0.015: [-97.2, -97.11, -198.74, -100.0], 0.0175: [-96.13, -97.45, -176.85, -98.69], 0.02: [-96.72, -93.76, -154.36, -100.95], 0.0225: [-94.4, -104.02, -163.13, -105.82], 0.025: [-96.36, -97.6, -160.66, -109.22], 0.0275: [-96.48, -90.59, -186.5, -103.03], 0.03: [-95.89, -106.31, -193.1, -110.83]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","4\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","5\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","6\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","7\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76, -83.74], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62, -79.24], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18, -79.94], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52, -80.18], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32, -81.33], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53, -81.97], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21, -88.91], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48, -91.72], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34, -101.1], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84, -105.22], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28, -103.73], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03, -103.14], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4, -102.56]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69, -82.29], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16, -79.42], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71, -81.05], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01, -84.23], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61, -80.83], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96, -80.92], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62, -89.0], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12, -87.88], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87, -87.44], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2, -87.51], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1, -91.58], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71, -95.08], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77, -100.75]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18, -83.81], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14, -84.46], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52, -88.47], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57, -85.19], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85, -81.67], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31, -84.52], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25, -88.12], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39, -101.69], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67, -97.2], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87, -97.61], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14, -100.92], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91, -99.78], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76, -100.08]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","8\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76, -83.74, -89.99], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62, -79.24, -94.69], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18, -79.94, -99.06], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52, -80.18, -98.52], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32, -81.33, -104.08], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53, -81.97, -105.25], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21, -88.91, -111.85], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48, -91.72, -115.45], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34, -101.1, -117.83], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84, -105.22, -124.74], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28, -103.73, -149.51], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03, -103.14, -185.92], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4, -102.56, -170.94]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69, -82.29, -89.85], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16, -79.42, -93.23], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71, -81.05, -89.04], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01, -84.23, -92.47], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61, -80.83, -97.27], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96, -80.92, -97.93], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62, -89.0, -99.12], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12, -87.88, -105.34], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87, -87.44, -109.0], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2, -87.51, -121.6], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1, -91.58, -143.53], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71, -95.08, -139.06], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77, -100.75, -168.19]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18, -83.81, -89.29], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14, -84.46, -92.16], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52, -88.47, -95.5], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57, -85.19, -93.44], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85, -81.67, -94.54], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31, -84.52, -98.24], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25, -88.12, -96.23], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39, -101.69, -99.02], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67, -97.2, -99.51], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87, -97.61, -104.07], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14, -100.92, -109.3], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91, -99.78, -108.76], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76, -100.08, -117.32]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","9\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76, -83.74, -89.99, -87.48], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62, -79.24, -94.69, -82.09], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18, -79.94, -99.06, -88.69], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52, -80.18, -98.52, -88.17], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32, -81.33, -104.08, -90.4], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53, -81.97, -105.25, -96.2], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21, -88.91, -111.85, -98.06], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48, -91.72, -115.45, -103.77], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34, -101.1, -117.83, -108.11], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84, -105.22, -124.74, -143.65], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28, -103.73, -149.51, -157.32], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03, -103.14, -185.92, -164.52], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4, -102.56, -170.94, -167.65]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69, -82.29, -89.85, -87.79], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16, -79.42, -93.23, -85.05], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71, -81.05, -89.04, -89.93], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01, -84.23, -92.47, -83.81], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61, -80.83, -97.27, -84.44], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96, -80.92, -97.93, -83.64], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62, -89.0, -99.12, -86.95], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12, -87.88, -105.34, -87.5], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87, -87.44, -109.0, -104.78], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2, -87.51, -121.6, -138.68], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1, -91.58, -143.53, -150.02], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71, -95.08, -139.06, -163.25], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77, -100.75, -168.19, -139.56]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18, -83.81, -89.29, -93.53], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14, -84.46, -92.16, -86.19], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52, -88.47, -95.5, -88.53], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57, -85.19, -93.44, -87.12], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85, -81.67, -94.54, -88.68], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31, -84.52, -98.24, -90.2], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25, -88.12, -96.23, -87.69], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39, -101.69, -99.02, -93.65], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67, -97.2, -99.51, -96.74], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87, -97.61, -104.07, -99.26], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14, -100.92, -109.3, -103.78], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91, -99.78, -108.76, -100.72], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76, -100.08, -117.32, -102.21]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","10\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76, -83.74, -89.99, -87.48, -96.63], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62, -79.24, -94.69, -82.09, -90.01], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18, -79.94, -99.06, -88.69, -93.62], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52, -80.18, -98.52, -88.17, -89.56], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32, -81.33, -104.08, -90.4, -97.7], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53, -81.97, -105.25, -96.2, -99.27], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21, -88.91, -111.85, -98.06, -100.22], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48, -91.72, -115.45, -103.77, -103.49], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34, -101.1, -117.83, -108.11, -105.86], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84, -105.22, -124.74, -143.65, -101.58], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28, -103.73, -149.51, -157.32, -105.11], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03, -103.14, -185.92, -164.52, -105.71], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4, -102.56, -170.94, -167.65, -106.83]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69, -82.29, -89.85, -87.79, -94.79], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16, -79.42, -93.23, -85.05, -98.65], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71, -81.05, -89.04, -89.93, -92.98], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01, -84.23, -92.47, -83.81, -96.06], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61, -80.83, -97.27, -84.44, -98.64], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96, -80.92, -97.93, -83.64, -100.1], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62, -89.0, -99.12, -86.95, -104.26], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12, -87.88, -105.34, -87.5, -102.71], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87, -87.44, -109.0, -104.78, -101.88], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2, -87.51, -121.6, -138.68, -105.43], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1, -91.58, -143.53, -150.02, -107.32], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71, -95.08, -139.06, -163.25, -106.08], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77, -100.75, -168.19, -139.56, -108.82]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18, -83.81, -89.29, -93.53, -91.66], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14, -84.46, -92.16, -86.19, -93.67], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52, -88.47, -95.5, -88.53, -92.85], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57, -85.19, -93.44, -87.12, -93.47], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85, -81.67, -94.54, -88.68, -93.72], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31, -84.52, -98.24, -90.2, -89.99], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25, -88.12, -96.23, -87.69, -98.07], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39, -101.69, -99.02, -93.65, -92.12], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67, -97.2, -99.51, -96.74, -92.07], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87, -97.61, -104.07, -99.26, -92.61], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14, -100.92, -109.3, -103.78, -93.15], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91, -99.78, -108.76, -100.72, -96.04], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76, -100.08, -117.32, -102.21, -92.39]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n","Targeted: {'both': {0.0: [-95.42, -86.81, -138.19, -92.18, -76.88, -86.79, -92.76, -83.74, -89.99, -87.48, -96.63], 0.0025: [-97.86, -87.41, -137.33, -90.22, -79.28, -87.72, -92.62, -79.24, -94.69, -82.09, -90.01], 0.005: [-99.11, -91.32, -115.66, -103.11, -80.49, -90.01, -93.18, -79.94, -99.06, -88.69, -93.62], 0.0075: [-100.9, -89.73, -92.43, -101.49, -79.81, -91.28, -86.52, -80.18, -98.52, -88.17, -89.56], 0.01: [-96.07, -87.38, -98.21, -127.77, -95.2, -95.2, -91.32, -81.33, -104.08, -90.4, -97.7], 0.0125: [-114.32, -89.41, -94.78, -134.6, -88.25, -104.52, -93.53, -81.97, -105.25, -96.2, -99.27], 0.015: [-119.39, -93.05, -95.29, -136.2, -91.94, -129.98, -93.21, -88.91, -111.85, -98.06, -100.22], 0.0175: [-114.82, -95.79, -101.12, -132.13, -98.94, -119.49, -100.48, -91.72, -115.45, -103.77, -103.49], 0.02: [-117.03, -94.04, -104.66, -148.44, -108.25, -114.03, -99.34, -101.1, -117.83, -108.11, -105.86], 0.0225: [-127.63, -96.43, -108.53, -142.31, -111.32, -116.94, -100.84, -105.22, -124.74, -143.65, -101.58], 0.025: [-122.41, -100.07, -116.84, -134.85, -113.51, -123.98, -102.28, -103.73, -149.51, -157.32, -105.11], 0.0275: [-124.3, -97.06, -129.0, -126.81, -142.27, -116.82, -104.03, -103.14, -185.92, -164.52, -105.71], 0.03: [-146.24, -112.42, -128.89, -128.34, -165.85, -123.29, -107.4, -102.56, -170.94, -167.65, -106.83]}, 'goal_only': {0.0: [-92.32, -93.01, -152.67, -92.75, -82.45, -89.26, -93.69, -82.29, -89.85, -87.79, -94.79], 0.0025: [-96.31, -88.03, -143.17, -106.52, -80.96, -90.55, -93.16, -79.42, -93.23, -85.05, -98.65], 0.005: [-94.1, -86.8, -105.78, -114.24, -78.01, -89.63, -90.71, -81.05, -89.04, -89.93, -92.98], 0.0075: [-101.48, -91.83, -107.64, -117.17, -79.6, -99.99, -94.01, -84.23, -92.47, -83.81, -96.06], 0.01: [-91.48, -89.53, -91.67, -128.87, -84.41, -95.6, -90.61, -80.83, -97.27, -84.44, -98.64], 0.0125: [-96.54, -95.19, -91.76, -127.64, -85.81, -125.67, -89.96, -80.92, -97.93, -83.64, -100.1], 0.015: [-102.71, -94.57, -93.55, -137.14, -92.88, -144.72, -93.62, -89.0, -99.12, -86.95, -104.26], 0.0175: [-104.31, -98.65, -94.47, -142.21, -100.04, -118.09, -92.12, -87.88, -105.34, -87.5, -102.71], 0.02: [-115.98, -106.44, -93.03, -156.34, -106.16, -138.61, -100.87, -87.44, -109.0, -104.78, -101.88], 0.0225: [-105.39, -95.93, -90.67, -115.6, -133.1, -159.92, -102.2, -87.51, -121.6, -138.68, -105.43], 0.025: [-102.84, -99.07, -96.23, -114.24, -148.99, -198.87, -106.1, -91.58, -143.53, -150.02, -107.32], 0.0275: [-123.53, -103.61, -94.32, -122.7, -165.37, -243.65, -104.71, -95.08, -139.06, -163.25, -106.08], 0.03: [-123.79, -99.48, -93.5, -109.61, -181.54, -321.58, -106.77, -100.75, -168.19, -139.56, -108.82]}, 'action_only': {0.0: [-99.28, -92.73, -180.74, -90.72, -78.76, -88.72, -95.18, -83.81, -89.29, -93.53, -91.66], 0.0025: [-100.16, -94.33, -144.34, -90.44, -77.36, -87.7, -87.14, -84.46, -92.16, -86.19, -93.67], 0.005: [-95.38, -97.7, -143.47, -92.13, -77.74, -85.01, -94.52, -88.47, -95.5, -88.53, -92.85], 0.0075: [-102.32, -91.2, -181.0, -90.75, -80.09, -89.42, -93.57, -85.19, -93.44, -87.12, -93.47], 0.01: [-100.72, -95.2, -181.27, -97.18, -78.42, -87.51, -90.85, -81.67, -94.54, -88.68, -93.72], 0.0125: [-102.34, -89.08, -161.5, -92.39, -80.13, -86.7, -95.31, -84.52, -98.24, -90.2, -89.99], 0.015: [-97.2, -97.11, -198.74, -100.0, -78.33, -89.78, -94.25, -88.12, -96.23, -87.69, -98.07], 0.0175: [-96.13, -97.45, -176.85, -98.69, -80.03, -86.09, -93.39, -101.69, -99.02, -93.65, -92.12], 0.02: [-96.72, -93.76, -154.36, -100.95, -80.82, -88.03, -93.67, -97.2, -99.51, -96.74, -92.07], 0.0225: [-94.4, -104.02, -163.13, -105.82, -80.3, -87.86, -90.87, -97.61, -104.07, -99.26, -92.61], 0.025: [-96.36, -97.6, -160.66, -109.22, -80.72, -89.08, -92.14, -100.92, -109.3, -103.78, -93.15], 0.0275: [-96.48, -90.59, -186.5, -103.03, -80.42, -94.38, -88.91, -99.78, -108.76, -100.72, -96.04], 0.03: [-95.89, -106.31, -193.1, -110.83, -79.59, -90.73, -90.76, -100.08, -117.32, -102.21, -92.39]}}\n","Untargeted: {'both': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'goal_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}, 'action_only': {0.0: [], 0.0025: [], 0.005: [], 0.0075: [], 0.01: [], 0.0125: [], 0.015: [], 0.0175: [], 0.02: [], 0.0225: [], 0.025: [], 0.0275: [], 0.03: []}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rBfZ2tc_U8w8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609335368435,"user_tz":-60,"elapsed":486300,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"2901fcce-c518-4cf4-c356-5ec704e07d4f"},"source":["same_noise = {}\n","diff_noise = {}\n","goal_only = {}\n","action_only = {}\n","for l2norm in np.arange(0,0.31,0.03):\n","    for i in [same_noise, diff_noise, goal_only, action_only]:\n","        i[l2norm] = []\n","\n","n_actions = env.action_space.n\n","n_observations = env.observation_space.shape[0]\n","\n","i = 10\n","while i < 11:\n","    #agent = train_model()\n","    agent = HDQN(n_observations, n_actions).to(device)\n","    load_model(agent, f\"hdqn_acrobot_{i}\")\n","    if agent is not None:\n","        # goal_attack, action_attack, same_noise\n","        eval_model(agent, same_noise, True, True, True)\n","        eval_model(agent, diff_noise, True, True, False)\n","        eval_model(agent, goal_only, True, False, False)\n","        eval_model(agent, action_only, False, True, False)\n","        print(i)\n","        print(f\"same noise: {same_noise}\")\n","        print(f\"diff noise: {diff_noise}\")\n","        print(f\"goal only: {goal_only}\")\n","        print(f\"action only: {action_only}\")\n","        i += 1\n","\n","print(f\"same noise: {same_noise}\")\n","print(f\"diff noise: {diff_noise}\")\n","print(f\"goal only: {goal_only}\")\n","print(f\"action only: {action_only}\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["10\n","same noise: {0.0: [-91.89], 0.03: [-94.96], 0.06: [-99.97], 0.09: [-101.06], 0.12: [-107.54], 0.15: [-114.63], 0.18: [-117.84], 0.21: [-125.39], 0.24: [-141.21], 0.27: [-148.73], 0.3: [-153.65]}\n","diff noise: {0.0: [-91.46], 0.03: [-93.28], 0.06: [-99.33], 0.09: [-103.65], 0.12: [-110.83], 0.15: [-109.97], 0.18: [-122.57], 0.21: [-122.41], 0.24: [-140.41], 0.27: [-145.1], 0.3: [-162.54]}\n","goal only: {0.0: [-94.85], 0.03: [-96.59], 0.06: [-99.86], 0.09: [-104.94], 0.12: [-105.7], 0.15: [-110.61], 0.18: [-112.61], 0.21: [-120.17], 0.24: [-127.97], 0.27: [-137.54], 0.3: [-149.36]}\n","action only: {0.0: [-92.3], 0.03: [-99.47], 0.06: [-94.77], 0.09: [-94.28], 0.12: [-91.94], 0.15: [-96.1], 0.18: [-101.82], 0.21: [-98.42], 0.24: [-98.57], 0.27: [-100.67], 0.3: [-97.36]}\n","same noise: {0.0: [-91.89], 0.03: [-94.96], 0.06: [-99.97], 0.09: [-101.06], 0.12: [-107.54], 0.15: [-114.63], 0.18: [-117.84], 0.21: [-125.39], 0.24: [-141.21], 0.27: [-148.73], 0.3: [-153.65]}\n","diff noise: {0.0: [-91.46], 0.03: [-93.28], 0.06: [-99.33], 0.09: [-103.65], 0.12: [-110.83], 0.15: [-109.97], 0.18: [-122.57], 0.21: [-122.41], 0.24: [-140.41], 0.27: [-145.1], 0.3: [-162.54]}\n","goal only: {0.0: [-94.85], 0.03: [-96.59], 0.06: [-99.86], 0.09: [-104.94], 0.12: [-105.7], 0.15: [-110.61], 0.18: [-112.61], 0.21: [-120.17], 0.24: [-127.97], 0.27: [-137.54], 0.3: [-149.36]}\n","action only: {0.0: [-92.3], 0.03: [-99.47], 0.06: [-94.77], 0.09: [-94.28], 0.12: [-91.94], 0.15: [-96.1], 0.18: [-101.82], 0.21: [-98.42], 0.24: [-98.57], 0.27: [-100.67], 0.3: [-97.36]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xTzYH6SSyr_j"},"source":[""],"execution_count":null,"outputs":[]}]}