{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"td3_cartpole.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CJMjXntuErvs"},"source":["%matplotlib inline\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from IPython import display\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCbXyTAQHNs"},"source":["class NormalizedEnv(gym.ActionWrapper):\n","    \"\"\" Wrap action \"\"\"\n","\n","    def action(self, action):\n","        act_k = (self.action_space.high - self.action_space.low)/ 2.\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k * action + act_b\n","\n","    def reverse_action(self, action):\n","        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n","        act_b = (self.action_space.high + self.action_space.low)/ 2.\n","        return act_k_inv * (action - act_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRSC05Y-Erv0"},"source":["from continuous_cartpole import ContinuousCartPoleEnv \n","env = NormalizedEnv(ContinuousCartPoleEnv())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiZFY63MErv3"},"source":["***"]},{"cell_type":"code","metadata":{"id":"DQtcj2j8Erv4"},"source":["def plot_durations(episode_durations):\n","    fig, axs = plt.subplots(2, figsize=(10,10))\n","    \n","    durations_t, durations = list(map(list, zip(*episode_durations)))\n","    durations = torch.tensor(durations, dtype=torch.float)\n","    \n","    fig.suptitle('Training')\n","    axs[0].set_xlabel('Episode')\n","    axs[0].set_ylabel('Reward')\n","    \n","    axs[0].plot(durations_t, durations.numpy())\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyQnUb6KErv6"},"source":["# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n","\n","class RandomProcess(object):\n","    def reset_states(self):\n","        pass\n","\n","class AnnealedGaussianProcess(RandomProcess):\n","    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.n_steps = 0\n","\n","        if sigma_min is not None:\n","            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n","            self.c = sigma\n","            self.sigma_min = sigma_min\n","        else:\n","            self.m = 0.\n","            self.c = sigma\n","            self.sigma_min = sigma\n","\n","    @property\n","    def current_sigma(self):\n","        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n","        return sigma\n","\n","\n","# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n","class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n","    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n","        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n","        self.theta = theta\n","        self.mu = mu\n","        self.dt = dt\n","        self.x0 = x0\n","        self.size = size\n","        self.reset_states()\n","\n","    def sample(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n","        self.x_prev = x\n","        self.n_steps += 1\n","        return x\n","\n","    def reset_states(self):\n","        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWIkep5aErv9"},"source":["def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(\n","            target_param.data * (1.0 - tau) + param.data * tau\n","        )\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","            target_param.data.copy_(param.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtW05marErwA"},"source":["# (state, action) -> (next_state, reward, done)\n","transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","# replay memory D with capacity N\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    # implemented as a cyclical queue\n","    def store(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        \n","        self.memory[self.position] = transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrMrvwO1ErwC"},"source":["***"]},{"cell_type":"code","metadata":{"id":"0oyBjK1AErwD"},"source":["def fanin_init(size, fanin=None):\n","    fanin = fanin or size[0]\n","    v = 1. / np.sqrt(fanin)\n","    return torch.Tensor(size).uniform_(-v, v)\n","\n","class Actor(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(nb_states, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.head = nn.Linear(128, nb_actions)\n","        \n","        self.tanh = nn.Tanh()\n","        self.init_weights(3e-3)\n","    \n","    def init_weights(self, init_w):\n","        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n","        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n","        self.head.weight.data.uniform_(-init_w, init_w)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.tanh(self.head(x))\n","\n","class Critic(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(Critic, self).__init__()\n","\n","        # Q1 architecture\n","        self.l1 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l2 = nn.Linear(128, 128)\n","        self.l3 = nn.Linear(128, 1)\n","\n","        # Q2 architecture\n","        self.l4 = nn.Linear(nb_states + nb_actions, 128)\n","        self.l5 = nn.Linear(128, 128)\n","        self.l6 = nn.Linear(128, 1)\n","    \n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","    def Q1(self, state, action):\n","        sa = torch.cat([state, action], 1).float()\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-9mozrWErwG"},"source":["BATCH_SIZE = 64\n","GAMMA = 0.99\n","\n","# https://spinningup.openai.com/en/latest/algorithms/td3.html\n","class TD3(nn.Module):\n","    def __init__(self, nb_states, nb_actions):\n","        super(TD3, self).__init__()\n","        self.nb_states = nb_states\n","        self.nb_actions= nb_actions\n","        \n","        self.actor = Actor(self.nb_states, self.nb_actions)\n","        self.actor_target = Actor(self.nb_states, self.nb_actions)\n","        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=0.0001)\n","\n","        self.critic = Critic(self.nb_states, self.nb_actions)\n","        self.critic_target = Critic(self.nb_states, self.nb_actions)\n","        self.critic_optimizer  = optim.Adam(self.critic.parameters(), lr=0.0001)\n","\n","        hard_update(self.actor_target, self.actor)\n","        hard_update(self.critic_target, self.critic)\n","        \n","        #Create replay buffer\n","        self.memory = ReplayMemory(100000)\n","        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.15, mu=0.0, sigma=0.2)\n","\n","        # Hyper-parameters\n","        self.tau = 0.005\n","        self.depsilon = 1.0 / 50000\n","        self.policy_noise=0.2\n","        self.noise_clip=0.5\n","        self.policy_freq=2\n","        self.total_it = 0\n","\n","        # \n","        self.epsilon = 1.0\n","        self.s_t = None # Most recent state\n","        self.a_t = None # Most recent action\n","        self.is_training = True\n","\n","    def update_policy(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        self.total_it += 1\n","        \n","        # in the form (state, action) -> (next_state, reward, done)\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = transition(*zip(*transitions))\n","        \n","        state_batch = torch.cat(batch.state)\n","        next_state_batch = torch.cat(batch.next_state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        done_mask = np.array(batch.done)\n","        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n","\n","        # Target Policy Smoothing\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (\n","                torch.randn_like(action_batch) * self.policy_noise\n","            ).clamp(-self.noise_clip, self.noise_clip).float()\n","            \n","            next_action = (\n","                self.actor_target(next_state_batch) + noise\n","            ).clamp(-1.0, 1.0).float()\n","\n","            # Compute the target Q value\n","            # Clipped Double-Q Learning\n","            target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2).squeeze(1)\n","            target_Q = (reward_batch + GAMMA * not_done_mask  * target_Q).float()\n","        \n","        # Critic update\n","        current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","        \n","        critic_loss = F.mse_loss(current_Q1, target_Q.unsqueeze(1)) + F.mse_loss(current_Q2, target_Q.unsqueeze(1))\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % self.policy_freq == 0:\n","            # Compute actor loss\n","            actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Target update\n","            soft_update(self.actor_target, self.actor, self.tau)\n","            soft_update(self.critic_target, self.critic, self.tau)\n","\n","    def eval(self):\n","        self.actor.eval()\n","        self.actor_target.eval()\n","        self.critic.eval()\n","        self.critic_target.eval()\n","\n","    def observe(self, r_t, s_t1, done):\n","        if self.is_training:\n","            self.memory.store(self.s_t, self.a_t, s_t1, r_t, done)\n","            self.s_t = s_t1\n","\n","    def random_action(self):\n","        action = torch.tensor([np.random.uniform(-1.,1.,self.nb_actions)], device=device, dtype=torch.float)\n","        self.a_t = action\n","        \n","        return action\n","\n","    def select_action(self, s_t, decay_epsilon=True):\n","        with torch.no_grad():\n","            action = self.actor(s_t).squeeze(0)\n","            action += torch.from_numpy(self.is_training * max(self.epsilon, 0) * self.random_process.sample()).to(device).float()\n","            action = torch.clamp(action, -1., 1.)\n","\n","            action = action.unsqueeze(0)\n","            \n","            if decay_epsilon:\n","                self.epsilon -= self.depsilon\n","            \n","            self.a_t = action\n","            return action\n","\n","    def reset(self, obs):\n","        self.s_t = obs\n","        self.random_process.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_KKbeSErwI"},"source":["def train_model():\n","    n_observations = env.observation_space.shape[0]\n","    n_actions = env.action_space.shape[0]\n","    \n","    agent = TD3(n_observations, n_actions).to(device)\n","    \n","    max_episode_length = 500\n","    \n","    agent.is_training = True\n","    episode_reward = 0.\n","    observation = None\n","    \n","    warmup = 50\n","    num_episodes = 2000 # M\n","    episode_durations = []\n","\n","    steps = 0\n","\n","    for i_episode in range(num_episodes):\n","        observation = env.reset()\n","        state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","        agent.reset(state)\n","        \n","        overall_reward = 0\n","        episode_steps = 0\n","        done = False\n","        while not done:\n","            # agent pick action ...\n","            if i_episode <= warmup:\n","                action = agent.random_action()\n","            else:\n","                action = agent.select_action(state)\n","\n","            # env response with next_observation, reward, terminate_info\n","            observation, reward, done, info = env.step(action.detach().cpu().squeeze(0).numpy())\n","            steps += 1\n","            \n","            if max_episode_length and episode_steps >= max_episode_length -1:\n","                done = True\n","                \n","            extrinsic_reward = torch.tensor([reward], device=device)\n","            \n","            overall_reward += reward\n","            \n","            next_state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            # agent observe and update policy\n","            agent.observe(extrinsic_reward, next_state, done)\n","\n","            episode_steps += 1            \n","\n","            state = next_state\n","            \n","            if i_episode > warmup:\n","                agent.update_policy()\n","\n","        episode_durations.append((i_episode, overall_reward))\n","        #plot_durations(episode_durations)\n","        _, dur = list(map(list, zip(*episode_durations)))\n","        if len(dur) > 100:\n","            if np.mean(dur[-100:]) >= 195:\n","                print(f\"Solved after {i_episode} episodes!\")\n","                return agent\n","    return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxbm4KEzAsq_"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\n","def eval_model(agent, episode_durations):\n","    agent.eval()\n","    agent.is_training = False\n","    max_episode_length = 200\n","    num_episodes = 100\n","\n","    for noise in np.arange(0,0.31,0.03):\n","\n","        overall_reward = 0\n","        for i_episode in range(num_episodes):\n","            observation = env.reset()\n","            # unsqueeze adds batch dimension\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","            episode_steps = 0\n","            done = False\n","            while not done:\n","                state = state + state_max * torch.FloatTensor(state.shape).uniform_(-noise/2, noise/2).to(device)\n","                state = state.float()\n","\n","                action = agent.select_action(state)\n","                observation, reward, done, _ = env.step(action.detach().cpu().squeeze(0).numpy())\n","                overall_reward += reward\n","\n","                if max_episode_length and episode_steps >= max_episode_length - 1:\n","                    done = True\n","                episode_steps += 1\n","\n","                state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n","\n","        episode_durations[noise].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgdtuIjSK-Aa"},"source":["state_max = torch.from_numpy(env.observation_space.high).to(device)\r\n","state_min = torch.from_numpy(env.observation_space.low).to(device)\r\n","def fgsm_attack(data, eps, data_grad):\r\n","    sign_data_grad = data_grad.sign()\r\n","\r\n","    perturbed_data = data + eps * sign_data_grad * state_max\r\n","\r\n","    clipped_perturbed_data = torch.max(torch.min(perturbed_data, state_max), state_min)\r\n","\r\n","    return clipped_perturbed_data\r\n","\r\n","def fgsm_action(state, agent, eps, target, targetted):\r\n","    state = state.clone().detach().requires_grad_(True)\r\n","\r\n","    # initial forward pass\r\n","    action = agent.actor(state)\r\n","    action = torch.clamp(action, -1., 1.)\r\n","\r\n","    if not targetted:\r\n","        loss = F.mse_loss(action, target)\r\n","    else:\r\n","        loss = F.mse_loss(action, target if action > 0 else -target)\r\n","    agent.actor.zero_grad()\r\n","\r\n","    # calc loss\r\n","    loss.backward()\r\n","    data_grad = state.grad.data\r\n","    # perturb state\r\n","    state_p = fgsm_attack(state, eps, data_grad).float()\r\n","    return agent.select_action(state_p)\r\n","\r\n","def apply_fgsm(agent, episode_durations, targetted):\r\n","    TARGET_ACTION = torch.tensor([[1.0]], device=device, dtype=torch.float)\r\n","\r\n","    agent.eval()\r\n","\r\n","    max_episode_length = 200\r\n","    agent.is_training = False\r\n","\r\n","    num_episodes = 100\r\n","\r\n","    for eps in np.arange(0.0, 0.031, 0.0025):\r\n","\r\n","        overall_reward = 0\r\n","        for i_episode in range(num_episodes):\r\n","            observation = env.reset()\r\n","            # unsqueeze adds batch dimension\r\n","            state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","            episode_steps = 0\r\n","            done = False\r\n","            while not done:\r\n","                action = fgsm_action(state, agent, eps, TARGET_ACTION, targetted)\r\n","                action_i = action.detach().cpu().squeeze(0).numpy()\r\n","\r\n","                tot_rew = 0\r\n","                for _ in range(4):\r\n","                    if done:\r\n","                        continue\r\n","                    observation, reward, done, info = env.step(action_i)\r\n","                    \r\n","                    tot_rew += reward\r\n","                    \r\n","                    if max_episode_length and episode_steps >= max_episode_length - 1:\r\n","                        done = True\r\n","                    \r\n","                    episode_steps += 1 \r\n","\r\n","                overall_reward += tot_rew\r\n","\r\n","                state = torch.from_numpy(observation).float().unsqueeze(0).to(device)\r\n","\r\n","        episode_durations[eps].append(overall_reward / num_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34H37Md4A-PZ"},"source":["def plot_norms(episode_durations):\n","    plt.figure(2, figsize=(10,10))\n","    \n","    x, ys = np.array(list(episode_durations.keys())), np.array(list(episode_durations.values()))\n","    \n","    plt.title('Action Prediction $\\mu$ and $\\pm \\sigma$ interval')\n","    plt.xlabel('L2 Norm')\n","    plt.ylabel('Average Reward')\n","    \n","    mu = np.mean(ys, axis=1)\n","    plt.plot(x / 10, mu)\n","    stds = np.std(ys, axis = 1)\n","    plt.fill_between(x / 10, mu + stds , mu - stds, alpha=0.2)\n","        \n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    display.clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jazaVnJNErwK","colab":{"base_uri":"https://localhost:8080/","height":938},"executionInfo":{"status":"error","timestamp":1608900368314,"user_tz":-60,"elapsed":1937483,"user":{"displayName":"D.G. Khachaturov","photoUrl":"","userId":"02831277067458900124"}},"outputId":"1c135c9a-c9b3-4fc2-bffc-31e54e07d843"},"source":["episodes = {}\n","for l2norm in np.arange(0,0.31,0.03):\n","    episodes[l2norm] = []\n","targeted = {}\n","untargeted = {}\n","for eps in np.arange(0.0, 0.031, 0.0025):\n","    targeted[eps] = []\n","    untargeted[eps] = []\n","\n","# train 20 models for 200 steps, then eval them\n","i = 0\n","while i < 10:\n","    agent = train_model()\n","    if agent is not None:\n","        #eval_model(agent, episodes)\n","        apply_fgsm(agent, targeted, True)\n","        apply_fgsm(agent, untargeted, False)\n","        print(i)\n","        print(f\"Targeted: {targeted}\")\n","        print(f\"Untargeted: {untargeted}\")\n","        i += 1\n","\n","print(f\"Targeted: {targeted}\")\n","print(f\"Untargeted: {untargeted}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Solved after 723 episodes!\n","0\n","Targeted: {0.0: [199.4], 0.0025: [198.71], 0.005: [97.99], 0.0075: [46.26], 0.01: [18.48], 0.0125: [11.95], 0.015: [10.28], 0.0175: [10.23], 0.02: [9.98], 0.0225: [9.94], 0.025: [10.21], 0.0275: [10.34], 0.03: [9.99]}\n","Untargeted: {0.0: [197.96], 0.0025: [183.03], 0.005: [145.35], 0.0075: [82.12], 0.01: [29.38], 0.0125: [18.95], 0.015: [20.88], 0.0175: [22.81], 0.02: [21.42], 0.0225: [22.84], 0.025: [23.6], 0.0275: [21.98], 0.03: [23.09]}\n","Solved after 916 episodes!\n","1\n","Targeted: {0.0: [199.4, 174.14], 0.0025: [198.71, 163.11], 0.005: [97.99, 148.84], 0.0075: [46.26, 72.76], 0.01: [18.48, 62.38], 0.0125: [11.95, 21.82], 0.015: [10.28, 14.06], 0.0175: [10.23, 10.74], 0.02: [9.98, 9.76], 0.0225: [9.94, 9.6], 0.025: [10.21, 9.4], 0.0275: [10.34, 9.45], 0.03: [9.99, 9.42]}\n","Untargeted: {0.0: [197.96, 168.23], 0.0025: [183.03, 126.24], 0.005: [145.35, 97.34], 0.0075: [82.12, 50.28], 0.01: [29.38, 22.91], 0.0125: [18.95, 19.23], 0.015: [20.88, 16.91], 0.0175: [22.81, 14.55], 0.02: [21.42, 13.58], 0.0225: [22.84, 12.92], 0.025: [23.6, 11.8], 0.0275: [21.98, 11.71], 0.03: [23.09, 11.93]}\n","Solved after 584 episodes!\n","2\n","Targeted: {0.0: [199.4, 174.14, 155.65], 0.0025: [198.71, 163.11, 53.92], 0.005: [97.99, 148.84, 79.62], 0.0075: [46.26, 72.76, 68.56], 0.01: [18.48, 62.38, 14.48], 0.0125: [11.95, 21.82, 11.09], 0.015: [10.28, 14.06, 10.05], 0.0175: [10.23, 10.74, 10.13], 0.02: [9.98, 9.76, 9.81], 0.0225: [9.94, 9.6, 11.18], 0.025: [10.21, 9.4, 10.33], 0.0275: [10.34, 9.45, 10.81], 0.03: [9.99, 9.42, 10.41]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77], 0.0025: [183.03, 126.24, 193.81], 0.005: [145.35, 97.34, 130.41], 0.0075: [82.12, 50.28, 69.11], 0.01: [29.38, 22.91, 12.02], 0.0125: [18.95, 19.23, 9.98], 0.015: [20.88, 16.91, 10.19], 0.0175: [22.81, 14.55, 10.27], 0.02: [21.42, 13.58, 10.3], 0.0225: [22.84, 12.92, 10.2], 0.025: [23.6, 11.8, 10.08], 0.0275: [21.98, 11.71, 10.18], 0.03: [23.09, 11.93, 10.12]}\n","Solved after 441 episodes!\n","3\n","Targeted: {0.0: [199.4, 174.14, 155.65, 199.8], 0.0025: [198.71, 163.11, 53.92, 156.62], 0.005: [97.99, 148.84, 79.62, 76.74], 0.0075: [46.26, 72.76, 68.56, 34.73], 0.01: [18.48, 62.38, 14.48, 13.2], 0.0125: [11.95, 21.82, 11.09, 10.54], 0.015: [10.28, 14.06, 10.05, 10.33], 0.0175: [10.23, 10.74, 10.13, 10.35], 0.02: [9.98, 9.76, 9.81, 10.46], 0.0225: [9.94, 9.6, 11.18, 10.4], 0.025: [10.21, 9.4, 10.33, 10.37], 0.0275: [10.34, 9.45, 10.81, 10.53], 0.03: [9.99, 9.42, 10.41, 10.6]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77, 199.98], 0.0025: [183.03, 126.24, 193.81, 156.41], 0.005: [145.35, 97.34, 130.41, 75.02], 0.0075: [82.12, 50.28, 69.11, 15.04], 0.01: [29.38, 22.91, 12.02, 13.83], 0.0125: [18.95, 19.23, 9.98, 11.84], 0.015: [20.88, 16.91, 10.19, 10.46], 0.0175: [22.81, 14.55, 10.27, 10.45], 0.02: [21.42, 13.58, 10.3, 10.46], 0.0225: [22.84, 12.92, 10.2, 10.2], 0.025: [23.6, 11.8, 10.08, 10.26], 0.0275: [21.98, 11.71, 10.18, 10.53], 0.03: [23.09, 11.93, 10.12, 10.12]}\n","Solved after 858 episodes!\n","4\n","Targeted: {0.0: [199.4, 174.14, 155.65, 199.8, 199.16], 0.0025: [198.71, 163.11, 53.92, 156.62, 48.35], 0.005: [97.99, 148.84, 79.62, 76.74, 105.91], 0.0075: [46.26, 72.76, 68.56, 34.73, 47.73], 0.01: [18.48, 62.38, 14.48, 13.2, 36.73], 0.0125: [11.95, 21.82, 11.09, 10.54, 33.97], 0.015: [10.28, 14.06, 10.05, 10.33, 35.43], 0.0175: [10.23, 10.74, 10.13, 10.35, 35.85], 0.02: [9.98, 9.76, 9.81, 10.46, 32.1], 0.0225: [9.94, 9.6, 11.18, 10.4, 35.77], 0.025: [10.21, 9.4, 10.33, 10.37, 37.03], 0.0275: [10.34, 9.45, 10.81, 10.53, 42.03], 0.03: [9.99, 9.42, 10.41, 10.6, 35.7]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77, 199.98, 199.38], 0.0025: [183.03, 126.24, 193.81, 156.41, 195.89], 0.005: [145.35, 97.34, 130.41, 75.02, 141.36], 0.0075: [82.12, 50.28, 69.11, 15.04, 59.51], 0.01: [29.38, 22.91, 12.02, 13.83, 47.7], 0.0125: [18.95, 19.23, 9.98, 11.84, 30.91], 0.015: [20.88, 16.91, 10.19, 10.46, 27.78], 0.0175: [22.81, 14.55, 10.27, 10.45, 35.02], 0.02: [21.42, 13.58, 10.3, 10.46, 32.36], 0.0225: [22.84, 12.92, 10.2, 10.2, 31.79], 0.025: [23.6, 11.8, 10.08, 10.26, 32.48], 0.0275: [21.98, 11.71, 10.18, 10.53, 31.84], 0.03: [23.09, 11.93, 10.12, 10.12, 28.43]}\n","Solved after 602 episodes!\n","5\n","Targeted: {0.0: [199.4, 174.14, 155.65, 199.8, 199.16, 155.93], 0.0025: [198.71, 163.11, 53.92, 156.62, 48.35, 34.91], 0.005: [97.99, 148.84, 79.62, 76.74, 105.91, 46.9], 0.0075: [46.26, 72.76, 68.56, 34.73, 47.73, 29.47], 0.01: [18.48, 62.38, 14.48, 13.2, 36.73, 28.89], 0.0125: [11.95, 21.82, 11.09, 10.54, 33.97, 29.66], 0.015: [10.28, 14.06, 10.05, 10.33, 35.43, 25.44], 0.0175: [10.23, 10.74, 10.13, 10.35, 35.85, 30.85], 0.02: [9.98, 9.76, 9.81, 10.46, 32.1, 30.76], 0.0225: [9.94, 9.6, 11.18, 10.4, 35.77, 27.71], 0.025: [10.21, 9.4, 10.33, 10.37, 37.03, 27.49], 0.0275: [10.34, 9.45, 10.81, 10.53, 42.03, 29.7], 0.03: [9.99, 9.42, 10.41, 10.6, 35.7, 27.5]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77, 199.98, 199.38, 152.23], 0.0025: [183.03, 126.24, 193.81, 156.41, 195.89, 99.71], 0.005: [145.35, 97.34, 130.41, 75.02, 141.36, 53.2], 0.0075: [82.12, 50.28, 69.11, 15.04, 59.51, 20.02], 0.01: [29.38, 22.91, 12.02, 13.83, 47.7, 12.42], 0.0125: [18.95, 19.23, 9.98, 11.84, 30.91, 12.39], 0.015: [20.88, 16.91, 10.19, 10.46, 27.78, 12.27], 0.0175: [22.81, 14.55, 10.27, 10.45, 35.02, 12.13], 0.02: [21.42, 13.58, 10.3, 10.46, 32.36, 13.42], 0.0225: [22.84, 12.92, 10.2, 10.2, 31.79, 13.14], 0.025: [23.6, 11.8, 10.08, 10.26, 32.48, 13.69], 0.0275: [21.98, 11.71, 10.18, 10.53, 31.84, 12.77], 0.03: [23.09, 11.93, 10.12, 10.12, 28.43, 13.5]}\n","Solved after 811 episodes!\n","6\n","Targeted: {0.0: [199.4, 174.14, 155.65, 199.8, 199.16, 155.93, 200.0], 0.0025: [198.71, 163.11, 53.92, 156.62, 48.35, 34.91, 102.63], 0.005: [97.99, 148.84, 79.62, 76.74, 105.91, 46.9, 77.99], 0.0075: [46.26, 72.76, 68.56, 34.73, 47.73, 29.47, 12.97], 0.01: [18.48, 62.38, 14.48, 13.2, 36.73, 28.89, 9.89], 0.0125: [11.95, 21.82, 11.09, 10.54, 33.97, 29.66, 9.63], 0.015: [10.28, 14.06, 10.05, 10.33, 35.43, 25.44, 9.92], 0.0175: [10.23, 10.74, 10.13, 10.35, 35.85, 30.85, 10.05], 0.02: [9.98, 9.76, 9.81, 10.46, 32.1, 30.76, 10.05], 0.0225: [9.94, 9.6, 11.18, 10.4, 35.77, 27.71, 9.96], 0.025: [10.21, 9.4, 10.33, 10.37, 37.03, 27.49, 10.74], 0.0275: [10.34, 9.45, 10.81, 10.53, 42.03, 29.7, 10.4], 0.03: [9.99, 9.42, 10.41, 10.6, 35.7, 27.5, 12.14]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77, 199.98, 199.38, 152.23, 200.0], 0.0025: [183.03, 126.24, 193.81, 156.41, 195.89, 99.71, 183.74], 0.005: [145.35, 97.34, 130.41, 75.02, 141.36, 53.2, 84.31], 0.0075: [82.12, 50.28, 69.11, 15.04, 59.51, 20.02, 11.03], 0.01: [29.38, 22.91, 12.02, 13.83, 47.7, 12.42, 9.73], 0.0125: [18.95, 19.23, 9.98, 11.84, 30.91, 12.39, 9.76], 0.015: [20.88, 16.91, 10.19, 10.46, 27.78, 12.27, 10.37], 0.0175: [22.81, 14.55, 10.27, 10.45, 35.02, 12.13, 9.92], 0.02: [21.42, 13.58, 10.3, 10.46, 32.36, 13.42, 9.93], 0.0225: [22.84, 12.92, 10.2, 10.2, 31.79, 13.14, 9.86], 0.025: [23.6, 11.8, 10.08, 10.26, 32.48, 13.69, 10.45], 0.0275: [21.98, 11.71, 10.18, 10.53, 31.84, 12.77, 9.87], 0.03: [23.09, 11.93, 10.12, 10.12, 28.43, 13.5, 9.91]}\n","Solved after 495 episodes!\n","7\n","Targeted: {0.0: [199.4, 174.14, 155.65, 199.8, 199.16, 155.93, 200.0, 189.43], 0.0025: [198.71, 163.11, 53.92, 156.62, 48.35, 34.91, 102.63, 77.61], 0.005: [97.99, 148.84, 79.62, 76.74, 105.91, 46.9, 77.99, 127.71], 0.0075: [46.26, 72.76, 68.56, 34.73, 47.73, 29.47, 12.97, 45.49], 0.01: [18.48, 62.38, 14.48, 13.2, 36.73, 28.89, 9.89, 15.79], 0.0125: [11.95, 21.82, 11.09, 10.54, 33.97, 29.66, 9.63, 14.77], 0.015: [10.28, 14.06, 10.05, 10.33, 35.43, 25.44, 9.92, 15.47], 0.0175: [10.23, 10.74, 10.13, 10.35, 35.85, 30.85, 10.05, 15.5], 0.02: [9.98, 9.76, 9.81, 10.46, 32.1, 30.76, 10.05, 15.19], 0.0225: [9.94, 9.6, 11.18, 10.4, 35.77, 27.71, 9.96, 15.73], 0.025: [10.21, 9.4, 10.33, 10.37, 37.03, 27.49, 10.74, 15.47], 0.0275: [10.34, 9.45, 10.81, 10.53, 42.03, 29.7, 10.4, 15.38], 0.03: [9.99, 9.42, 10.41, 10.6, 35.7, 27.5, 12.14, 15.43]}\n","Untargeted: {0.0: [197.96, 168.23, 153.77, 199.98, 199.38, 152.23, 200.0, 189.39], 0.0025: [183.03, 126.24, 193.81, 156.41, 195.89, 99.71, 183.74, 197.66], 0.005: [145.35, 97.34, 130.41, 75.02, 141.36, 53.2, 84.31, 129.29], 0.0075: [82.12, 50.28, 69.11, 15.04, 59.51, 20.02, 11.03, 33.13], 0.01: [29.38, 22.91, 12.02, 13.83, 47.7, 12.42, 9.73, 15.0], 0.0125: [18.95, 19.23, 9.98, 11.84, 30.91, 12.39, 9.76, 15.5], 0.015: [20.88, 16.91, 10.19, 10.46, 27.78, 12.27, 10.37, 15.15], 0.0175: [22.81, 14.55, 10.27, 10.45, 35.02, 12.13, 9.92, 15.73], 0.02: [21.42, 13.58, 10.3, 10.46, 32.36, 13.42, 9.93, 15.55], 0.0225: [22.84, 12.92, 10.2, 10.2, 31.79, 13.14, 9.86, 15.49], 0.025: [23.6, 11.8, 10.08, 10.26, 32.48, 13.69, 10.45, 15.66], 0.0275: [21.98, 11.71, 10.18, 10.53, 31.84, 12.77, 9.87, 15.17], 0.03: [23.09, 11.93, 10.12, 10.12, 28.43, 13.5, 9.91, 15.78]}\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9e3b762f6e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#eval_model(agent, episodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e90e92569ce3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mepisode_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-8862f3972c48>\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnext_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"6T1_obc7eegO"},"source":[""],"execution_count":null,"outputs":[]}]}