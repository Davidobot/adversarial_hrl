{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install torch==1.4.0 torchvision==0.5.0\n",
    "!pip install gym\n",
    "!pip install gym[atari]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "!cp \"/content/drive/My Drive/Dissertation/preprocessing.py\" .\n",
    "!cp -r \"/content/drive/My Drive/Dissertation/gym_maze\" .\n",
    "!cp -r \"/content/drive/My Drive/Dissertation/envs\" .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "from preprocessing import AtariPreprocessing\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SKIP = 4\n",
    "\n",
    "# resize to 84, make greyscale and scale observations from 0 to 1\n",
    "env = AtariPreprocessing(gym.make('MontezumaRevengeNoFrameskip-v0'), frame_skip=FRAME_SKIP, screen_size=84, grayscale_newaxis=True, scale_obs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model):\n",
    "  model_save_name = 'hdqn_montezuma.pt'\n",
    "  path = F\"/content/drive/My Drive/Dissertation/{model_save_name}\" \n",
    "\n",
    "  torch.save({\n",
    "    'steps': epoch,\n",
    "    'meta_controller_state_dict': model.meta_controller.state_dict(),\n",
    "    'controller_state_dict': model.controller.state_dict(),\n",
    "    'meta_controller_optimizer': model.meta_controller.optimizer.state_dict(),\n",
    "    'controller_optimizer': model.controller.optimizer.state_dict(),\n",
    "    'meta_controller_memory': model.meta_controller.memory,\n",
    "    'controller_memory': model.controller.memory\n",
    "    }, path)\n",
    "\n",
    "def load_model(model):\n",
    "  model_save_name = 'hdqn_montezuma.pt'\n",
    "  path = F\"/content/drive/My Drive/Dissertation/{model_save_name}\" \n",
    "  checkpoint = torch.load(path)\n",
    "\n",
    "  model.meta_controller.load_state_dict(checkpoint['meta_controller_state_dict'])\n",
    "  model.controller.load_state_dict(checkpoint['controller_state_dict'])\n",
    "  model.meta_controller.optimizer.load_state_dict(checkpoint['meta_controller_optimizer'])\n",
    "  model.controller.optimizer.load_state_dict(checkpoint['controller_optimizer'])\n",
    "  model.meta_controller.memory = checkpoint['meta_controller_memory']\n",
    "  model.controller.memory = checkpoint['controller_memory']\n",
    "\n",
    "  # model.eval() for evaluation instead\n",
    "  model.train()\n",
    "  return checkpoint['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "    \n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    visits_t = [(x[0], torch.tensor(x[1], dtype=torch.float)) for x in state_visits.items()]\n",
    "    \n",
    "    fig.suptitle('Training')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Reward')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('State Visits')\n",
    "    \n",
    "    axs[0].plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 10:\n",
    "        means = durations_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(9), means))\n",
    "        axs[0].plot(means.numpy())\n",
    "    \n",
    "    if len(durations_t) >= 10:\n",
    "        for t in visits_t:\n",
    "            means = t[1].unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            axs[1].plot(means.numpy(), label=f\"State {t[0]}\")\n",
    "            axs[1].legend()\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen():   \n",
    "    # transpose it into torch order (THW)\n",
    "    screen = env._get_obs().transpose((2, 0, 1))\n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # add a batch dimension (BTHW)\n",
    "    return screen.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_bounding_box():\n",
    "    \"\"\"\n",
    "    output: (x, y, width, height)\n",
    "    \"\"\"\n",
    "    screen = env._get_obs_coloured()\n",
    "    \n",
    "    if np.count_nonzero(screen[:,:,:,3]) > 0:\n",
    "        screen = screen[:,:,:,3]\n",
    "    else:\n",
    "        # eg right after reset \n",
    "        screen = screen[:,:,:,0]\n",
    "    \n",
    "    screen = np.asarray(screen * 255, dtype=np.uint8)\n",
    "    \n",
    "    hsv = cv2.cvtColor(screen, cv2.COLOR_RGB2HSV) \n",
    "      \n",
    "    # lower mask (0-10)\n",
    "    lower_red = np.array([0,50,50])\n",
    "    upper_red = np.array([10,255,255])\n",
    "    mask0 = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "    # upper mask (170-180)\n",
    "    lower_red = np.array([170,50,50])\n",
    "    upper_red = np.array([180,255,255])\n",
    "    mask1 = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "    # join masks\n",
    "    mask = mask0+mask1\n",
    "    mask[0:25,:] = 0\n",
    "    \n",
    "    i, j = np.where(mask)\n",
    "    \n",
    "    # no player detected?\n",
    "    if i.size == 0 or j.size == 0:\n",
    "        return (0, 0, 1, 1)\n",
    "    \n",
    "    return (min(j) - 2, min(i) - 2, max(j) + 3 - (min(j) - 2), max(i) + 3 - (min(i) - 2))\n",
    "\n",
    "# in form (x, y, width, height)\n",
    "objs = {\n",
    "    0: ( 16,  67, 11, 11),\n",
    "    1: (133,  67, 11, 11),\n",
    "    2: ( 74, 113, 11, 11),\n",
    "    3: ( 19, 161, 11, 11),\n",
    "    4: (130, 161, 11, 11),\n",
    "    5: ( 11, 102, 11, 11)\n",
    "}\n",
    "\n",
    "def get_object_bounding_box(i):\n",
    "    \"\"\"\n",
    "    6 \"entities\" as defined in the paper, also\n",
    "    doubling up as the goals\n",
    "        0: top-left door\n",
    "        1: top-right door\n",
    "        2: middle-ladder\n",
    "        3: bottom-left-ladder\n",
    "        4: bottom-right-ladder\n",
    "        5: key\n",
    "    bounding box checking is done in original screen space\n",
    "    \"\"\"\n",
    "    return objs[i]\n",
    "\n",
    "def get_screen_with_goal(i):\n",
    "    o = np.array(get_object_bounding_box(i))\n",
    "    screen = get_screen()\n",
    "    \n",
    "    # scale\n",
    "    o[0] = np.ceil(o[0] * (84 / 160))\n",
    "    o[2] = np.ceil(o[2] * (84 / 160))\n",
    "    o[1] = np.ceil(o[1] * (84 / 210))\n",
    "    o[3] = np.ceil(o[3] * (84 / 210))\n",
    "    \n",
    "    mask = np.zeros([84,84])\n",
    "    mask[o[1]:(o[1] + o[3]),o[0]:(o[0] + o[2])] = 1\n",
    "    mask = np.stack([mask]*4, 0)\n",
    "    mask = torch.from_numpy(mask).unsqueeze(0).to(device)\n",
    "    \n",
    "    return np.maximum(screen, mask)\n",
    "\n",
    "def goal_reached(i):\n",
    "    \"\"\"\n",
    "    does simple AABB collision checking\n",
    "    \"\"\"\n",
    "    p = get_player_bounding_box()\n",
    "    o = get_object_bounding_box(i)\n",
    "    if (o[0] >= p[0] + p[2]) or (o[0] + o[2] <= p[0]) or\\\n",
    "       (o[1] >= p[1] + p[3]) or (o[1] + o[3] <= p[1]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(5)\n",
    "plt.figure()\n",
    "plt.imshow(np.average(get_screen_with_goal(5).cpu().squeeze(0).permute(1, 2, 0).numpy(), axis=2), interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (state, action) -> (next_state, reward, done)\n",
    "transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# replay memory D with capacity N\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # implemented as a cyclical queue\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def one_hot(n, v):\n",
    "    a = np.zeros(n)\n",
    "    a[v] = 1.0\n",
    "    return np.expand_dims(a, axis=0)\n",
    "\n",
    "def rev_one_hot(a):\n",
    "    return np.where(a[0] > 0)[0][0]\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs, mem_len = 100000):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(FRAME_SKIP, 16, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2)\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc = nn.Linear(linear_input_size, 256)\n",
    "        self.head = nn.Linear(256, outputs)\n",
    "        \n",
    "        self.memory = ReplayMemory(mem_len)\n",
    "        self.optimizer = None\n",
    "        self.target = None # to keep parameters frozen while propogating losses\n",
    "        \n",
    "        self.n_actions = outputs\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.05\n",
    "        self.EPS_DECAY = 50000 # in number of steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
    "        return self.head(x)\n",
    "    \n",
    "    def act(self, state, step_size=1, pretrain=False):\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * (1. - min(1., self.steps_done / self.EPS_DECAY))\n",
    "\n",
    "        if pretrain:\n",
    "          eps_threshold = 1.0\n",
    "        else:\n",
    "          self.steps_done += step_size\n",
    "\n",
    "        # With probability eps select a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "        # otherwise select action = maxa Q∗(φ(st), a; θ)\n",
    "        with torch.no_grad():\n",
    "            return self(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # in the form (state, action) -> (next_state, reward, done)\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_mask = np.array(batch.done)\n",
    "        not_done_mask = torch.from_numpy(1 - done_mask).float().to(device)\n",
    "        \n",
    "        current_Q_values = self(state_batch).gather(1, action_batch)\n",
    "        # Compute next Q value based on which goal gives max Q values\n",
    "        # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "        next_max_q = self.target(next_state_batch).detach().max(1)[0]\n",
    "        next_Q_values = not_done_mask * next_max_q\n",
    "        # Compute the target of the current Q values\n",
    "        target_Q_values = reward_batch + (GAMMA * next_Q_values)\n",
    "        # Compute Bellman error (using Huber loss)\n",
    "        loss = F.smooth_l1_loss(current_Q_values, target_Q_values.unsqueeze(1))\n",
    "\n",
    "        # Copy Q to target Q before updating parameters of Q\n",
    "        self.target.load_state_dict(self.state_dict(), strict=False)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "class HDQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(HDQN, self).__init__()\n",
    "        # Optimizer\n",
    "        learning_rate = 2.5e-4\n",
    "        \n",
    "        # 6 goals, hard-coded\n",
    "        # mem_len should be 5e4\n",
    "        self.meta_controller = DQN(h, w, 6, mem_len = 5000).to(device)\n",
    "        self.meta_controller.optimizer = optim.RMSprop(self.meta_controller.parameters(), lr=learning_rate)\n",
    "        self.meta_controller.target = DQN(h, w, 6, mem_len = 0).to(device)\n",
    "        \n",
    "        # mem_len should be 1e6\n",
    "        self.controller = DQN(h, w, outputs, mem_len = 50000).to(device)\n",
    "        self.controller.optimizer = optim.RMSprop(self.controller.parameters(), lr=learning_rate)\n",
    "        self.controller.target = DQN(h, w, outputs, mem_len = 0).to(device)\n",
    "    \n",
    "    def store_controller(self, *args):\n",
    "        self.controller.memory.store(*args)\n",
    "    \n",
    "    def store_meta_controller(self, *args):\n",
    "        self.meta_controller.memory.store(*args)\n",
    "    \n",
    "    def select_goal(self, external_observation, pretrain):\n",
    "        return self.meta_controller.act(external_observation, pretrain=pretrain)\n",
    "        \n",
    "    def select_action(self, joint_goal_obs):\n",
    "        return self.controller.act(joint_goal_obs, step_size = FRAME_SKIP)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        self.meta_controller.experience_replay()\n",
    "        self.controller.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e9df479392a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mhdqnAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mold_lives\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-4b0d4f833d81>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_controller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-4b0d4f833d81>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Compute next Q value based on which goal gives max Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Detach variable from the current graph since we don't want gradients for next Q to propagated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mnext_max_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mnext_Q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnot_done_mask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_max_q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# Compute the target of the current Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-4b0d4f833d81>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\grigor\\git\\adversarial_hrl\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 415\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get number of actions and observations from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Initialize action-value function Q with random weights\n",
    "hdqnAgent = HDQN(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "num_episodes = 7000 # M\n",
    "episode_durations = []\n",
    "state_visits = {0: [], 1: [], 2: [], 3: [], 4: [], 5: []}\n",
    "\n",
    "steps = 0\n",
    "\n",
    "#steps = load_model(hdqnAgent)\n",
    "\n",
    "last_saved = steps\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    state = get_screen().float()\n",
    "    \n",
    "    overall_reward = 0\n",
    "    ep_state_visits = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "    done = False\n",
    "    while not done:\n",
    "        # select a goal\n",
    "        goal = hdqnAgent.select_goal(state, pretrain = steps < 2300000)\n",
    "        goal_i = goal.item()\n",
    "        \n",
    "        goal_done = False\n",
    "        total_extrinsic = 0\n",
    "        s_0 = state\n",
    "        while not done and not goal_done:\n",
    "            joint_goal_state = get_screen_with_goal(goal_i).float()\n",
    "            \n",
    "            # uncomment to render\n",
    "            #plt.figure()\n",
    "            #plt.clf()\n",
    "            #plt.imshow(np.average(joint_goal_state.cpu().squeeze(0).permute(1, 2, 0).numpy(), axis=2), interpolation='none')\n",
    "            #plt.title('Example extracted screen')\n",
    "            #plt.pause(0.001)\n",
    "            #display.clear_output(wait=True)\n",
    "            \n",
    "            # Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n",
    "            action = hdqnAgent.select_action(joint_goal_state)\n",
    "            \n",
    "            old_lives = env.lives\n",
    "            _, reward, done, _ = env.step(action.item())\n",
    "            steps += 1\n",
    "            extrinsic_reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            overall_reward += reward\n",
    "            total_extrinsic += reward\n",
    "\n",
    "            # preprocess φ_{t+1} = φ(s_{t+1})\n",
    "            next_state = get_screen().float()\n",
    "            joint_next_state = get_screen_with_goal(goal_i).float()\n",
    "\n",
    "            goal_done = goal_reached(goal_i)\n",
    "            \n",
    "            if goal_done:\n",
    "                ep_state_visits[goal_i] += 1\n",
    "            \n",
    "            intrinsic_reward = torch.tensor([1.0 if goal_done else 0.0], device=device)\n",
    "                \n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            hdqnAgent.store_controller(joint_goal_state, action, joint_next_state, intrinsic_reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            hdqnAgent.experience_replay()\n",
    "            \n",
    "            if env.lives < old_lives:\n",
    "                break\n",
    "            \n",
    "        # Store transition for meta controller\n",
    "        hdqnAgent.store_meta_controller(s_0, goal, next_state, torch.tensor([total_extrinsic], device=device), done)\n",
    "        hdqnAgent.experience_replay()\n",
    "\n",
    "        if (steps - last_saved) > 100000:\n",
    "            last_saved = steps\n",
    "            save_model(steps, hdqnAgent)\n",
    "        \n",
    "    episode_durations.append((steps, overall_reward))\n",
    "    for i in ep_state_visits.keys():\n",
    "        state_visits[i].append(ep_state_visits[i])\n",
    "    plot_durations()\n",
    "\n",
    "print('Complete')\n",
    "plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
